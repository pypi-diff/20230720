# Comparing `tmp/datarobot_model_metrics-0.1.6-py2.py3-none-any.whl.zip` & `tmp/datarobot_model_metrics-0.1.7-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,24 +1,24 @@
-Zip file size: 28951 bytes, number of entries: 22
--rw-r--r--  2.0 unx      385 b- defN 23-Jun-06 10:49 dmm/__init__.py
--rw-r--r--  2.0 unx     9663 b- defN 23-Jun-06 10:49 dmm/batch_metric_evaluator.py
--rw-r--r--  2.0 unx      747 b- defN 23-Jun-06 10:49 dmm/constants.py
--rw-r--r--  2.0 unx     6955 b- defN 23-Jun-06 10:49 dmm/example_data_helper.py
--rw-r--r--  2.0 unx    13942 b- defN 23-Jun-06 10:49 dmm/metric_evaluator.py
--rw-r--r--  2.0 unx     3164 b- defN 23-Jun-06 10:49 dmm/time_bucket.py
--rw-r--r--  2.0 unx     1690 b- defN 23-Jun-06 10:49 dmm/utils.py
--rw-r--r--  2.0 unx      525 b- defN 23-Jun-06 10:49 dmm/data_source/__init__.py
--rw-r--r--  2.0 unx     2096 b- defN 23-Jun-06 10:49 dmm/data_source/data_source_base.py
--rw-r--r--  2.0 unx     2698 b- defN 23-Jun-06 10:49 dmm/data_source/dataframe_source.py
--rw-r--r--  2.0 unx    58576 b- defN 23-Jun-06 10:49 dmm/data_source/datarobot_source.py
--rw-r--r--  2.0 unx     3003 b- defN 23-Jun-06 10:49 dmm/data_source/generator_source.py
--rw-r--r--  2.0 unx      396 b- defN 23-Jun-06 10:49 dmm/metric/__init__.py
--rw-r--r--  2.0 unx     1620 b- defN 23-Jun-06 10:49 dmm/metric/asymmetric_error.py
--rw-r--r--  2.0 unx      320 b- defN 23-Jun-06 10:49 dmm/metric/median_absolute_error.py
--rw-r--r--  2.0 unx     3569 b- defN 23-Jun-06 10:49 dmm/metric/metric_base.py
--rw-r--r--  2.0 unx      957 b- defN 23-Jun-06 10:49 dmm/metric/missing_values.py
--rw-r--r--  2.0 unx     2014 b- defN 23-Jun-06 10:49 dmm/metric/sklearn_metric.py
--rw-r--r--  2.0 unx      886 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1857 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/RECORD
-22 files, 115177 bytes uncompressed, 25925 bytes compressed:  77.5%
+Zip file size: 29248 bytes, number of entries: 22
+-rw-r--r--  2.0 unx      385 b- defN 23-Jul-20 12:44 dmm/__init__.py
+-rw-r--r--  2.0 unx     9663 b- defN 23-Jul-20 12:44 dmm/batch_metric_evaluator.py
+-rw-r--r--  2.0 unx      747 b- defN 23-Jul-20 12:44 dmm/constants.py
+-rw-r--r--  2.0 unx     6955 b- defN 23-Jul-20 12:44 dmm/example_data_helper.py
+-rw-r--r--  2.0 unx    14267 b- defN 23-Jul-20 12:44 dmm/metric_evaluator.py
+-rw-r--r--  2.0 unx     3164 b- defN 23-Jul-20 12:44 dmm/time_bucket.py
+-rw-r--r--  2.0 unx     1690 b- defN 23-Jul-20 12:44 dmm/utils.py
+-rw-r--r--  2.0 unx      525 b- defN 23-Jul-20 12:44 dmm/data_source/__init__.py
+-rw-r--r--  2.0 unx     2096 b- defN 23-Jul-20 12:44 dmm/data_source/data_source_base.py
+-rw-r--r--  2.0 unx     2698 b- defN 23-Jul-20 12:44 dmm/data_source/dataframe_source.py
+-rw-r--r--  2.0 unx    60597 b- defN 23-Jul-20 12:44 dmm/data_source/datarobot_source.py
+-rw-r--r--  2.0 unx     3003 b- defN 23-Jul-20 12:44 dmm/data_source/generator_source.py
+-rw-r--r--  2.0 unx      396 b- defN 23-Jul-20 12:44 dmm/metric/__init__.py
+-rw-r--r--  2.0 unx     1620 b- defN 23-Jul-20 12:44 dmm/metric/asymmetric_error.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Jul-20 12:44 dmm/metric/median_absolute_error.py
+-rw-r--r--  2.0 unx     3569 b- defN 23-Jul-20 12:44 dmm/metric/metric_base.py
+-rw-r--r--  2.0 unx      957 b- defN 23-Jul-20 12:44 dmm/metric/missing_values.py
+-rw-r--r--  2.0 unx     2014 b- defN 23-Jul-20 12:44 dmm/metric/sklearn_metric.py
+-rw-r--r--  2.0 unx      886 b- defN 23-Jul-20 12:46 datarobot_model_metrics-0.1.7.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Jul-20 12:46 datarobot_model_metrics-0.1.7.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-Jul-20 12:46 datarobot_model_metrics-0.1.7.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1857 b- defN 23-Jul-20 12:46 datarobot_model_metrics-0.1.7.dist-info/RECORD
+22 files, 117523 bytes uncompressed, 26222 bytes compressed:  77.7%
```

## zipnote {}

```diff
@@ -48,20 +48,20 @@
 
 Filename: dmm/metric/missing_values.py
 Comment: 
 
 Filename: dmm/metric/sklearn_metric.py
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.6.dist-info/METADATA
+Filename: datarobot_model_metrics-0.1.7.dist-info/METADATA
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.6.dist-info/WHEEL
+Filename: datarobot_model_metrics-0.1.7.dist-info/WHEEL
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.6.dist-info/top_level.txt
+Filename: datarobot_model_metrics-0.1.7.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.6.dist-info/RECORD
+Filename: datarobot_model_metrics-0.1.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dmm/metric_evaluator.py

```diff
@@ -194,29 +194,36 @@
             chunk_of_data = chunk_of_data.loc[
                 chunk_of_data[self._segment_attribute] == self._segment_value
             ]
 
         self._stats.total_rows += len(chunk_of_data)
         return chunk_of_data, time_bucket_id, done
 
-    def _validate_data_chunk(self, data_chunk: pd.DataFrame) -> pd.DataFrame:
+    def _validate_data_chunk(
+        self, data_chunk: pd.DataFrame
+    ) -> Tuple[pd.DataFrame, bool]:
+        is_valid = True
         for metric_name, metric in self._metrics.items():
             if metric.need_predictions() and self._prediction_col not in data_chunk:
-                raise Exception(
+                logger.warning(
                     f"Metric {metric_name} requires predictions, but data chunk is missing column "
-                    f"{self._prediction_col}, columns present: {', '.join(data_chunk.columns)}"
+                    f"'{self._prediction_col}'"
                 )
+                is_valid = False
+                return data_chunk, is_valid
             if metric.need_actuals() and self._actuals_col not in data_chunk:
-                raise Exception(
+                logger.warning(
                     f"Metric {metric_name} requires actuals, but data chunk is missing column "
-                    f"({self._actuals_col}), columns present: {', '.join(data_chunk.columns)}"
+                    f"'{self._actuals_col}'"
                 )
+                is_valid = False
+                return data_chunk, is_valid
         if self._groups_to_filter:
             data_chunk = self._drop_missing_values_from_data_chunk(data_chunk)
-        return data_chunk
+        return data_chunk, is_valid
 
     def _drop_missing_values_from_data_chunk(
         self, data_chunk: pd.DataFrame
     ) -> pd.DataFrame:
         """
         Remove missing values from a chunk of data based on selected groups to be filtered.
         """
@@ -322,17 +329,22 @@
                 prev_time_bucket_id = time_bucket_id
 
             # Running the score and keeping the value until it is time to reduce
             # If we are done - we will not call the transform.
             if done:
                 continue
 
-            data_chunk = self._validate_data_chunk(data_chunk)
+            data_chunk, is_valid = self._validate_data_chunk(data_chunk)
+            if not is_valid:
+                logger.warning(
+                    "data chunk does not require all the necessary columns, skipping scoring..."
+                )
+                continue
             if data_chunk.index.size == 0:
-                logger.warning(f"data chunk is empty, skipping scoring...")
+                logger.warning("data chunk is empty, skipping scoring...")
                 continue
 
             for metric_name, value in self._run_score(data_chunk).items():
                 time_bucket_scores[metric_name].append(value)
             time_bucket_metric_nr_samples += data_chunk.index.size
             time_bucket_timestamp = data_chunk[self._timestamp_col].iat[0]
```

## dmm/data_source/datarobot_source.py

```diff
@@ -46,24 +46,26 @@
         self,
         base_url: str,
         token: str,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: str = None,
-        max_rows: int = 100,
+        max_rows: int = 10000,
+        delete_exports: bool = False,
     ):
         super().__init__(max_rows)
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         self._deployment_id = deployment_id
         self._start = start
         self._end = end
         self._model_id = model_id
+        self._delete_exports = delete_exports
         self._current_chunk_id = 0
         self._prev_chunk_datetime = None
         self._actuals_caches = {}
 
         api = DataRobotApiClient(token, base_url)
         self._api = api
         self._deployment = Deployment(self._api, deployment_id)
@@ -87,22 +89,26 @@
         """
         Resets the current state of data exports to the beginning of the data.
         """
         self._prediction_data_provider = self._get_prediction_data_provider()
         self._actuals_data_provider = self._get_actuals_data_provider()
         self._actuals_caches = {}
 
-    def get_actuals_data(self) -> (pd.DataFrame, int):
+    def get_actuals_data(
+        self, return_original_column_names: bool = False
+    ) -> (pd.DataFrame, int):
         """
         Method to return a chunk of actuals data that can be sent to a metric object to be transformed.
         :return:
             - DataFrame: if there is more data to process, or None
             - int: ID of the time bucket
         """
-        actuals_df, chunk_id = self._actuals_data_provider.get_data()
+        actuals_df, chunk_id = self._actuals_data_provider.get_data(
+            return_original_column_names
+        )
         return actuals_df, chunk_id
 
     def get_all_actuals_data(self) -> pd.DataFrame:
         """
         Returns all actuals data available for that source object in a single DataFrame.
         :return:
             DataFrame: Actuals data for all chunks.
@@ -175,14 +181,15 @@
             )
             actuals_data_provider = ActualsDataExportProvider(
                 api=self._api,
                 deployment_id=self._deployment_id,
                 start=actuals_export_start,
                 end=actuals_export_end,
                 max_rows=self.max_rows,
+                delete_exports=self._delete_exports,
             )
             actuals_df = self._get_corresponding_actuals(actuals_data_provider)
             actuals_df_from_caches = self._get_actuals_df_from_caches()
             if not actuals_df.empty:
                 self._update_actuals_caches(actuals_df)
             actuals_df = pd.concat([actuals_df, actuals_df_from_caches])
 
@@ -211,28 +218,30 @@
             api=self._api,
             deployment_id=self._deployment_id,
             start=self._start,
             end=self._end,
             model_id=self._model_id,
             time_bucket=self._time_bucket,
             max_rows=self.max_rows,
+            delete_exports=self._delete_exports,
         )
 
     def _get_prediction_data_provider(self) -> PredictionDataExportProvider:
         """
         Retrieves a new instance of the PredictionDataExportProvider class.
         """
         return PredictionDataExportProvider(
             api=self._api,
             deployment_id=self._deployment_id,
             start=self._start,
             end=self._end,
             model_id=self._model_id,
             time_bucket=self.time_bucket,
             max_rows=self.max_rows,
+            delete_exports=self._delete_exports,
         )
 
     def _get_training_data_provider(self) -> TrainingDataExportProvider:
         """
         Retrieves a new instance of the TrainingDataExportProvider class.
         """
         return TrainingDataExportProvider(
@@ -411,15 +420,16 @@
     def __init__(
         self,
         base_url: str,
         token: str,
         deployment_id: str,
         batch_ids: List[str],
         model_id: str = None,
-        max_rows: int = 100,
+        max_rows: int = 10000,
+        delete_exports: bool = False,
     ):
         super().__init__(max_rows)
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         self._deployment_id = deployment_id
         self._model_id = model_id
@@ -428,14 +438,15 @@
         self._batch_ids = batch_ids
 
         api = DataRobotApiClient(token, base_url)
         self._api = api
         self._deployment = Deployment(self._api, deployment_id)
         self._start = self._deployment.created_at
         self._end = datetime.datetime.utcnow().replace(tzinfo=pytz.UTC)
+        self._delete_exports = delete_exports
         self._prediction_data_provider = self._get_prediction_data_provider()
 
     def init(self, time_bucket: TimeBucket):
         pass
 
     def reset(self):
         self._prediction_data_provider = self._get_prediction_data_provider()
@@ -487,14 +498,15 @@
             api=self._api,
             deployment_id=self._deployment_id,
             start=self._start,
             end=self._end,
             model_id=self._model_id,
             max_rows=self.max_rows,
             batch_ids=self._batch_ids,
+            delete_exports=self._delete_exports,
         )
 
     def _format_prediction_df(self, pred_df: pd.DataFrame) -> pd.DataFrame:
         """
         Formats data from prediction data export, before merging with actuals.
         Renames the columns to standardize the output names.
 
@@ -579,26 +591,28 @@
         start: datetime.datetime,
         end: datetime.datetime,
         timestamp_col: str,
         max_rows: int,
         time_bucket: TimeBucket,
         start_export: Callable,
         get_export_dataset_ids: Callable,
+        delete_exports: bool,
     ):
         self._api = api
         self._deployment_id = deployment_id
         self._model_id = model_id
         self._deployment = Deployment(self._api, deployment_id)
         self._start = start
         self._end = end
         self._time_bucket = time_bucket
         self._max_rows = max_rows
         self._timestamp_col = timestamp_col
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
+        self._delete_exports = delete_exports
         self._time_bucket_chunks = self._get_time_bucket_chunks()
 
         self._current_chunk_id = 0
         self._prev_bucket_id = None
 
     def set_new_export_parameters(
         self,
@@ -629,14 +643,15 @@
                         deployment_id=self._deployment_id,
                         model_id=self._model_id,
                         start_dt=self._start,
                         end_dt=self._end,
                         start_export=self._start_export,
                         get_export_dataset_ids=self._get_export_dataset_ids,
                         sort_column=self._timestamp_col,
+                        delete_exports=self._delete_exports,
                     )
                 )
             ),
         )
 
     def _update_chunk_info(self, bucket_id: any):
         """
@@ -660,16 +675,17 @@
         self,
         api: DataRobotApiClient,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: str = None,
         timestamp_col: str = ColumnName.DR_TIMESTAMP_COLUMN,
-        max_rows: int = 100,
+        max_rows: int = 10000,
         time_bucket: TimeBucket = TimeBucket.ALL,
+        delete_exports: bool = False,
     ):
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         super().__init__(
             api=api,
             deployment_id=deployment_id,
@@ -677,14 +693,15 @@
             start=start,
             end=end,
             timestamp_col=timestamp_col,
             max_rows=max_rows,
             time_bucket=time_bucket,
             start_export=api.start_prediction_data_export,
             get_export_dataset_ids=api.get_prediction_export_dataset_ids,
+            delete_exports=delete_exports,
         )
 
     def get_data(self) -> (pd.DataFrame, int):
         """
         Method to return a chunk of prediction data that can be sent to a metric object to be transformed.
         :return:
             - DataFrame: if there is more data to process, or None
@@ -721,24 +738,26 @@
         deployment_id: str,
         model_id: str | None,
         start: datetime.datetime,
         end: datetime.datetime,
         max_rows: int,
         start_export: Callable,
         get_export_dataset_ids: Callable,
+        delete_exports: bool,
     ):
         self._api = api
         self._deployment_id = deployment_id
         self._model_id = model_id
         self._deployment = Deployment(self._api, deployment_id)
         self._start = start
         self._end = end
         self._max_rows = max_rows
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
+        self._delete_exports = delete_exports
         self._batch_bucket_chunks = self._get_batch_bucket_chunks()
 
         self._current_chunk_id = 0
         self._prev_bucket_id = None
 
     def set_new_export_parameters(self, max_rows: int) -> DataRobotBatchExportBase:
         self._max_rows = max_rows
@@ -759,14 +778,15 @@
                         api=self._api,
                         deployment_id=self._deployment_id,
                         model_id=self._model_id,
                         start_dt=self._start,
                         end_dt=self._end,
                         start_export=self._start_export,
                         get_export_dataset_ids=self._get_export_dataset_ids,
+                        delete_exports=self._delete_exports,
                     )
                 )
             ),
         )
 
     def _update_chunk_info(self, bucket_id: str):
         """
@@ -788,15 +808,16 @@
         self,
         api: DataRobotApiClient,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         batch_ids: List[str],
         model_id: str = None,
-        max_rows: int = 100,
+        max_rows: int = 10000,
+        delete_exports: bool = False,
     ):
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         super().__init__(
             api=api,
             deployment_id=deployment_id,
@@ -804,14 +825,15 @@
             start=start,
             end=end,
             max_rows=max_rows,
             start_export=functools.partial(
                 api.start_batch_prediction_data_export, batch_ids
             ),
             get_export_dataset_ids=api.get_prediction_export_dataset_ids,
+            delete_exports=delete_exports,
         )
 
     def get_data(self) -> (pd.DataFrame, int):
         """
         Method to return a chunk of prediction data that can be sent to a metric object to be transformed.
         :return:
             - DataFrame: if there is more data to process, or None
@@ -836,16 +858,17 @@
         self,
         api: DataRobotApiClient,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: str = None,
         timestamp_col: str = ColumnName.TIMESTAMP,
-        max_rows: int = 100,
+        max_rows: int = 10000,
         time_bucket: TimeBucket = TimeBucket.ALL,
+        delete_exports: bool = False,
     ):
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         super().__init__(
             api=api,
             deployment_id=deployment_id,
@@ -853,17 +876,20 @@
             start=start,
             end=end,
             timestamp_col=timestamp_col,
             max_rows=max_rows,
             time_bucket=time_bucket,
             start_export=api.start_actuals_export,
             get_export_dataset_ids=api.get_actuals_export_dataset_ids,
+            delete_exports=delete_exports,
         )
 
-    def get_data(self) -> (pd.DataFrame, int):
+    def get_data(
+        self, return_original_column_names: bool = False
+    ) -> (pd.DataFrame, int):
         """
         Method to return a chunk of actuals data that can be sent to a metric object to be transformed.
         :return:
             - DataFrame: if there is more data to process, or None
             - int: ID of the time bucket
         """
         supported_time_buckets = self.get_supported_time_buckets()
@@ -879,14 +905,17 @@
         chunk_df, chunk_ts = self._time_bucket_chunks.load_time_bucket_chunk(
             time_bucket=self._time_bucket, max_rows=self._max_rows
         )
         if self._deployment.type == DeploymentType.BINARY_CLASSIFICATION:
             chunk_df = self._drop_negative_class(chunk_df)
             self._add_column_with_predicted_class(chunk_df)
 
+        if return_original_column_names:
+            chunk_df = self._rename_to_original_names(chunk_df)
+
         self._update_chunk_info(chunk_ts)
         return chunk_df, self._current_chunk_id
 
     @staticmethod
     def get_supported_time_buckets():
         return [
             TimeBucket.HOUR,
@@ -910,14 +939,24 @@
         ].map(
             {
                 True: self._deployment.positive_class_label,
                 False: self._deployment.negative_class_label,
             }
         )
 
+    def _rename_to_original_names(self, chunk_df: pd.DataFrame) -> pd.DataFrame:
+        rename_columns = {
+            ColumnName.ASSOCIATION_ID_COLUMN: self._api.get_association_id(
+                self._deployment_id
+            ),
+            ColumnName.PREDICTIONS: self._deployment.target_column,
+        }
+        chunk_df = chunk_df.rename(rename_columns, axis=1)
+        return chunk_df
+
 
 class TrainingDataExportProvider:
     def __init__(
         self,
         api: DataRobotApiClient,
         deployment_id: str,
         model_id: str = None,
@@ -1351,14 +1390,24 @@
             elif dataset.processing_state == "ERROR":
                 raise ValueError(
                     "Dataset creation failed, most likely you requested range with no predictions"
                 )
             time.sleep(5)
         raise Exception(f"Failed to fetch dataset within {max_wait_sec} seconds")
 
+    def remove_dataset_with_exported_data(self, dataset_id: str) -> None:
+        """
+        Removes a catalog item from AI catalog.
+
+        Parameters
+        ----------
+        dataset_id: str
+        """
+        self._client.delete(f"datasets/{dataset_id}")
+
 
 class DataRobotChunksIterator:
     """
     Uses DataRobot API to perform exports and read data in chunk-by-chunk fashion.
     """
 
     def __init__(
@@ -1366,24 +1415,26 @@
         api: DataRobotApiClient,
         deployment_id: str,
         model_id: str | None,
         start_dt: datetime.datetime,
         end_dt: datetime.datetime,
         start_export: Callable,
         get_export_dataset_ids: Callable,
+        delete_exports: bool = False,
         sort_column: Optional[str] = None,
     ):
         self._deployment_id = deployment_id
         self._model_id = model_id
         self._start_dt = start_dt
         self._end_dt = end_dt
         self._sort_column = sort_column
         self._api = api
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
+        self._delete_exports = delete_exports
 
     def __iter__(self) -> Generator[pd.DataFrame, None, None]:
         """
         Returns an iterator over chunks for time range defined in the constructor. It returns data sorted by timestamp.
         The way it works is as follows
         - it is a generator to lazily fetch data from DR API,
         - it will try to perform a full export (start_dt to end_dt),
@@ -1444,14 +1495,16 @@
                 except MemoryError:
                     os.remove(tempfile_path)
                     raise Exception(
                         f"We ran out of memory for interval {start_dt} - {start_dt + interval}. "
                         f"Consider fetching smaller chunks of data"
                     )
             os.remove(tempfile_path)
+            if self._delete_exports:
+                self._api.remove_dataset_with_exported_data(dataset_id)
 
         return result, interval
 
     def _get_export_response(
         self, from_dt: datetime.datetime, suggested_interval: datetime.timedelta
     ) -> (requests.Response, datetime.timedelta):
         # we try to increase interval to mitigate too aggressive shrinking
```

## Comparing `datarobot_model_metrics-0.1.6.dist-info/METADATA` & `datarobot_model_metrics-0.1.7.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobot-model-metrics
-Version: 0.1.6
+Version: 0.1.7
 Summary: datarobot-model-metrics is a framework to compute model ML metrics
 Home-page: https://github.com/datarobot/datarobot-model-metrics
 Author: DataRobot
 Author-email: info@datarobot.com
 License: DataRobot
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

## Comparing `datarobot_model_metrics-0.1.6.dist-info/RECORD` & `datarobot_model_metrics-0.1.7.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 dmm/__init__.py,sha256=w3zRSdtt2_upBZCJNj93DqZeXKPyfMKqo3Xo1yOHLeY,385
 dmm/batch_metric_evaluator.py,sha256=LmZkvxYeF8Dk98x0PWhbyWCGSqAO4cBeWAMzhyWvo50,9663
 dmm/constants.py,sha256=sNtJziqxxioScZUK_YlF08__C3zjxk16YYAOhRjAd6Y,747
 dmm/example_data_helper.py,sha256=oLcOTo-dNV0Pyapq9G6e7DZoVjfIZMWj7QrkBx61Mj4,6955
-dmm/metric_evaluator.py,sha256=3HK5k8ynCLpF5UsqD6AaFjiksbkSfWQFvHw2ei52-7E,13942
+dmm/metric_evaluator.py,sha256=0JZlhxb0Kcfx52HreUrAYDVfHP4uL2iiIldUVWhDkzM,14267
 dmm/time_bucket.py,sha256=cg42R1U_o7is6nZf5qUuvdc-2HBH9UKuACKjKe3dLto,3164
 dmm/utils.py,sha256=kaZ7erHZofSwqZL1ddd8GXK3wrZfvYZ7I0ziz2CgbL0,1690
 dmm/data_source/__init__.py,sha256=rsK8s1quedmNYchnUodsc4gUXAENM-N_rVVohhX9HMA,525
 dmm/data_source/data_source_base.py,sha256=5zJ9SZYs1ufbku11GYa4PQRY1lwOOsXbx8kDkiG1VMk,2096
 dmm/data_source/dataframe_source.py,sha256=mKEGXyNFVAsqiocAVKQlhbGlUlwjx77sl5KAlXT6nx4,2698
-dmm/data_source/datarobot_source.py,sha256=YgOtXH5uL0i_9APSDzUBJd_7HP5mQJ4WA9uDHosRsq4,58576
+dmm/data_source/datarobot_source.py,sha256=5HVm2LBZ89n_FhEoN0nAUaPFyNB2mu4ccSbtcj0XFBI,60597
 dmm/data_source/generator_source.py,sha256=O-GsreX8mQognj3u6LQxHkZ3yWj3sOgx_dojvObVdlI,3003
 dmm/metric/__init__.py,sha256=nIX41kZJKfQztTw93CXVLRbgTt5W1qhdK1uakWxL9CE,396
 dmm/metric/asymmetric_error.py,sha256=h4J7nzXw9BurtGrkoe4oe7QBI2l69XeB9CiGfpcKUxc,1620
 dmm/metric/median_absolute_error.py,sha256=qV0NpJdF6daTzJOaGpd8KAPG3gZWPS43FZxOb-rR0kE,320
 dmm/metric/metric_base.py,sha256=Rzsju5eZhoyafxo7ErFmO76sDcsvkb3SyODDwcNudy4,3569
 dmm/metric/missing_values.py,sha256=i9ujXCuOWEPrUteFXTCDGX6SM8RVd7cQoI6byQPga4E,957
 dmm/metric/sklearn_metric.py,sha256=Bv4ukOSZyOKjXK-_4b7KKmlkOVrYnwSX5GST_Hf-qpc,2014
-datarobot_model_metrics-0.1.6.dist-info/METADATA,sha256=9fHrKIBsrvlVuxDRF8jsz-CEyVURUj8lnAd2KEgjRW0,886
-datarobot_model_metrics-0.1.6.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-datarobot_model_metrics-0.1.6.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
-datarobot_model_metrics-0.1.6.dist-info/RECORD,,
+datarobot_model_metrics-0.1.7.dist-info/METADATA,sha256=a03C5GE7YeNLTBofcRoAOElqT-9ASUeuBhxhcgEDdKo,886
+datarobot_model_metrics-0.1.7.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+datarobot_model_metrics-0.1.7.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
+datarobot_model_metrics-0.1.7.dist-info/RECORD,,
```

