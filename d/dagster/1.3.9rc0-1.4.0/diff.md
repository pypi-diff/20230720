# Comparing `tmp/dagster-1.3.9rc0.tar.gz` & `tmp/dagster-1.4.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dagster-1.3.9rc0.tar", last modified: Thu Jun  8 18:21:09 2023, max compression
+gzip compressed data, was "dagster-1.4.0.tar", last modified: Thu Jul 20 21:53:51 2023, max compression
```

## Comparing `dagster-1.3.9rc0.tar` & `dagster-1.4.0.tar`

### file list

```diff
@@ -1,634 +1,634 @@
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.092564 dagster-1.3.9rc0/
--rw-r--r--   0 root         (0) root         (0)      549 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/COPYING
--rw-r--r--   0 root         (0) root         (0)    11344 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/LICENSE
--rw-r--r--   0 root         (0) root         (0)      485 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/MANIFEST.in
--rw-r--r--   0 root         (0) root         (0)     8793 2023-06-08 18:21:09.092564 dagster-1.3.9rc0/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)     7167 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/README.md
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.024563 dagster-1.3.9rc0/dagster/
--rw-r--r--   0 root         (0) root         (0)    26213 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/__init__.py
--rw-r--r--   0 root         (0) root         (0)       31 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/__main__.py
--rw-r--r--   0 root         (0) root         (0)     5456 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_annotations.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.024563 dagster-1.3.9rc0/dagster/_api/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/__init__.py
--rw-r--r--   0 root         (0) root         (0)      731 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/get_server_id.py
--rw-r--r--   0 root         (0) root         (0)     2147 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/list_repositories.py
--rw-r--r--   0 root         (0) root         (0)      531 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/notebook_data.py
--rw-r--r--   0 root         (0) root         (0)     2882 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/snapshot_execution_plan.py
--rw-r--r--   0 root         (0) root         (0)     1664 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/snapshot_job.py
--rw-r--r--   0 root         (0) root         (0)     5479 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/snapshot_partition.py
--rw-r--r--   0 root         (0) root         (0)     1668 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/snapshot_repository.py
--rw-r--r--   0 root         (0) root         (0)     2727 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/snapshot_schedule.py
--rw-r--r--   0 root         (0) root         (0)     2901 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_api/snapshot_sensor.py
--rw-r--r--   0 root         (0) root         (0)      478 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_builtins.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.024563 dagster-1.3.9rc0/dagster/_check/
--rw-r--r--   0 root         (0) root         (0)     1352 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_check/README.md
--rw-r--r--   0 root         (0) root         (0)    51637 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_check/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.028563 dagster-1.3.9rc0/dagster/_cli/
--rw-r--r--   0 root         (0) root         (0)     1182 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/__init__.py
--rw-r--r--   0 root         (0) root         (0)    26292 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/api.py
--rw-r--r--   0 root         (0) root         (0)     8263 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/asset.py
--rw-r--r--   0 root         (0) root         (0)     7730 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/code_server.py
--rw-r--r--   0 root         (0) root         (0)     2300 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/config_scaffolder.py
--rw-r--r--   0 root         (0) root         (0)     3540 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/debug.py
--rw-r--r--   0 root         (0) root         (0)     5867 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/dev.py
--rw-r--r--   0 root         (0) root         (0)     4154 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/instance.py
--rw-r--r--   0 root         (0) root         (0)    29907 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/job.py
--rw-r--r--   0 root         (0) root         (0)     1695 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/load_handle.py
--rw-r--r--   0 root         (0) root         (0)     5852 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/project.py
--rw-r--r--   0 root         (0) root         (0)     5129 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/run.py
--rw-r--r--   0 root         (0) root         (0)    19958 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/schedule.py
--rw-r--r--   0 root         (0) root         (0)    15707 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/sensor.py
--rw-r--r--   0 root         (0) root         (0)     4259 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.028563 dagster-1.3.9rc0/dagster/_cli/workspace/
--rw-r--r--   0 root         (0) root         (0)      180 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/workspace/__init__.py
--rw-r--r--   0 root         (0) root         (0)    28391 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_cli/workspace/cli_target.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.028563 dagster-1.3.9rc0/dagster/_config/
--rw-r--r--   0 root         (0) root         (0)     3186 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3403 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/config_schema.py
--rw-r--r--   0 root         (0) root         (0)    15864 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/config_type.py
--rw-r--r--   0 root         (0) root         (0)    19601 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/errors.py
--rw-r--r--   0 root         (0) root         (0)     1783 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/evaluate_value_result.py
--rw-r--r--   0 root         (0) root         (0)    15269 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/field.py
--rw-r--r--   0 root         (0) root         (0)    16880 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/field_utils.py
--rw-r--r--   0 root         (0) root         (0)     9466 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/post_process.py
--rw-r--r--   0 root         (0) root         (0)      855 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/primitive_mapping.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.028563 dagster-1.3.9rc0/dagster/_config/pythonic_config/
--rw-r--r--   0 root         (0) root         (0)    71995 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/pythonic_config/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1644 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/pythonic_config/attach_other_object_to_context.py
--rw-r--r--   0 root         (0) root         (0)     7261 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/pythonic_config/typing_utils.py
--rw-r--r--   0 root         (0) root         (0)      577 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/pythonic_config/utils.py
--rw-r--r--   0 root         (0) root         (0)    12143 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/snap.py
--rw-r--r--   0 root         (0) root         (0)     3267 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/source.py
--rw-r--r--   0 root         (0) root         (0)     3528 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/stack.py
--rw-r--r--   0 root         (0) root         (0)     7772 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/traversal_context.py
--rw-r--r--   0 root         (0) root         (0)     4167 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/type_printer.py
--rw-r--r--   0 root         (0) root         (0)    17162 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_config/validate.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.032563 dagster-1.3.9rc0/dagster/_core/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/__init__.py
--rw-r--r--   0 root         (0) root         (0)      994 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/assets.py
--rw-r--r--   0 root         (0) root         (0)    13645 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/code_pointer.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.032563 dagster-1.3.9rc0/dagster/_core/container_context/
--rw-r--r--   0 root         (0) root         (0)      184 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/container_context/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1277 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/container_context/config.py
--rw-r--r--   0 root         (0) root         (0)     2310 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/debug.py
--rw-r--r--   0 root         (0) root         (0)     3005 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/decorator_utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.040563 dagster-1.3.9rc0/dagster/_core/definitions/
--rw-r--r--   0 root         (0) root         (0)     7626 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/__init__.py
--rw-r--r--   0 root         (0) root         (0)    30543 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_graph.py
--rw-r--r--   0 root         (0) root         (0)    10704 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_graph_subset.py
--rw-r--r--   0 root         (0) root         (0)     3816 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_in.py
--rw-r--r--   0 root         (0) root         (0)    37134 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_layer.py
--rw-r--r--   0 root         (0) root         (0)     5685 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_out.py
--rw-r--r--   0 root         (0) root         (0)    59306 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_reconciliation_sensor.py
--rw-r--r--   0 root         (0) root         (0)    18474 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_selection.py
--rw-r--r--   0 root         (0) root         (0)     7164 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/asset_sensor_definition.py
--rw-r--r--   0 root         (0) root         (0)    63783 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/assets.py
--rw-r--r--   0 root         (0) root         (0)    24376 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/assets_job.py
--rw-r--r--   0 root         (0) root         (0)     2806 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/auto_materialize_condition.py
--rw-r--r--   0 root         (0) root         (0)     5297 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/auto_materialize_policy.py
--rw-r--r--   0 root         (0) root         (0)    16105 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/cacheable_assets.py
--rw-r--r--   0 root         (0) root         (0)    45671 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/composition.py
--rw-r--r--   0 root         (0) root         (0)     4287 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/config.py
--rw-r--r--   0 root         (0) root         (0)    10845 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/configurable.py
--rw-r--r--   0 root         (0) root         (0)    20795 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/data_time.py
--rw-r--r--   0 root         (0) root         (0)    18695 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/data_version.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.044564 dagster-1.3.9rc0/dagster/_core/definitions/decorators/
--rw-r--r--   0 root         (0) root         (0)      620 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/__init__.py
--rw-r--r--   0 root         (0) root         (0)    44389 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/asset_decorator.py
--rw-r--r--   0 root         (0) root         (0)     4915 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/config_mapping_decorator.py
--rw-r--r--   0 root         (0) root         (0)     8250 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/graph_decorator.py
--rw-r--r--   0 root         (0) root         (0)     9444 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/hook_decorator.py
--rw-r--r--   0 root         (0) root         (0)    10676 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/job_decorator.py
--rw-r--r--   0 root         (0) root         (0)    17845 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/op_decorator.py
--rw-r--r--   0 root         (0) root         (0)    14396 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/repository_decorator.py
--rw-r--r--   0 root         (0) root         (0)     8656 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/schedule_decorator.py
--rw-r--r--   0 root         (0) root         (0)    11936 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/sensor_decorator.py
--rw-r--r--   0 root         (0) root         (0)     7085 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/decorators/source_asset_decorator.py
--rw-r--r--   0 root         (0) root         (0)     5275 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/definition_config_schema.py
--rw-r--r--   0 root         (0) root         (0)    21205 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/definitions_class.py
--rw-r--r--   0 root         (0) root         (0)    39988 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/dependency.py
--rw-r--r--   0 root         (0) root         (0)    31576 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/events.py
--rw-r--r--   0 root         (0) root         (0)    21013 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/executor_definition.py
--rw-r--r--   0 root         (0) root         (0)    10548 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/external_asset_graph.py
--rw-r--r--   0 root         (0) root         (0)     8010 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/freshness_policy.py
--rw-r--r--   0 root         (0) root         (0)    16195 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/freshness_policy_sensor_definition.py
--rw-r--r--   0 root         (0) root         (0)    44740 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/graph_definition.py
--rw-r--r--   0 root         (0) root         (0)     6546 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/hook_definition.py
--rw-r--r--   0 root         (0) root         (0)     1515 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/hook_invocation.py
--rw-r--r--   0 root         (0) root         (0)     3205 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/inference.py
--rw-r--r--   0 root         (0) root         (0)    22898 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/input.py
--rw-r--r--   0 root         (0) root         (0)     5386 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/instigation_logger.py
--rw-r--r--   0 root         (0) root         (0)     2152 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/job_base.py
--rw-r--r--   0 root         (0) root         (0)    51157 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/job_definition.py
--rw-r--r--   0 root         (0) root         (0)    20308 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/load_assets_from_modules.py
--rw-r--r--   0 root         (0) root         (0)     6845 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/logger_definition.py
--rw-r--r--   0 root         (0) root         (0)      636 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/logger_invocation.py
--rw-r--r--   0 root         (0) root         (0)     8841 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/materialize.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.044564 dagster-1.3.9rc0/dagster/_core/definitions/metadata/
--rw-r--r--   0 root         (0) root         (0)    32319 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/metadata/__init__.py
--rw-r--r--   0 root         (0) root         (0)     8557 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/metadata/table.py
--rw-r--r--   0 root         (0) root         (0)    56090 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/multi_asset_sensor_definition.py
--rw-r--r--   0 root         (0) root         (0)    20153 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/multi_dimensional_partitions.py
--rw-r--r--   0 root         (0) root         (0)       82 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/no_step_launcher.py
--rw-r--r--   0 root         (0) root         (0)    11927 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/node_container.py
--rw-r--r--   0 root         (0) root         (0)     8041 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/node_definition.py
--rw-r--r--   0 root         (0) root         (0)     2867 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/observe.py
--rw-r--r--   0 root         (0) root         (0)    22119 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/op_definition.py
--rw-r--r--   0 root         (0) root         (0)    19256 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/op_invocation.py
--rw-r--r--   0 root         (0) root         (0)     7509 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/op_selection.py
--rw-r--r--   0 root         (0) root         (0)    18908 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/output.py
--rw-r--r--   0 root         (0) root         (0)    39191 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/partition.py
--rw-r--r--   0 root         (0) root         (0)      196 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/partition_key_range.py
--rw-r--r--   0 root         (0) root         (0)    47274 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/partition_mapping.py
--rw-r--r--   0 root         (0) root         (0)     8902 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/partitioned_schedule.py
--rw-r--r--   0 root         (0) root         (0)     3779 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/policy.py
--rw-r--r--   0 root         (0) root         (0)    27231 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/reconstruct.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.044564 dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/
--rw-r--r--   0 root         (0) root         (0)      654 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6670 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/caching_index.py
--rw-r--r--   0 root         (0) root         (0)    18860 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/repository_data.py
--rw-r--r--   0 root         (0) root         (0)    17401 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/repository_data_builder.py
--rw-r--r--   0 root         (0) root         (0)    16917 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/repository_definition.py
--rw-r--r--   0 root         (0) root         (0)     1479 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/valid_definitions.py
--rw-r--r--   0 root         (0) root         (0)     5108 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/resolved_asset_deps.py
--rw-r--r--   0 root         (0) root         (0)     1336 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/resource_annotation.py
--rw-r--r--   0 root         (0) root         (0)    16162 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/resource_definition.py
--rw-r--r--   0 root         (0) root         (0)     5398 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/resource_invocation.py
--rw-r--r--   0 root         (0) root         (0)     7806 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/resource_requirement.py
--rw-r--r--   0 root         (0) root         (0)    24105 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/run_config.py
--rw-r--r--   0 root         (0) root         (0)     1445 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/run_config_schema.py
--rw-r--r--   0 root         (0) root         (0)    15824 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/run_request.py
--rw-r--r--   0 root         (0) root         (0)    38400 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/run_status_sensor_definition.py
--rw-r--r--   0 root         (0) root         (0)    37556 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/schedule_definition.py
--rw-r--r--   0 root         (0) root         (0)     5189 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/scoped_resources_builder.py
--rw-r--r--   0 root         (0) root         (0)    10837 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/selector.py
--rw-r--r--   0 root         (0) root         (0)    47376 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/sensor_definition.py
--rw-r--r--   0 root         (0) root         (0)    15391 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/source_asset.py
--rw-r--r--   0 root         (0) root         (0)     2298 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/step_launcher.py
--rw-r--r--   0 root         (0) root         (0)     1535 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/target.py
--rw-r--r--   0 root         (0) root         (0)      473 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/test_op_definition.py
--rw-r--r--   0 root         (0) root         (0)    12374 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/time_window_partition_mapping.py
--rw-r--r--   0 root         (0) root         (0)    78072 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/time_window_partitions.py
--rw-r--r--   0 root         (0) root         (0)    16744 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/unresolved_asset_job_definition.py
--rw-r--r--   0 root         (0) root         (0)     7992 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/utils.py
--rw-r--r--   0 root         (0) root         (0)     2930 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/definitions/version_strategy.py
--rw-r--r--   0 root         (0) root         (0)    26237 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/errors.py
--rw-r--r--   0 root         (0) root         (0)     6241 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/event_api.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.044564 dagster-1.3.9rc0/dagster/_core/events/
--rw-r--r--   0 root         (0) root         (0)    64981 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/events/__init__.py
--rw-r--r--   0 root         (0) root         (0)     7783 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/events/log.py
--rw-r--r--   0 root         (0) root         (0)     1614 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/events/utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.048564 dagster-1.3.9rc0/dagster/_core/execution/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/__init__.py
--rw-r--r--   0 root         (0) root         (0)    38315 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/api.py
--rw-r--r--   0 root         (0) root         (0)    36589 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/asset_backfill.py
--rw-r--r--   0 root         (0) root         (0)    14710 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/backfill.py
--rw-r--r--   0 root         (0) root         (0)     6487 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/build_resources.py
--rw-r--r--   0 root         (0) root         (0)      224 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/bulk_actions.py
--rw-r--r--   0 root         (0) root         (0)     5587 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/compute_logs.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.048564 dagster-1.3.9rc0/dagster/_core/execution/context/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/__init__.py
--rw-r--r--   0 root         (0) root         (0)    21992 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/compute.py
--rw-r--r--   0 root         (0) root         (0)    15991 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/hook.py
--rw-r--r--   0 root         (0) root         (0)     9369 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/init.py
--rw-r--r--   0 root         (0) root         (0)    26845 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/input.py
--rw-r--r--   0 root         (0) root         (0)    27349 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/invocation.py
--rw-r--r--   0 root         (0) root         (0)     3164 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/logger.py
--rw-r--r--   0 root         (0) root         (0)    33974 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/output.py
--rw-r--r--   0 root         (0) root         (0)    44193 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context/system.py
--rw-r--r--   0 root         (0) root         (0)    18501 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/context_creation_job.py
--rw-r--r--   0 root         (0) root         (0)     5149 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/execute_in_process.py
--rw-r--r--   0 root         (0) root         (0)     5426 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/execute_in_process_result.py
--rw-r--r--   0 root         (0) root         (0)     9183 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/execution_result.py
--rw-r--r--   0 root         (0) root         (0)     8544 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/host_mode.py
--rw-r--r--   0 root         (0) root         (0)    14451 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/job_backfill.py
--rw-r--r--   0 root         (0) root         (0)     6487 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/job_execution_result.py
--rw-r--r--   0 root         (0) root         (0)      998 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/memoization.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.052564 dagster-1.3.9rc0/dagster/_core/execution/plan/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/__init__.py
--rw-r--r--   0 root         (0) root         (0)    25252 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/active.py
--rw-r--r--   0 root         (0) root         (0)     7279 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/compute.py
--rw-r--r--   0 root         (0) root         (0)    12550 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/compute_generator.py
--rw-r--r--   0 root         (0) root         (0)    16658 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/execute_plan.py
--rw-r--r--   0 root         (0) root         (0)    27355 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/execute_step.py
--rw-r--r--   0 root         (0) root         (0)    10000 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/external_step.py
--rw-r--r--   0 root         (0) root         (0)     3664 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/handle.py
--rw-r--r--   0 root         (0) root         (0)    37831 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/inputs.py
--rw-r--r--   0 root         (0) root         (0)     4352 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/instance_concurrency_context.py
--rw-r--r--   0 root         (0) root         (0)     1159 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/local_external_step_main.py
--rw-r--r--   0 root         (0) root         (0)     5395 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/objects.py
--rw-r--r--   0 root         (0) root         (0)     7057 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/outputs.py
--rw-r--r--   0 root         (0) root         (0)    58041 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/plan.py
--rw-r--r--   0 root         (0) root         (0)      114 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/resume_retry.py
--rw-r--r--   0 root         (0) root         (0)    15616 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/state.py
--rw-r--r--   0 root         (0) root         (0)    15920 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/step.py
--rw-r--r--   0 root         (0) root         (0)     3796 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/plan/utils.py
--rw-r--r--   0 root         (0) root         (0)     1762 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/poll_compute_logs.py
--rw-r--r--   0 root         (0) root         (0)     8186 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/resolve_versions.py
--rw-r--r--   0 root         (0) root         (0)    19280 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/resources_init.py
--rw-r--r--   0 root         (0) root         (0)     2104 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/retries.py
--rw-r--r--   0 root         (0) root         (0)     1651 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/run_cancellation_thread.py
--rw-r--r--   0 root         (0) root         (0)    10329 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/stats.py
--rw-r--r--   0 root         (0) root         (0)     1125 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/tags.py
--rw-r--r--   0 root         (0) root         (0)     1172 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/validate_run_config.py
--rw-r--r--   0 root         (0) root         (0)     1282 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/watch_orphans.py
--rw-r--r--   0 root         (0) root         (0)     4233 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/execution/with_resources.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.052564 dagster-1.3.9rc0/dagster/_core/executor/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1265 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/base.py
--rw-r--r--   0 root         (0) root         (0)     5990 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/child_process_executor.py
--rw-r--r--   0 root         (0) root         (0)     3431 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/in_process.py
--rw-r--r--   0 root         (0) root         (0)     1509 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/init.py
--rw-r--r--   0 root         (0) root         (0)    15592 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/multiprocess.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.052564 dagster-1.3.9rc0/dagster/_core/executor/step_delegating/
--rw-r--r--   0 root         (0) root         (0)      247 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/step_delegating/__init__.py
--rw-r--r--   0 root         (0) root         (0)    15580 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/step_delegating/step_delegating_executor.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.052564 dagster-1.3.9rc0/dagster/_core/executor/step_delegating/step_handler/
--rw-r--r--   0 root         (0) root         (0)      152 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/step_delegating/step_handler/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3078 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/executor/step_delegating/step_handler/base.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.052564 dagster-1.3.9rc0/dagster/_core/host_representation/
--rw-r--r--   0 root         (0) root         (0)     2789 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/__init__.py
--rw-r--r--   0 root         (0) root         (0)    33518 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/code_location.py
--rw-r--r--   0 root         (0) root         (0)    32089 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/external.py
--rw-r--r--   0 root         (0) root         (0)    70934 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/external_data.py
--rw-r--r--   0 root         (0) root         (0)    12517 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/grpc_server_registry.py
--rw-r--r--   0 root         (0) root         (0)     1487 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/grpc_server_state_subscriber.py
--rw-r--r--   0 root         (0) root         (0)     4333 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/handle.py
--rw-r--r--   0 root         (0) root         (0)     1581 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/historical.py
--rw-r--r--   0 root         (0) root         (0)     4850 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/job_index.py
--rw-r--r--   0 root         (0) root         (0)    19051 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/origin.py
--rw-r--r--   0 root         (0) root         (0)     3610 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/host_representation/represented.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.052564 dagster-1.3.9rc0/dagster/_core/instance/
--rw-r--r--   0 root         (0) root         (0)   104144 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/instance/__init__.py
--rw-r--r--   0 root         (0) root         (0)    12956 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/instance/config.py
--rw-r--r--   0 root         (0) root         (0)    24084 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/instance/ref.py
--rw-r--r--   0 root         (0) root         (0)     3899 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/instance_for_test.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.056564 dagster-1.3.9rc0/dagster/_core/launcher/
--rw-r--r--   0 root         (0) root         (0)      297 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/launcher/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3758 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/launcher/base.py
--rw-r--r--   0 root         (0) root         (0)     6808 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/launcher/default_run_launcher.py
--rw-r--r--   0 root         (0) root         (0)     1586 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/launcher/sync_in_memory_run_launcher.py
--rw-r--r--   0 root         (0) root         (0)      473 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/libraries.py
--rw-r--r--   0 root         (0) root         (0)    17249 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/log_manager.py
--rw-r--r--   0 root         (0) root         (0)     1029 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/nux.py
--rw-r--r--   0 root         (0) root         (0)     3691 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/origin.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.056564 dagster-1.3.9rc0/dagster/_core/run_coordinator/
--rw-r--r--   0 root         (0) root         (0)      267 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/run_coordinator/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2005 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/run_coordinator/base.py
--rw-r--r--   0 root         (0) root         (0)     1922 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/run_coordinator/default_run_coordinator.py
--rw-r--r--   0 root         (0) root         (0)    11030 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/run_coordinator/queued_run_coordinator.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.056564 dagster-1.3.9rc0/dagster/_core/scheduler/
--rw-r--r--   0 root         (0) root         (0)      534 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/scheduler/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1241 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/scheduler/execution.py
--rw-r--r--   0 root         (0) root         (0)    21903 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/scheduler/instigation.py
--rw-r--r--   0 root         (0) root         (0)     9410 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/scheduler/scheduler.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.056564 dagster-1.3.9rc0/dagster/_core/secrets/
--rw-r--r--   0 root         (0) root         (0)       51 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/secrets/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1811 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/secrets/env_file.py
--rw-r--r--   0 root         (0) root         (0)      388 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/secrets/loader.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.056564 dagster-1.3.9rc0/dagster/_core/selector/
--rw-r--r--   0 root         (0) root         (0)      295 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/selector/__init__.py
--rw-r--r--   0 root         (0) root         (0)    18247 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/selector/subset_selector.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.056564 dagster-1.3.9rc0/dagster/_core/snap/
--rw-r--r--   0 root         (0) root         (0)     2815 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/__init__.py
--rw-r--r--   0 root         (0) root         (0)      494 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/config_types.py
--rw-r--r--   0 root         (0) root         (0)     3999 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/dagster_types.py
--rw-r--r--   0 root         (0) root         (0)     9421 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/dep_snapshot.py
--rw-r--r--   0 root         (0) root         (0)    12099 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/execution_plan_snapshot.py
--rw-r--r--   0 root         (0) root         (0)    16654 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/job_snapshot.py
--rw-r--r--   0 root         (0) root         (0)     4495 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/mode.py
--rw-r--r--   0 root         (0) root         (0)    14452 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/node.py
--rw-r--r--   0 root         (0) root         (0)     1964 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/snap/snap_to_yaml.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.060564 dagster-1.3.9rc0/dagster/_core/storage/
--rw-r--r--   0 root         (0) root         (0)     3057 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/DEVELOPING.md
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.060564 dagster-1.3.9rc0/dagster/_core/storage/alembic/
--rw-r--r--   0 root         (0) root         (0)     6676 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/README.md
--rw-r--r--   0 root         (0) root         (0)      687 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/env.py
--rw-r--r--   0 root         (0) root         (0)      494 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/script.py.mako
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.072564 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/
--rw-r--r--   0 root         (0) root         (0)     3150 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/001_initial_1.py
--rw-r--r--   0 root         (0) root         (0)      311 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/001_initial_schedule.py
--rw-r--r--   0 root         (0) root         (0)     1485 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_postgres.py
--rw-r--r--   0 root         (0) root         (0)      598 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      972 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_postgres.py
--rw-r--r--   0 root         (0) root         (0)      972 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     2548 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/004_add_snapshots_to_run_storage.py
--rw-r--r--   0 root         (0) root         (0)     1405 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/005_add_asset_key_postgres.py
--rw-r--r--   0 root         (0) root         (0)     1405 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/005_add_asset_key_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     1130 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/006_scheduler_update_postgres.py
--rw-r--r--   0 root         (0) root         (0)      952 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_postgres.py
--rw-r--r--   0 root         (0) root         (0)      952 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      955 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_postgres.py
--rw-r--r--   0 root         (0) root         (0)      955 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     1170 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/009_add_partition_column_postgres.py
--rw-r--r--   0 root         (0) root         (0)     1170 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/009_add_partition_column_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     1729 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_postgres.py
--rw-r--r--   0 root         (0) root         (0)     1729 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     1146 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_postgres.py
--rw-r--r--   0 root         (0) root         (0)     1124 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_1.py
--rw-r--r--   0 root         (0) root         (0)     1142 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_2.py
--rw-r--r--   0 root         (0) root         (0)      416 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/012_0_10_0_create_new_run_tables_postgres.py
--rw-r--r--   0 root         (0) root         (0)      416 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/012_0_10_0_create_new_run_tables_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      434 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/013_0_10_0_create_new_event_log_tables_postgres.py
--rw-r--r--   0 root         (0) root         (0)      434 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/013_0_10_0_create_new_event_log_tables_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      431 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/014_0_10_0_create_new_schedule_tables_postgres.py
--rw-r--r--   0 root         (0) root         (0)      431 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/014_0_10_0_create_new_schedule_tables_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     3926 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_postgres.py
--rw-r--r--   0 root         (0) root         (0)     3926 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      408 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/016_add_bulk_actions_table_postgres.py
--rw-r--r--   0 root         (0) root         (0)      408 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/016_add_bulk_actions_table_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      935 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/017_add_run_status_index_postgres.py
--rw-r--r--   0 root         (0) root         (0)      935 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/017_add_run_status_index_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      325 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/017_initial_mysql.py
--rw-r--r--   0 root         (0) root         (0)      420 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/018_add_asset_tags_mysql.py
--rw-r--r--   0 root         (0) root         (0)      420 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/018_add_asset_tags_postgres.py
--rw-r--r--   0 root         (0) root         (0)      420 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/018_add_asset_tags_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     1569 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_postgres.py
--rw-r--r--   0 root         (0) root         (0)     1569 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      409 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/020_add_column_asset_body_mysql.py
--rw-r--r--   0 root         (0) root         (0)      409 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/020_add_column_asset_body_postgres.py
--rw-r--r--   0 root         (0) root         (0)      409 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/020_add_column_asset_body_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     1033 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/021_add_column_mode_mysql.py
--rw-r--r--   0 root         (0) root         (0)     1031 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/021_add_column_mode_postgres.py
--rw-r--r--   0 root         (0) root         (0)     1033 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/021_add_column_mode_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      432 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/022_extract_asset_keys_index_columns_mysql.py
--rw-r--r--   0 root         (0) root         (0)      432 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/022_extract_asset_keys_index_columns_postgres.py
--rw-r--r--   0 root         (0) root         (0)      432 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/022_extract_asset_keys_index_columns_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      420 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/023_add_event_log_event_type_idx_mysql.py
--rw-r--r--   0 root         (0) root         (0)      420 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/023_add_event_log_event_type_idx_postgres.py
--rw-r--r--   0 root         (0) root         (0)      420 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/023_add_event_log_event_type_idx_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      530 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_mysql.py
--rw-r--r--   0 root         (0) root         (0)     1274 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_postgres.py
--rw-r--r--   0 root         (0) root         (0)     1274 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      403 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/025_add_range_index_mysql.py
--rw-r--r--   0 root         (0) root         (0)      403 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/025_add_range_index_postgres.py
--rw-r--r--   0 root         (0) root         (0)      403 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/025_add_range_index_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      634 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/026_convert_start_end_times_format_mysql.py
--rw-r--r--   0 root         (0) root         (0)      433 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/027_add_migration_table_mysql.py
--rw-r--r--   0 root         (0) root         (0)      433 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/027_add_migration_table_postgres.py
--rw-r--r--   0 root         (0) root         (0)      433 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/027_add_migration_table_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      409 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/028_add_instigators_table_mysql.py
--rw-r--r--   0 root         (0) root         (0)      409 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/028_add_instigators_table_postgres.py
--rw-r--r--   0 root         (0) root         (0)      409 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/028_add_instigators_table_sqlite.py
--rw-r--r--   0 root         (0) root         (0)      415 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/029_add_tick_selector_index_mysql.py
--rw-r--r--   0 root         (0) root         (0)      415 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/029_add_tick_selector_index_postgres.py
--rw-r--r--   0 root         (0) root         (0)      415 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/029_add_tick_selector_index_sqlite.py
--rw-r--r--   0 root         (0) root         (0)     1892 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/030_add_columns_action_type_and_selector_id_.py
--rw-r--r--   0 root         (0) root         (0)      957 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/031_add_kvs_table.py
--rw-r--r--   0 root         (0) root         (0)      498 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/032_rebuild_event_indexes.py
--rw-r--r--   0 root         (0) root         (0)     1950 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/033_add_asset_event_tags_table.py
--rw-r--r--   0 root         (0) root         (0)      428 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/034_add_cached_status_data_column.py
--rw-r--r--   0 root         (0) root         (0)      426 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/035_add_run_job_index.py
--rw-r--r--   0 root         (0) root         (0)     1536 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/036_add_dynamic_partitions_table.py
--rw-r--r--   0 root         (0) root         (0)     2480 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/037_d9092588866f_add_primary_key_cols.py
--rw-r--r--   0 root         (0) root         (0)     1176 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/038_701913684cb4_add_postgres_pks.py
--rw-r--r--   0 root         (0) root         (0)     2062 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/039_d3a4c9e87af3_add_asset_daemon_asset_evaluations_table.py
--rw-r--r--   0 root         (0) root         (0)     2358 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/040_add_in_progress_step_table.py
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/__init__.py
--rw-r--r--   0 root         (0) root         (0)     7204 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/asset_value_loader.py
--rw-r--r--   0 root         (0) root         (0)     1215 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/base_storage.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.072564 dagster-1.3.9rc0/dagster/_core/storage/branching/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/branching/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4304 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/branching/branching_io_manager.py
--rw-r--r--   0 root         (0) root         (0)     8736 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/captured_log_manager.py
--rw-r--r--   0 root         (0) root         (0)    16247 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/cloud_storage_compute_log_manager.py
--rw-r--r--   0 root         (0) root         (0)     9545 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/compute_log_manager.py
--rw-r--r--   0 root         (0) root         (0)     1863 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/config.py
--rw-r--r--   0 root         (0) root         (0)      417 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/daemon_cursor.py
--rw-r--r--   0 root         (0) root         (0)    24055 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/dagster_run.py
--rw-r--r--   0 root         (0) root         (0)    11640 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/db_io_manager.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.072564 dagster-1.3.9rc0/dagster/_core/storage/event_log/
--rw-r--r--   0 root         (0) root         (0)      742 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/__init__.py
--rw-r--r--   0 root         (0) root         (0)    16268 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/base.py
--rw-r--r--   0 root         (0) root         (0)     3753 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/in_memory.py
--rw-r--r--   0 root         (0) root         (0)     7273 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/migration.py
--rw-r--r--   0 root         (0) root         (0)     7911 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/polling_event_watcher.py
--rw-r--r--   0 root         (0) root         (0)     6408 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/schema.py
--rw-r--r--   0 root         (0) root         (0)   100680 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/sql_event_log.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/
--rw-r--r--   0 root         (0) root         (0)      200 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/alembic/
--rw-r--r--   0 root         (0) root         (0)     1040 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/alembic/alembic.ini
--rw-r--r--   0 root         (0) root         (0)     7526 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/consolidated_sqlite_event_log.py
--rw-r--r--   0 root         (0) root         (0)    19093 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/sqlite_event_log.py
--rw-r--r--   0 root         (0) root         (0)    10912 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/file_manager.py
--rw-r--r--   0 root         (0) root         (0)    13217 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/fs_io_manager.py
--rw-r--r--   0 root         (0) root         (0)     8915 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/input_manager.py
--rw-r--r--   0 root         (0) root         (0)    10638 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/io_manager.py
--rw-r--r--   0 root         (0) root         (0)    28589 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/legacy_storage.py
--rw-r--r--   0 root         (0) root         (0)    17301 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/local_compute_log_manager.py
--rw-r--r--   0 root         (0) root         (0)      876 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/mem_io_manager.py
--rw-r--r--   0 root         (0) root         (0)     4293 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/memoizable_io_manager.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/migration/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/migration/__init__.py
--rw-r--r--   0 root         (0) root         (0)    14469 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/migration/utils.py
--rw-r--r--   0 root         (0) root         (0)     3186 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/noop_compute_log_manager.py
--rw-r--r--   0 root         (0) root         (0)     2361 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/output_manager.py
--rw-r--r--   0 root         (0) root         (0)    23189 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/partition_status_cache.py
--rw-r--r--   0 root         (0) root         (0)     2121 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/root.py
--rw-r--r--   0 root         (0) root         (0)     8509 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/root_input_manager.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/runs/
--rw-r--r--   0 root         (0) root         (0)      386 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/__init__.py
--rw-r--r--   0 root         (0) root         (0)    15454 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/base.py
--rw-r--r--   0 root         (0) root         (0)     2416 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/in_memory.py
--rw-r--r--   0 root         (0) root         (0)     8697 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/migration.py
--rw-r--r--   0 root         (0) root         (0)     5982 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/schema.py
--rw-r--r--   0 root         (0) root         (0)    46286 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/sql_run_storage.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/runs/sqlite/
--rw-r--r--   0 root         (0) root         (0)       69 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/sqlite/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/runs/sqlite/alembic/
--rw-r--r--   0 root         (0) root         (0)     1040 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/sqlite/alembic/alembic.ini
--rw-r--r--   0 root         (0) root         (0)     6818 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/runs/sqlite/sqlite_run_storage.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/schedules/
--rw-r--r--   0 root         (0) root         (0)      272 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6358 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/base.py
--rw-r--r--   0 root         (0) root         (0)     4133 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/migration.py
--rw-r--r--   0 root         (0) root         (0)     3710 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/schema.py
--rw-r--r--   0 root         (0) root         (0)    22626 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/sql_schedule_storage.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/schedules/sqlite/
--rw-r--r--   0 root         (0) root         (0)       84 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/sqlite/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/storage/schedules/sqlite/alembic/
--rw-r--r--   0 root         (0) root         (0)     1039 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/sqlite/alembic/alembic.ini
--rw-r--r--   0 root         (0) root         (0)     3684 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/schedules/sqlite/sqlite_schedule_storage.py
--rw-r--r--   0 root         (0) root         (0)     7210 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/sql.py
--rw-r--r--   0 root         (0) root         (0)     1391 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/sqlalchemy_compat.py
--rw-r--r--   0 root         (0) root         (0)      926 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/sqlite.py
--rw-r--r--   0 root         (0) root         (0)     4863 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/sqlite_storage.py
--rw-r--r--   0 root         (0) root         (0)     3199 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/tags.py
--rw-r--r--   0 root         (0) root         (0)     1128 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/temp_file_manager.py
--rw-r--r--   0 root         (0) root         (0)    11346 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/storage/upath_io_manager.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.076564 dagster-1.3.9rc0/dagster/_core/system_config/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/system_config/__init__.py
--rw-r--r--   0 root         (0) root         (0)    13981 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/system_config/composite_descent.py
--rw-r--r--   0 root         (0) root         (0)    14855 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/system_config/objects.py
--rw-r--r--   0 root         (0) root         (0)    27932 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/telemetry.py
--rw-r--r--   0 root         (0) root         (0)     4824 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/telemetry_upload.py
--rw-r--r--   0 root         (0) root         (0)    18936 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/test_utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_core/types/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3163 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/builtin_config_schemas.py
--rw-r--r--   0 root         (0) root         (0)     7298 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/config_schema.py
--rw-r--r--   0 root         (0) root         (0)    35761 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/dagster_type.py
--rw-r--r--   0 root         (0) root         (0)     3579 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/decorator.py
--rw-r--r--   0 root         (0) root         (0)     1846 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/loadable_target_origin.py
--rw-r--r--   0 root         (0) root         (0)     1027 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/primitive_mapping.py
--rw-r--r--   0 root         (0) root         (0)     4881 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/python_dict.py
--rw-r--r--   0 root         (0) root         (0)     2836 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/python_set.py
--rw-r--r--   0 root         (0) root         (0)     3712 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/python_tuple.py
--rw-r--r--   0 root         (0) root         (0)     1772 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/types/transform_typing.py
--rw-r--r--   0 root         (0) root         (0)     1438 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/utility_ops.py
--rw-r--r--   0 root         (0) root         (0)     4402 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_core/workspace/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4722 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/autodiscovery.py
--rw-r--r--   0 root         (0) root         (0)     3478 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/config_schema.py
--rw-r--r--   0 root         (0) root         (0)    28195 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/context.py
--rw-r--r--   0 root         (0) root         (0)    11881 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/load.py
--rw-r--r--   0 root         (0) root         (0)     4684 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/load_target.py
--rw-r--r--   0 root         (0) root         (0)     4099 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/permissions.py
--rw-r--r--   0 root         (0) root         (0)     1912 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_core/workspace/workspace.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_daemon/
--rw-r--r--   0 root         (0) root         (0)     2015 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/__init__.py
--rw-r--r--   0 root         (0) root         (0)       30 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/__main__.py
--rw-r--r--   0 root         (0) root         (0)     6416 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/asset_daemon.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_daemon/auto_run_reexecution/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/auto_run_reexecution/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6822 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/auto_run_reexecution/auto_run_reexecution.py
--rw-r--r--   0 root         (0) root         (0)     9123 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/auto_run_reexecution/event_log_consumer.py
--rw-r--r--   0 root         (0) root         (0)     2125 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/backfill.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_daemon/cli/
--rw-r--r--   0 root         (0) root         (0)     4447 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/cli/__init__.py
--rw-r--r--   0 root         (0) root         (0)    18221 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/controller.py
--rw-r--r--   0 root         (0) root         (0)    10611 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/daemon.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_daemon/monitoring/
--rw-r--r--   0 root         (0) root         (0)      320 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/monitoring/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1792 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/monitoring/concurrency.py
--rw-r--r--   0 root         (0) root         (0)    10036 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/monitoring/run_monitoring.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_daemon/run_coordinator/
--rw-r--r--   0 root         (0) root         (0)      100 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/run_coordinator/__init__.py
--rw-r--r--   0 root         (0) root         (0)    16326 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/run_coordinator/queued_run_coordinator_daemon.py
--rw-r--r--   0 root         (0) root         (0)    39892 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/sensor.py
--rw-r--r--   0 root         (0) root         (0)     2860 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/types.py
--rw-r--r--   0 root         (0) root         (0)     6639 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_daemon/workspace.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_experimental/
--rw-r--r--   0 root         (0) root         (0)      300 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_experimental/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_generate/
--rw-r--r--   0 root         (0) root         (0)      253 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2720 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/download.py
--rw-r--r--   0 root         (0) root         (0)     4805 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/generate.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.020563 dagster-1.3.9rc0/dagster/_generate/templates/
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER/
--rw-r--r--   0 root         (0) root         (0)      175 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER/__init__.py
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER/assets.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.080564 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER_tests/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER_tests/__init__.py
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER_tests/test_assets.py.tmpl
--rw-r--r--   0 root         (0) root         (0)      137 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/pyproject.toml.tmpl
--rw-r--r--   0 root         (0) root         (0)       43 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/setup.cfg.tmpl
--rw-r--r--   0 root         (0) root         (0)      285 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/setup.py.tmpl
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_generate/templates/PROJECT_NAME_PLACEHOLDER/
--rw-r--r--   0 root         (0) root         (0)     1753 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/PROJECT_NAME_PLACEHOLDER/README.md
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/
--rw-r--r--   0 root         (0) root         (0)       40 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/__init__.py.tmpl
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/assets.py
--rw-r--r--   0 root         (0) root         (0)      164 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/repository.py.tmpl
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER_tests/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER_tests/__init__.py
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER_tests/test_assets.py.tmpl
--rw-r--r--   0 root         (0) root         (0)       80 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/pyproject.toml
--rw-r--r--   0 root         (0) root         (0)       34 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/setup.cfg.tmpl
--rw-r--r--   0 root         (0) root         (0)      267 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/setup.py.tmpl
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_grpc/
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_grpc/__generated__/
--rw-r--r--   0 root         (0) root         (0)      178 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/__generated__/__init__.py
--rw-r--r--   0 root         (0) root         (0)    29251 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/__generated__/api_pb2.py
--rw-r--r--   0 root         (0) root         (0)    39700 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/__generated__/api_pb2_grpc.py
--rw-r--r--   0 root         (0) root         (0)     2051 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/__init__.py
--rw-r--r--   0 root         (0) root         (0)       89 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/__main__.py
--rw-r--r--   0 root         (0) root         (0)    18451 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/client.py
--rw-r--r--   0 root         (0) root         (0)     4202 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/compile.py
--rw-r--r--   0 root         (0) root         (0)    21967 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/impl.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_grpc/protos/
--rw-r--r--   0 root         (0) root         (0)     5551 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/protos/api.proto
--rw-r--r--   0 root         (0) root         (0)    12415 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/proxy_server.py
--rw-r--r--   0 root         (0) root         (0)    51532 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/server.py
--rw-r--r--   0 root         (0) root         (0)     5301 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/server_watcher.py
--rw-r--r--   0 root         (0) root         (0)    26559 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/types.py
--rw-r--r--   0 root         (0) root         (0)     2323 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_grpc/utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_legacy/
--rw-r--r--   0 root         (0) root         (0)      222 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_legacy/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.084564 dagster-1.3.9rc0/dagster/_loggers/
--rw-r--r--   0 root         (0) root         (0)     3781 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_loggers/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3269 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_module_alias_map.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.088564 dagster-1.3.9rc0/dagster/_scheduler/
--rw-r--r--   0 root         (0) root         (0)        0 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_scheduler/__init__.py
--rw-r--r--   0 root         (0) root         (0)    34396 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_scheduler/scheduler.py
--rw-r--r--   0 root         (0) root         (0)     1361 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_scheduler/stale.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.088564 dagster-1.3.9rc0/dagster/_serdes/
--rw-r--r--   0 root         (0) root         (0)      629 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_serdes/__init__.py
--rw-r--r--   0 root         (0) root         (0)     8004 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_serdes/config_class.py
--rw-r--r--   0 root         (0) root         (0)      142 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_serdes/errors.py
--rw-r--r--   0 root         (0) root         (0)     7467 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_serdes/ipc.py
--rw-r--r--   0 root         (0) root         (0)    37613 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_serdes/serdes.py
--rw-r--r--   0 root         (0) root         (0)      674 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_serdes/utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.088564 dagster-1.3.9rc0/dagster/_seven/
--rw-r--r--   0 root         (0) root         (0)     5496 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_seven/__init__.py
--rw-r--r--   0 root         (0) root         (0)      553 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_seven/abc.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.088564 dagster-1.3.9rc0/dagster/_seven/compat/
--rw-r--r--   0 root         (0) root         (0)      105 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_seven/compat/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1160 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_seven/compat/pendulum.py
--rw-r--r--   0 root         (0) root         (0)      383 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_seven/json.py
--rw-r--r--   0 root         (0) root         (0)      354 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_seven/temp_dir.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.088564 dagster-1.3.9rc0/dagster/_utils/
--rw-r--r--   0 root         (0) root         (0)    23319 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     8732 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/alert.py
--rw-r--r--   0 root         (0) root         (0)     6965 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/backcompat.py
--rw-r--r--   0 root         (0) root         (0)     2250 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/backoff.py
--rw-r--r--   0 root         (0) root         (0)     4116 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/cached_method.py
--rw-r--r--   0 root         (0) root         (0)    25123 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/caching_instance_queryer.py
--rw-r--r--   0 root         (0) root         (0)     3813 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/concurrency.py
--rw-r--r--   0 root         (0) root         (0)     2563 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/dagster_type.py
--rw-r--r--   0 root         (0) root         (0)      799 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/env.py
--rw-r--r--   0 root         (0) root         (0)     4094 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/error.py
--rw-r--r--   0 root         (0) root         (0)     1264 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/external.py
--rw-r--r--   0 root         (0) root         (0)      883 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/forked_pdb.py
--rw-r--r--   0 root         (0) root         (0)     2344 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/hosted_user_process.py
--rw-r--r--   0 root         (0) root         (0)     2796 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/indenting_printer.py
--rw-r--r--   0 root         (0) root         (0)      344 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/internal_init.py
--rw-r--r--   0 root         (0) root         (0)     3227 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/interrupts.py
--rw-r--r--   0 root         (0) root         (0)     9113 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/log.py
--rw-r--r--   0 root         (0) root         (0)     2313 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/merger.py
--rw-r--r--   0 root         (0) root         (0)     1507 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/net.py
--rw-r--r--   0 root         (0) root         (0)      208 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/partitions.py
--rw-r--r--   0 root         (0) root         (0)    12340 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/schedules.py
--rw-r--r--   0 root         (0) root         (0)     3289 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/tags.py
--rw-r--r--   0 root         (0) root         (0)     1820 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/temp_file.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.092564 dagster-1.3.9rc0/dagster/_utils/test/
--rw-r--r--   0 root         (0) root         (0)    12640 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/__init__.py
--rw-r--r--   0 root         (0) root         (0)      119 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/hello_world_defs.py
--rw-r--r--   0 root         (0) root         (0)      214 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/hello_world_repository.py
--rw-r--r--   0 root         (0) root         (0)     8622 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/mysql_instance.py
--rw-r--r--   0 root         (0) root         (0)      256 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/named_hello_world_repository.py
--rw-r--r--   0 root         (0) root         (0)     9330 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/postgres_instance.py
--rw-r--r--   0 root         (0) root         (0)    28864 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/schedule_storage.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.092564 dagster-1.3.9rc0/dagster/_utils/test/toys/
--rw-r--r--   0 root         (0) root         (0)       83 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/toys/__init__.py
--rw-r--r--   0 root         (0) root         (0)       84 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/test/toys/single_repository.py
--rw-r--r--   0 root         (0) root         (0)     2004 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/timing.py
--rw-r--r--   0 root         (0) root         (0)      170 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/types.py
--rw-r--r--   0 root         (0) root         (0)     3334 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/typing_api.py
--rw-r--r--   0 root         (0) root         (0)     4915 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/_utils/yaml_utils.py
--rw-r--r--   0 root         (0) root         (0)        8 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/py.typed
--rw-r--r--   0 root         (0) root         (0)       25 2023-06-08 18:20:45.000000 dagster-1.3.9rc0/dagster/version.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-08 18:21:09.024563 dagster-1.3.9rc0/dagster.egg-info/
--rw-r--r--   0 root         (0) root         (0)     8793 2023-06-08 18:21:08.000000 dagster-1.3.9rc0/dagster.egg-info/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)    25431 2023-06-08 18:21:08.000000 dagster-1.3.9rc0/dagster.egg-info/SOURCES.txt
--rw-r--r--   0 root         (0) root         (0)        1 2023-06-08 18:21:08.000000 dagster-1.3.9rc0/dagster.egg-info/dependency_links.txt
--rw-r--r--   0 root         (0) root         (0)       86 2023-06-08 18:21:08.000000 dagster-1.3.9rc0/dagster.egg-info/entry_points.txt
--rw-r--r--   0 root         (0) root         (0)     1443 2023-06-08 18:21:08.000000 dagster-1.3.9rc0/dagster.egg-info/requires.txt
--rw-r--r--   0 root         (0) root         (0)        8 2023-06-08 18:21:08.000000 dagster-1.3.9rc0/dagster.egg-info/top_level.txt
--rw-r--r--   0 root         (0) root         (0)      154 2023-06-08 18:21:09.092564 dagster-1.3.9rc0/setup.cfg
--rw-r--r--   0 root         (0) root         (0)     6820 2023-06-08 18:20:46.000000 dagster-1.3.9rc0/setup.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.874996 dagster-1.4.0/
+-rw-r--r--   0 root         (0) root         (0)      549 2023-07-20 21:53:15.000000 dagster-1.4.0/COPYING
+-rw-r--r--   0 root         (0) root         (0)    11344 2023-07-20 21:53:15.000000 dagster-1.4.0/LICENSE
+-rw-r--r--   0 root         (0) root         (0)      485 2023-07-20 21:53:15.000000 dagster-1.4.0/MANIFEST.in
+-rw-r--r--   0 root         (0) root         (0)     8744 2023-07-20 21:53:51.874996 dagster-1.4.0/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)     7171 2023-07-20 21:53:15.000000 dagster-1.4.0/README.md
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.794994 dagster-1.4.0/dagster/
+-rw-r--r--   0 root         (0) root         (0)    25856 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/__init__.py
+-rw-r--r--   0 root         (0) root         (0)       31 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/__main__.py
+-rw-r--r--   0 root         (0) root         (0)     5456 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_annotations.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.798994 dagster-1.4.0/dagster/_api/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      731 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/get_server_id.py
+-rw-r--r--   0 root         (0) root         (0)     2147 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/list_repositories.py
+-rw-r--r--   0 root         (0) root         (0)      531 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/notebook_data.py
+-rw-r--r--   0 root         (0) root         (0)     2882 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/snapshot_execution_plan.py
+-rw-r--r--   0 root         (0) root         (0)     1664 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/snapshot_job.py
+-rw-r--r--   0 root         (0) root         (0)     5479 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/snapshot_partition.py
+-rw-r--r--   0 root         (0) root         (0)     1668 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/snapshot_repository.py
+-rw-r--r--   0 root         (0) root         (0)     2727 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/snapshot_schedule.py
+-rw-r--r--   0 root         (0) root         (0)     2901 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_api/snapshot_sensor.py
+-rw-r--r--   0 root         (0) root         (0)      478 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_builtins.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.798994 dagster-1.4.0/dagster/_check/
+-rw-r--r--   0 root         (0) root         (0)     1352 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_check/README.md
+-rw-r--r--   0 root         (0) root         (0)    51577 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_check/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.798994 dagster-1.4.0/dagster/_cli/
+-rw-r--r--   0 root         (0) root         (0)     1182 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    26302 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/api.py
+-rw-r--r--   0 root         (0) root         (0)     8271 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/asset.py
+-rw-r--r--   0 root         (0) root         (0)     7730 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/code_server.py
+-rw-r--r--   0 root         (0) root         (0)     2300 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/config_scaffolder.py
+-rw-r--r--   0 root         (0) root         (0)     3561 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/debug.py
+-rw-r--r--   0 root         (0) root         (0)     6088 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/dev.py
+-rw-r--r--   0 root         (0) root         (0)     4618 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/instance.py
+-rw-r--r--   0 root         (0) root         (0)    29907 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/job.py
+-rw-r--r--   0 root         (0) root         (0)     1695 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/load_handle.py
+-rw-r--r--   0 root         (0) root         (0)     6218 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/project.py
+-rw-r--r--   0 root         (0) root         (0)     5129 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/run.py
+-rw-r--r--   0 root         (0) root         (0)    19958 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/schedule.py
+-rw-r--r--   0 root         (0) root         (0)    15707 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/sensor.py
+-rw-r--r--   0 root         (0) root         (0)     4259 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.798994 dagster-1.4.0/dagster/_cli/workspace/
+-rw-r--r--   0 root         (0) root         (0)      180 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/workspace/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    28391 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_cli/workspace/cli_target.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.802994 dagster-1.4.0/dagster/_config/
+-rw-r--r--   0 root         (0) root         (0)     3186 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3403 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/config_schema.py
+-rw-r--r--   0 root         (0) root         (0)    15971 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/config_type.py
+-rw-r--r--   0 root         (0) root         (0)    19601 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/errors.py
+-rw-r--r--   0 root         (0) root         (0)     1783 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/evaluate_value_result.py
+-rw-r--r--   0 root         (0) root         (0)    15605 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/field.py
+-rw-r--r--   0 root         (0) root         (0)    16980 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/field_utils.py
+-rw-r--r--   0 root         (0) root         (0)     9466 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/post_process.py
+-rw-r--r--   0 root         (0) root         (0)      855 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/primitive_mapping.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.802994 dagster-1.4.0/dagster/_config/pythonic_config/
+-rw-r--r--   0 root         (0) root         (0)    73675 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/pythonic_config/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1650 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/pythonic_config/attach_other_object_to_context.py
+-rw-r--r--   0 root         (0) root         (0)     7448 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/pythonic_config/typing_utils.py
+-rw-r--r--   0 root         (0) root         (0)      577 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/pythonic_config/utils.py
+-rw-r--r--   0 root         (0) root         (0)    12143 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/snap.py
+-rw-r--r--   0 root         (0) root         (0)     3267 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/source.py
+-rw-r--r--   0 root         (0) root         (0)     3528 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/stack.py
+-rw-r--r--   0 root         (0) root         (0)     7772 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/traversal_context.py
+-rw-r--r--   0 root         (0) root         (0)     4167 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/type_printer.py
+-rw-r--r--   0 root         (0) root         (0)    17162 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_config/validate.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.802994 dagster-1.4.0/dagster/_core/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      994 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/assets.py
+-rw-r--r--   0 root         (0) root         (0)    13645 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/code_pointer.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.806995 dagster-1.4.0/dagster/_core/container_context/
+-rw-r--r--   0 root         (0) root         (0)      184 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/container_context/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1277 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/container_context/config.py
+-rw-r--r--   0 root         (0) root         (0)     2310 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/debug.py
+-rw-r--r--   0 root         (0) root         (0)     4257 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/decorator_utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.814995 dagster-1.4.0/dagster/_core/definitions/
+-rw-r--r--   0 root         (0) root         (0)     7632 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    33355 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_graph.py
+-rw-r--r--   0 root         (0) root         (0)    10944 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_graph_subset.py
+-rw-r--r--   0 root         (0) root         (0)     3604 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_in.py
+-rw-r--r--   0 root         (0) root         (0)    37132 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_layer.py
+-rw-r--r--   0 root         (0) root         (0)     5685 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_out.py
+-rw-r--r--   0 root         (0) root         (0)    50413 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_reconciliation_sensor.py
+-rw-r--r--   0 root         (0) root         (0)    20868 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_selection.py
+-rw-r--r--   0 root         (0) root         (0)     7255 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/asset_sensor_definition.py
+-rw-r--r--   0 root         (0) root         (0)    64803 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/assets.py
+-rw-r--r--   0 root         (0) root         (0)    24376 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/assets_job.py
+-rw-r--r--   0 root         (0) root         (0)     2945 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/auto_materialize_condition.py
+-rw-r--r--   0 root         (0) root         (0)     5317 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/auto_materialize_policy.py
+-rw-r--r--   0 root         (0) root         (0)    16093 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/cacheable_assets.py
+-rw-r--r--   0 root         (0) root         (0)    45671 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/composition.py
+-rw-r--r--   0 root         (0) root         (0)     4287 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/config.py
+-rw-r--r--   0 root         (0) root         (0)    13908 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/configurable.py
+-rw-r--r--   0 root         (0) root         (0)    21526 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/data_time.py
+-rw-r--r--   0 root         (0) root         (0)    20464 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/data_version.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.818995 dagster-1.4.0/dagster/_core/definitions/decorators/
+-rw-r--r--   0 root         (0) root         (0)      620 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    48782 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/asset_decorator.py
+-rw-r--r--   0 root         (0) root         (0)     4915 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/config_mapping_decorator.py
+-rw-r--r--   0 root         (0) root         (0)     8250 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/graph_decorator.py
+-rw-r--r--   0 root         (0) root         (0)     9444 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/hook_decorator.py
+-rw-r--r--   0 root         (0) root         (0)    10673 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/job_decorator.py
+-rw-r--r--   0 root         (0) root         (0)    17845 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/op_decorator.py
+-rw-r--r--   0 root         (0) root         (0)    14396 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/repository_decorator.py
+-rw-r--r--   0 root         (0) root         (0)     8665 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/schedule_decorator.py
+-rw-r--r--   0 root         (0) root         (0)    12195 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/sensor_decorator.py
+-rw-r--r--   0 root         (0) root         (0)     7648 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/decorators/source_asset_decorator.py
+-rw-r--r--   0 root         (0) root         (0)     5275 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/definition_config_schema.py
+-rw-r--r--   0 root         (0) root         (0)    21443 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/definitions_class.py
+-rw-r--r--   0 root         (0) root         (0)    40472 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/dependency.py
+-rw-r--r--   0 root         (0) root         (0)    32058 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/events.py
+-rw-r--r--   0 root         (0) root         (0)    21168 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/executor_definition.py
+-rw-r--r--   0 root         (0) root         (0)    11045 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/external_asset_graph.py
+-rw-r--r--   0 root         (0) root         (0)    10700 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/freshness_based_auto_materialize.py
+-rw-r--r--   0 root         (0) root         (0)     8241 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/freshness_policy.py
+-rw-r--r--   0 root         (0) root         (0)    16201 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/freshness_policy_sensor_definition.py
+-rw-r--r--   0 root         (0) root         (0)    46922 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/graph_definition.py
+-rw-r--r--   0 root         (0) root         (0)     6546 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/hook_definition.py
+-rw-r--r--   0 root         (0) root         (0)     1515 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/hook_invocation.py
+-rw-r--r--   0 root         (0) root         (0)     3481 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/inference.py
+-rw-r--r--   0 root         (0) root         (0)    21170 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/input.py
+-rw-r--r--   0 root         (0) root         (0)     5386 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/instigation_logger.py
+-rw-r--r--   0 root         (0) root         (0)     2152 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/job_base.py
+-rw-r--r--   0 root         (0) root         (0)    53187 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/job_definition.py
+-rw-r--r--   0 root         (0) root         (0)    20308 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/load_assets_from_modules.py
+-rw-r--r--   0 root         (0) root         (0)     7183 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/logger_definition.py
+-rw-r--r--   0 root         (0) root         (0)      636 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/logger_invocation.py
+-rw-r--r--   0 root         (0) root         (0)     8851 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/materialize.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.818995 dagster-1.4.0/dagster/_core/definitions/metadata/
+-rw-r--r--   0 root         (0) root         (0)    33115 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/metadata/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     8557 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/metadata/table.py
+-rw-r--r--   0 root         (0) root         (0)    56485 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/multi_asset_sensor_definition.py
+-rw-r--r--   0 root         (0) root         (0)    20891 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/multi_dimensional_partitions.py
+-rw-r--r--   0 root         (0) root         (0)      197 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/no_step_launcher.py
+-rw-r--r--   0 root         (0) root         (0)    11927 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/node_container.py
+-rw-r--r--   0 root         (0) root         (0)     8041 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/node_definition.py
+-rw-r--r--   0 root         (0) root         (0)     2867 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/observe.py
+-rw-r--r--   0 root         (0) root         (0)    22760 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/op_definition.py
+-rw-r--r--   0 root         (0) root         (0)    19256 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/op_invocation.py
+-rw-r--r--   0 root         (0) root         (0)     7509 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/op_selection.py
+-rw-r--r--   0 root         (0) root         (0)    18908 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/output.py
+-rw-r--r--   0 root         (0) root         (0)    43384 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/partition.py
+-rw-r--r--   0 root         (0) root         (0)      196 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/partition_key_range.py
+-rw-r--r--   0 root         (0) root         (0)    44961 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/partition_mapping.py
+-rw-r--r--   0 root         (0) root         (0)     8934 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/partitioned_schedule.py
+-rw-r--r--   0 root         (0) root         (0)     3779 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/policy.py
+-rw-r--r--   0 root         (0) root         (0)    27227 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/reconstruct.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.818995 dagster-1.4.0/dagster/_core/definitions/repository_definition/
+-rw-r--r--   0 root         (0) root         (0)      654 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/repository_definition/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6670 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/repository_definition/caching_index.py
+-rw-r--r--   0 root         (0) root         (0)    18860 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/repository_definition/repository_data.py
+-rw-r--r--   0 root         (0) root         (0)    17543 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/repository_definition/repository_data_builder.py
+-rw-r--r--   0 root         (0) root         (0)    17864 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/repository_definition/repository_definition.py
+-rw-r--r--   0 root         (0) root         (0)     1479 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/repository_definition/valid_definitions.py
+-rw-r--r--   0 root         (0) root         (0)     5108 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/resolved_asset_deps.py
+-rw-r--r--   0 root         (0) root         (0)     1465 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/resource_annotation.py
+-rw-r--r--   0 root         (0) root         (0)    17618 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/resource_definition.py
+-rw-r--r--   0 root         (0) root         (0)     5398 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/resource_invocation.py
+-rw-r--r--   0 root         (0) root         (0)     7796 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/resource_requirement.py
+-rw-r--r--   0 root         (0) root         (0)    22861 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/run_config.py
+-rw-r--r--   0 root         (0) root         (0)     1445 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/run_config_schema.py
+-rw-r--r--   0 root         (0) root         (0)    15838 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/run_request.py
+-rw-r--r--   0 root         (0) root         (0)    39126 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/run_status_sensor_definition.py
+-rw-r--r--   0 root         (0) root         (0)    38361 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/schedule_definition.py
+-rw-r--r--   0 root         (0) root         (0)     5137 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/scoped_resources_builder.py
+-rw-r--r--   0 root         (0) root         (0)    10837 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/selector.py
+-rw-r--r--   0 root         (0) root         (0)    48845 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/sensor_definition.py
+-rw-r--r--   0 root         (0) root         (0)    15772 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/source_asset.py
+-rw-r--r--   0 root         (0) root         (0)     2298 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/step_launcher.py
+-rw-r--r--   0 root         (0) root         (0)     1535 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/target.py
+-rw-r--r--   0 root         (0) root         (0)      473 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/test_op_definition.py
+-rw-r--r--   0 root         (0) root         (0)    14173 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/time_window_partition_mapping.py
+-rw-r--r--   0 root         (0) root         (0)    78808 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/time_window_partitions.py
+-rw-r--r--   0 root         (0) root         (0)    15622 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/unresolved_asset_job_definition.py
+-rw-r--r--   0 root         (0) root         (0)     7988 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/utils.py
+-rw-r--r--   0 root         (0) root         (0)     3982 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/definitions/version_strategy.py
+-rw-r--r--   0 root         (0) root         (0)    26509 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/errors.py
+-rw-r--r--   0 root         (0) root         (0)     6391 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/event_api.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.818995 dagster-1.4.0/dagster/_core/events/
+-rw-r--r--   0 root         (0) root         (0)    66637 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/events/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     8156 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/events/log.py
+-rw-r--r--   0 root         (0) root         (0)     1614 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/events/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.822995 dagster-1.4.0/dagster/_core/execution/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    38315 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/api.py
+-rw-r--r--   0 root         (0) root         (0)    38679 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/asset_backfill.py
+-rw-r--r--   0 root         (0) root         (0)    14701 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/backfill.py
+-rw-r--r--   0 root         (0) root         (0)     6295 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/build_resources.py
+-rw-r--r--   0 root         (0) root         (0)      224 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/bulk_actions.py
+-rw-r--r--   0 root         (0) root         (0)     5587 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/compute_logs.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.822995 dagster-1.4.0/dagster/_core/execution/context/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    22931 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/compute.py
+-rw-r--r--   0 root         (0) root         (0)    16053 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/hook.py
+-rw-r--r--   0 root         (0) root         (0)     9622 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/init.py
+-rw-r--r--   0 root         (0) root         (0)    27620 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/input.py
+-rw-r--r--   0 root         (0) root         (0)    31611 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/invocation.py
+-rw-r--r--   0 root         (0) root         (0)     3164 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/logger.py
+-rw-r--r--   0 root         (0) root         (0)    34894 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/output.py
+-rw-r--r--   0 root         (0) root         (0)    45125 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context/system.py
+-rw-r--r--   0 root         (0) root         (0)    18501 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/context_creation_job.py
+-rw-r--r--   0 root         (0) root         (0)     5097 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/execute_in_process.py
+-rw-r--r--   0 root         (0) root         (0)     5897 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/execute_in_process_result.py
+-rw-r--r--   0 root         (0) root         (0)     9183 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/execution_result.py
+-rw-r--r--   0 root         (0) root         (0)     8544 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/host_mode.py
+-rw-r--r--   0 root         (0) root         (0)    14451 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/job_backfill.py
+-rw-r--r--   0 root         (0) root         (0)     6487 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/job_execution_result.py
+-rw-r--r--   0 root         (0) root         (0)      998 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/memoization.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.826995 dagster-1.4.0/dagster/_core/execution/plan/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    25266 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/active.py
+-rw-r--r--   0 root         (0) root         (0)     7270 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/compute.py
+-rw-r--r--   0 root         (0) root         (0)    12550 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/compute_generator.py
+-rw-r--r--   0 root         (0) root         (0)    16658 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/execute_plan.py
+-rw-r--r--   0 root         (0) root         (0)    27342 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/execute_step.py
+-rw-r--r--   0 root         (0) root         (0)    11129 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/external_step.py
+-rw-r--r--   0 root         (0) root         (0)     3664 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/handle.py
+-rw-r--r--   0 root         (0) root         (0)    37483 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/inputs.py
+-rw-r--r--   0 root         (0) root         (0)     4352 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/instance_concurrency_context.py
+-rw-r--r--   0 root         (0) root         (0)     1159 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/local_external_step_main.py
+-rw-r--r--   0 root         (0) root         (0)     5395 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/objects.py
+-rw-r--r--   0 root         (0) root         (0)     7057 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/outputs.py
+-rw-r--r--   0 root         (0) root         (0)    57998 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/plan.py
+-rw-r--r--   0 root         (0) root         (0)      114 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/resume_retry.py
+-rw-r--r--   0 root         (0) root         (0)    15616 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/state.py
+-rw-r--r--   0 root         (0) root         (0)    15920 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/step.py
+-rw-r--r--   0 root         (0) root         (0)     3796 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/plan/utils.py
+-rw-r--r--   0 root         (0) root         (0)     1762 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/poll_compute_logs.py
+-rw-r--r--   0 root         (0) root         (0)     8186 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/resolve_versions.py
+-rw-r--r--   0 root         (0) root         (0)    19280 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/resources_init.py
+-rw-r--r--   0 root         (0) root         (0)     2100 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/retries.py
+-rw-r--r--   0 root         (0) root         (0)     1651 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/run_cancellation_thread.py
+-rw-r--r--   0 root         (0) root         (0)    10329 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/stats.py
+-rw-r--r--   0 root         (0) root         (0)     1125 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/tags.py
+-rw-r--r--   0 root         (0) root         (0)     1172 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/validate_run_config.py
+-rw-r--r--   0 root         (0) root         (0)     1282 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/watch_orphans.py
+-rw-r--r--   0 root         (0) root         (0)     4228 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/execution/with_resources.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.826995 dagster-1.4.0/dagster/_core/executor/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1265 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/base.py
+-rw-r--r--   0 root         (0) root         (0)     5990 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/child_process_executor.py
+-rw-r--r--   0 root         (0) root         (0)     3431 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/in_process.py
+-rw-r--r--   0 root         (0) root         (0)     1509 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/init.py
+-rw-r--r--   0 root         (0) root         (0)    15592 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/multiprocess.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.826995 dagster-1.4.0/dagster/_core/executor/step_delegating/
+-rw-r--r--   0 root         (0) root         (0)      247 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/step_delegating/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    15576 2023-07-20 21:53:15.000000 dagster-1.4.0/dagster/_core/executor/step_delegating/step_delegating_executor.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.830995 dagster-1.4.0/dagster/_core/executor/step_delegating/step_handler/
+-rw-r--r--   0 root         (0) root         (0)      152 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/executor/step_delegating/step_handler/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3078 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/executor/step_delegating/step_handler/base.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.830995 dagster-1.4.0/dagster/_core/host_representation/
+-rw-r--r--   0 root         (0) root         (0)     2807 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    33518 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/code_location.py
+-rw-r--r--   0 root         (0) root         (0)    32236 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/external.py
+-rw-r--r--   0 root         (0) root         (0)    71828 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/external_data.py
+-rw-r--r--   0 root         (0) root         (0)    12517 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/grpc_server_registry.py
+-rw-r--r--   0 root         (0) root         (0)     1487 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/grpc_server_state_subscriber.py
+-rw-r--r--   0 root         (0) root         (0)     4333 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/handle.py
+-rw-r--r--   0 root         (0) root         (0)     1581 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/historical.py
+-rw-r--r--   0 root         (0) root         (0)     4850 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/job_index.py
+-rw-r--r--   0 root         (0) root         (0)    19063 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/origin.py
+-rw-r--r--   0 root         (0) root         (0)     3610 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/host_representation/represented.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.830995 dagster-1.4.0/dagster/_core/instance/
+-rw-r--r--   0 root         (0) root         (0)   112340 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/instance/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    12956 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/instance/config.py
+-rw-r--r--   0 root         (0) root         (0)    24084 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/instance/ref.py
+-rw-r--r--   0 root         (0) root         (0)     3899 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/instance_for_test.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.830995 dagster-1.4.0/dagster/_core/launcher/
+-rw-r--r--   0 root         (0) root         (0)      297 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/launcher/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3758 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/launcher/base.py
+-rw-r--r--   0 root         (0) root         (0)     6808 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/launcher/default_run_launcher.py
+-rw-r--r--   0 root         (0) root         (0)     1586 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/launcher/sync_in_memory_run_launcher.py
+-rw-r--r--   0 root         (0) root         (0)      473 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/libraries.py
+-rw-r--r--   0 root         (0) root         (0)    17249 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/log_manager.py
+-rw-r--r--   0 root         (0) root         (0)     1029 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/nux.py
+-rw-r--r--   0 root         (0) root         (0)     3691 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/origin.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.830995 dagster-1.4.0/dagster/_core/run_coordinator/
+-rw-r--r--   0 root         (0) root         (0)      267 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/run_coordinator/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2005 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/run_coordinator/base.py
+-rw-r--r--   0 root         (0) root         (0)     1922 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/run_coordinator/default_run_coordinator.py
+-rw-r--r--   0 root         (0) root         (0)    11030 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/run_coordinator/queued_run_coordinator.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.830995 dagster-1.4.0/dagster/_core/scheduler/
+-rw-r--r--   0 root         (0) root         (0)      534 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/scheduler/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1241 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/scheduler/execution.py
+-rw-r--r--   0 root         (0) root         (0)    22355 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/scheduler/instigation.py
+-rw-r--r--   0 root         (0) root         (0)     9410 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/scheduler/scheduler.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.834995 dagster-1.4.0/dagster/_core/secrets/
+-rw-r--r--   0 root         (0) root         (0)       51 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/secrets/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1811 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/secrets/env_file.py
+-rw-r--r--   0 root         (0) root         (0)      388 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/secrets/loader.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.834995 dagster-1.4.0/dagster/_core/selector/
+-rw-r--r--   0 root         (0) root         (0)      295 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/selector/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    18247 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/selector/subset_selector.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.834995 dagster-1.4.0/dagster/_core/snap/
+-rw-r--r--   0 root         (0) root         (0)     2842 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      494 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/config_types.py
+-rw-r--r--   0 root         (0) root         (0)     3999 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/dagster_types.py
+-rw-r--r--   0 root         (0) root         (0)     9421 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/dep_snapshot.py
+-rw-r--r--   0 root         (0) root         (0)    12099 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/execution_plan_snapshot.py
+-rw-r--r--   0 root         (0) root         (0)    16654 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/job_snapshot.py
+-rw-r--r--   0 root         (0) root         (0)     4495 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/mode.py
+-rw-r--r--   0 root         (0) root         (0)    14452 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/node.py
+-rw-r--r--   0 root         (0) root         (0)     3693 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/snap/snap_to_yaml.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.838995 dagster-1.4.0/dagster/_core/storage/
+-rw-r--r--   0 root         (0) root         (0)     3057 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/DEVELOPING.md
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.838995 dagster-1.4.0/dagster/_core/storage/alembic/
+-rw-r--r--   0 root         (0) root         (0)     6676 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/README.md
+-rw-r--r--   0 root         (0) root         (0)      687 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/env.py
+-rw-r--r--   0 root         (0) root         (0)      494 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/script.py.mako
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.850995 dagster-1.4.0/dagster/_core/storage/alembic/versions/
+-rw-r--r--   0 root         (0) root         (0)     3150 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/001_initial_1.py
+-rw-r--r--   0 root         (0) root         (0)      311 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/001_initial_schedule.py
+-rw-r--r--   0 root         (0) root         (0)     1485 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      598 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      972 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      972 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     2548 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/004_add_snapshots_to_run_storage.py
+-rw-r--r--   0 root         (0) root         (0)     1405 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/005_add_asset_key_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     1405 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/005_add_asset_key_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     1130 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/006_scheduler_update_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      952 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      952 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      955 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      955 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     1170 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/009_add_partition_column_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     1170 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/009_add_partition_column_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     1729 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     1729 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     1146 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     1124 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_1.py
+-rw-r--r--   0 root         (0) root         (0)     1142 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_2.py
+-rw-r--r--   0 root         (0) root         (0)      416 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/012_0_10_0_create_new_run_tables_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      416 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/012_0_10_0_create_new_run_tables_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      434 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/013_0_10_0_create_new_event_log_tables_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      434 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/013_0_10_0_create_new_event_log_tables_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      431 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/014_0_10_0_create_new_schedule_tables_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      431 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/014_0_10_0_create_new_schedule_tables_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     3926 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     3926 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      408 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/016_add_bulk_actions_table_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      408 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/016_add_bulk_actions_table_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      935 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/017_add_run_status_index_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      935 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/017_add_run_status_index_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      325 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/017_initial_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      420 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/018_add_asset_tags_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      420 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/018_add_asset_tags_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      420 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/018_add_asset_tags_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     1569 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     1569 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      409 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/020_add_column_asset_body_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      409 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/020_add_column_asset_body_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      409 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/020_add_column_asset_body_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/021_add_column_mode_mysql.py
+-rw-r--r--   0 root         (0) root         (0)     1031 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/021_add_column_mode_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/021_add_column_mode_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      432 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/022_extract_asset_keys_index_columns_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      432 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/022_extract_asset_keys_index_columns_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      432 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/022_extract_asset_keys_index_columns_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      420 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/023_add_event_log_event_type_idx_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      420 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/023_add_event_log_event_type_idx_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      420 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/023_add_event_log_event_type_idx_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      530 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_mysql.py
+-rw-r--r--   0 root         (0) root         (0)     1274 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_postgres.py
+-rw-r--r--   0 root         (0) root         (0)     1274 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      403 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/025_add_range_index_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      403 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/025_add_range_index_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      403 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/025_add_range_index_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      634 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/026_convert_start_end_times_format_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      433 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/027_add_migration_table_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      433 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/027_add_migration_table_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      433 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/027_add_migration_table_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      409 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/028_add_instigators_table_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      409 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/028_add_instigators_table_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      409 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/028_add_instigators_table_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)      415 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/029_add_tick_selector_index_mysql.py
+-rw-r--r--   0 root         (0) root         (0)      415 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/029_add_tick_selector_index_postgres.py
+-rw-r--r--   0 root         (0) root         (0)      415 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/029_add_tick_selector_index_sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     1892 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/030_add_columns_action_type_and_selector_id_.py
+-rw-r--r--   0 root         (0) root         (0)      957 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/031_add_kvs_table.py
+-rw-r--r--   0 root         (0) root         (0)      498 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/032_rebuild_event_indexes.py
+-rw-r--r--   0 root         (0) root         (0)     1950 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/033_add_asset_event_tags_table.py
+-rw-r--r--   0 root         (0) root         (0)      428 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/034_add_cached_status_data_column.py
+-rw-r--r--   0 root         (0) root         (0)      426 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/035_add_run_job_index.py
+-rw-r--r--   0 root         (0) root         (0)     1536 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/036_add_dynamic_partitions_table.py
+-rw-r--r--   0 root         (0) root         (0)     2480 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/037_d9092588866f_add_primary_key_cols.py
+-rw-r--r--   0 root         (0) root         (0)     1176 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/038_701913684cb4_add_postgres_pks.py
+-rw-r--r--   0 root         (0) root         (0)     2062 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/039_d3a4c9e87af3_add_asset_daemon_asset_evaluations_table.py
+-rw-r--r--   0 root         (0) root         (0)     2358 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/040_add_in_progress_step_table.py
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/alembic/versions/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     7204 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/asset_value_loader.py
+-rw-r--r--   0 root         (0) root         (0)     1227 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/base_storage.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.850995 dagster-1.4.0/dagster/_core/storage/branching/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/branching/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4304 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/branching/branching_io_manager.py
+-rw-r--r--   0 root         (0) root         (0)     8745 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/captured_log_manager.py
+-rw-r--r--   0 root         (0) root         (0)    16245 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/cloud_storage_compute_log_manager.py
+-rw-r--r--   0 root         (0) root         (0)     9557 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/compute_log_manager.py
+-rw-r--r--   0 root         (0) root         (0)     1863 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/config.py
+-rw-r--r--   0 root         (0) root         (0)      417 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/daemon_cursor.py
+-rw-r--r--   0 root         (0) root         (0)    24416 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/dagster_run.py
+-rw-r--r--   0 root         (0) root         (0)    11640 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/db_io_manager.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.850995 dagster-1.4.0/dagster/_core/storage/event_log/
+-rw-r--r--   0 root         (0) root         (0)      742 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    17062 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/base.py
+-rw-r--r--   0 root         (0) root         (0)     3762 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/in_memory.py
+-rw-r--r--   0 root         (0) root         (0)     7273 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/migration.py
+-rw-r--r--   0 root         (0) root         (0)     7911 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/polling_event_watcher.py
+-rw-r--r--   0 root         (0) root         (0)     6502 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/schema.py
+-rw-r--r--   0 root         (0) root         (0)   104937 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/sql_event_log.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.850995 dagster-1.4.0/dagster/_core/storage/event_log/sqlite/
+-rw-r--r--   0 root         (0) root         (0)      200 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/sqlite/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.850995 dagster-1.4.0/dagster/_core/storage/event_log/sqlite/alembic/
+-rw-r--r--   0 root         (0) root         (0)     1064 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/sqlite/alembic/alembic.ini
+-rw-r--r--   0 root         (0) root         (0)     7538 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/sqlite/consolidated_sqlite_event_log.py
+-rw-r--r--   0 root         (0) root         (0)    19492 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/event_log/sqlite/sqlite_event_log.py
+-rw-r--r--   0 root         (0) root         (0)    10970 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/file_manager.py
+-rw-r--r--   0 root         (0) root         (0)    13540 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/fs_io_manager.py
+-rw-r--r--   0 root         (0) root         (0)     8896 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/input_manager.py
+-rw-r--r--   0 root         (0) root         (0)    10880 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/io_manager.py
+-rw-r--r--   0 root         (0) root         (0)    29582 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/legacy_storage.py
+-rw-r--r--   0 root         (0) root         (0)    17301 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/local_compute_log_manager.py
+-rw-r--r--   0 root         (0) root         (0)     1167 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/mem_io_manager.py
+-rw-r--r--   0 root         (0) root         (0)     4355 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/memoizable_io_manager.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.850995 dagster-1.4.0/dagster/_core/storage/migration/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/migration/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    14469 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/migration/utils.py
+-rw-r--r--   0 root         (0) root         (0)     3186 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/noop_compute_log_manager.py
+-rw-r--r--   0 root         (0) root         (0)     2361 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/output_manager.py
+-rw-r--r--   0 root         (0) root         (0)    23483 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/partition_status_cache.py
+-rw-r--r--   0 root         (0) root         (0)     2121 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/root.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.854995 dagster-1.4.0/dagster/_core/storage/runs/
+-rw-r--r--   0 root         (0) root         (0)      386 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    15474 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/base.py
+-rw-r--r--   0 root         (0) root         (0)     2425 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/in_memory.py
+-rw-r--r--   0 root         (0) root         (0)     8697 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/migration.py
+-rw-r--r--   0 root         (0) root         (0)     5982 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/schema.py
+-rw-r--r--   0 root         (0) root         (0)    46286 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/sql_run_storage.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.854995 dagster-1.4.0/dagster/_core/storage/runs/sqlite/
+-rw-r--r--   0 root         (0) root         (0)       69 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/sqlite/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.854995 dagster-1.4.0/dagster/_core/storage/runs/sqlite/alembic/
+-rw-r--r--   0 root         (0) root         (0)     1064 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/sqlite/alembic/alembic.ini
+-rw-r--r--   0 root         (0) root         (0)     6830 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/runs/sqlite/sqlite_run_storage.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.854995 dagster-1.4.0/dagster/_core/storage/schedules/
+-rw-r--r--   0 root         (0) root         (0)      272 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6620 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/base.py
+-rw-r--r--   0 root         (0) root         (0)     4133 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/migration.py
+-rw-r--r--   0 root         (0) root         (0)     3710 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/schema.py
+-rw-r--r--   0 root         (0) root         (0)    23005 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/sql_schedule_storage.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.854995 dagster-1.4.0/dagster/_core/storage/schedules/sqlite/
+-rw-r--r--   0 root         (0) root         (0)       84 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/sqlite/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.854995 dagster-1.4.0/dagster/_core/storage/schedules/sqlite/alembic/
+-rw-r--r--   0 root         (0) root         (0)     1063 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/sqlite/alembic/alembic.ini
+-rw-r--r--   0 root         (0) root         (0)     3684 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/schedules/sqlite/sqlite_schedule_storage.py
+-rw-r--r--   0 root         (0) root         (0)     7210 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/sql.py
+-rw-r--r--   0 root         (0) root         (0)     1391 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/sqlalchemy_compat.py
+-rw-r--r--   0 root         (0) root         (0)      926 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/sqlite.py
+-rw-r--r--   0 root         (0) root         (0)     4875 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/sqlite_storage.py
+-rw-r--r--   0 root         (0) root         (0)     3268 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/tags.py
+-rw-r--r--   0 root         (0) root         (0)     1186 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/temp_file_manager.py
+-rw-r--r--   0 root         (0) root         (0)    18046 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/storage/upath_io_manager.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.854995 dagster-1.4.0/dagster/_core/system_config/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/system_config/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    13981 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/system_config/composite_descent.py
+-rw-r--r--   0 root         (0) root         (0)    14855 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/system_config/objects.py
+-rw-r--r--   0 root         (0) root         (0)    28851 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/telemetry.py
+-rw-r--r--   0 root         (0) root         (0)     4824 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/telemetry_upload.py
+-rw-r--r--   0 root         (0) root         (0)    20093 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/test_utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_core/types/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3163 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/builtin_config_schemas.py
+-rw-r--r--   0 root         (0) root         (0)     7298 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/config_schema.py
+-rw-r--r--   0 root         (0) root         (0)    36380 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/dagster_type.py
+-rw-r--r--   0 root         (0) root         (0)     3579 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/decorator.py
+-rw-r--r--   0 root         (0) root         (0)     1846 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/loadable_target_origin.py
+-rw-r--r--   0 root         (0) root         (0)     1027 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/primitive_mapping.py
+-rw-r--r--   0 root         (0) root         (0)     4881 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/python_dict.py
+-rw-r--r--   0 root         (0) root         (0)     2836 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/python_set.py
+-rw-r--r--   0 root         (0) root         (0)     3712 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/python_tuple.py
+-rw-r--r--   0 root         (0) root         (0)     1772 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/types/transform_typing.py
+-rw-r--r--   0 root         (0) root         (0)     1438 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/utility_ops.py
+-rw-r--r--   0 root         (0) root         (0)     4402 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_core/workspace/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4722 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/autodiscovery.py
+-rw-r--r--   0 root         (0) root         (0)     3478 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/config_schema.py
+-rw-r--r--   0 root         (0) root         (0)    28580 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/context.py
+-rw-r--r--   0 root         (0) root         (0)    11881 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/load.py
+-rw-r--r--   0 root         (0) root         (0)     4719 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/load_target.py
+-rw-r--r--   0 root         (0) root         (0)     4099 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/permissions.py
+-rw-r--r--   0 root         (0) root         (0)     1912 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_core/workspace/workspace.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_daemon/
+-rw-r--r--   0 root         (0) root         (0)     1971 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/__init__.py
+-rw-r--r--   0 root         (0) root         (0)       30 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/__main__.py
+-rw-r--r--   0 root         (0) root         (0)     7880 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/asset_daemon.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_daemon/auto_run_reexecution/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/auto_run_reexecution/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6822 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/auto_run_reexecution/auto_run_reexecution.py
+-rw-r--r--   0 root         (0) root         (0)     9123 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/auto_run_reexecution/event_log_consumer.py
+-rw-r--r--   0 root         (0) root         (0)     2133 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/backfill.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_daemon/cli/
+-rw-r--r--   0 root         (0) root         (0)     4423 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/cli/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    18390 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/controller.py
+-rw-r--r--   0 root         (0) root         (0)    10611 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/daemon.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_daemon/monitoring/
+-rw-r--r--   0 root         (0) root         (0)      320 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/monitoring/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1792 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/monitoring/concurrency.py
+-rw-r--r--   0 root         (0) root         (0)    10031 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/monitoring/run_monitoring.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_daemon/run_coordinator/
+-rw-r--r--   0 root         (0) root         (0)      100 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/run_coordinator/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    16326 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/run_coordinator/queued_run_coordinator_daemon.py
+-rw-r--r--   0 root         (0) root         (0)    40350 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/sensor.py
+-rw-r--r--   0 root         (0) root         (0)     2860 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/types.py
+-rw-r--r--   0 root         (0) root         (0)     6647 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_daemon/workspace.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.858995 dagster-1.4.0/dagster/_experimental/
+-rw-r--r--   0 root         (0) root         (0)      300 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_experimental/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/
+-rw-r--r--   0 root         (0) root         (0)      253 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2936 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/download.py
+-rw-r--r--   0 root         (0) root         (0)     4805 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/generate.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.790994 dagster-1.4.0/dagster/_generate/templates/
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER/
+-rw-r--r--   0 root         (0) root         (0)      175 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER/__init__.py
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER/assets.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER_tests/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER_tests/__init__.py
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/CODE_LOCATION_NAME_PLACEHOLDER_tests/test_assets.py.tmpl
+-rw-r--r--   0 root         (0) root         (0)      137 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/pyproject.toml.tmpl
+-rw-r--r--   0 root         (0) root         (0)       43 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/setup.cfg.tmpl
+-rw-r--r--   0 root         (0) root         (0)      297 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/CODE_LOCATION_NAME_PLACEHOLDER/setup.py.tmpl
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/templates/PROJECT_NAME_PLACEHOLDER/
+-rw-r--r--   0 root         (0) root         (0)     1753 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/PROJECT_NAME_PLACEHOLDER/README.md
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/
+-rw-r--r--   0 root         (0) root         (0)       40 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/__init__.py.tmpl
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/assets.py
+-rw-r--r--   0 root         (0) root         (0)      164 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER/repository.py.tmpl
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.862995 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER_tests/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER_tests/__init__.py
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/REPO_NAME_PLACEHOLDER_tests/test_assets.py.tmpl
+-rw-r--r--   0 root         (0) root         (0)       80 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/pyproject.toml
+-rw-r--r--   0 root         (0) root         (0)       34 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/setup.cfg.tmpl
+-rw-r--r--   0 root         (0) root         (0)      279 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_generate/templates/REPO_NAME_PLACEHOLDER/setup.py.tmpl
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_grpc/
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_grpc/__generated__/
+-rw-r--r--   0 root         (0) root         (0)      178 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/__generated__/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    29251 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/__generated__/api_pb2.py
+-rw-r--r--   0 root         (0) root         (0)    39700 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/__generated__/api_pb2_grpc.py
+-rw-r--r--   0 root         (0) root         (0)     2060 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/__init__.py
+-rw-r--r--   0 root         (0) root         (0)       89 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/__main__.py
+-rw-r--r--   0 root         (0) root         (0)    18451 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/client.py
+-rw-r--r--   0 root         (0) root         (0)     4204 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/compile.py
+-rw-r--r--   0 root         (0) root         (0)    21967 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/impl.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_grpc/protos/
+-rw-r--r--   0 root         (0) root         (0)     5551 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/protos/api.proto
+-rw-r--r--   0 root         (0) root         (0)    12415 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/proxy_server.py
+-rw-r--r--   0 root         (0) root         (0)    52785 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/server.py
+-rw-r--r--   0 root         (0) root         (0)     5301 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/server_watcher.py
+-rw-r--r--   0 root         (0) root         (0)    26559 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/types.py
+-rw-r--r--   0 root         (0) root         (0)     2323 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_grpc/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_legacy/
+-rw-r--r--   0 root         (0) root         (0)      222 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_legacy/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_loggers/
+-rw-r--r--   0 root         (0) root         (0)     3781 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_loggers/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3269 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_module_alias_map.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_scheduler/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_scheduler/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    37218 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_scheduler/scheduler.py
+-rw-r--r--   0 root         (0) root         (0)     1361 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_scheduler/stale.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_serdes/
+-rw-r--r--   0 root         (0) root         (0)      629 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_serdes/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     8004 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_serdes/config_class.py
+-rw-r--r--   0 root         (0) root         (0)      142 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_serdes/errors.py
+-rw-r--r--   0 root         (0) root         (0)     7557 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_serdes/ipc.py
+-rw-r--r--   0 root         (0) root         (0)    37610 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_serdes/serdes.py
+-rw-r--r--   0 root         (0) root         (0)      674 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_serdes/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_seven/
+-rw-r--r--   0 root         (0) root         (0)     5496 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_seven/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      553 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_seven/abc.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.866995 dagster-1.4.0/dagster/_seven/compat/
+-rw-r--r--   0 root         (0) root         (0)      105 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_seven/compat/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1160 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_seven/compat/pendulum.py
+-rw-r--r--   0 root         (0) root         (0)      383 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_seven/json.py
+-rw-r--r--   0 root         (0) root         (0)      354 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_seven/temp_dir.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.870995 dagster-1.4.0/dagster/_utils/
+-rw-r--r--   0 root         (0) root         (0)    23346 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     8782 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/alert.py
+-rw-r--r--   0 root         (0) root         (0)     6965 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/backcompat.py
+-rw-r--r--   0 root         (0) root         (0)     2250 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/backoff.py
+-rw-r--r--   0 root         (0) root         (0)     4116 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/cached_method.py
+-rw-r--r--   0 root         (0) root         (0)    27785 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/caching_instance_queryer.py
+-rw-r--r--   0 root         (0) root         (0)     3813 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/concurrency.py
+-rw-r--r--   0 root         (0) root         (0)     2563 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/dagster_type.py
+-rw-r--r--   0 root         (0) root         (0)      799 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/env.py
+-rw-r--r--   0 root         (0) root         (0)     4094 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/error.py
+-rw-r--r--   0 root         (0) root         (0)     1264 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/external.py
+-rw-r--r--   0 root         (0) root         (0)      891 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/forked_pdb.py
+-rw-r--r--   0 root         (0) root         (0)     2344 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/hosted_user_process.py
+-rw-r--r--   0 root         (0) root         (0)     2796 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/indenting_printer.py
+-rw-r--r--   0 root         (0) root         (0)      344 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/internal_init.py
+-rw-r--r--   0 root         (0) root         (0)     3227 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/interrupts.py
+-rw-r--r--   0 root         (0) root         (0)     9412 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/log.py
+-rw-r--r--   0 root         (0) root         (0)     2313 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/merger.py
+-rw-r--r--   0 root         (0) root         (0)     1507 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/net.py
+-rw-r--r--   0 root         (0) root         (0)      208 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/partitions.py
+-rw-r--r--   0 root         (0) root         (0)    12571 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/schedules.py
+-rw-r--r--   0 root         (0) root         (0)     3289 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/tags.py
+-rw-r--r--   0 root         (0) root         (0)     1820 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/temp_file.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.870995 dagster-1.4.0/dagster/_utils/test/
+-rw-r--r--   0 root         (0) root         (0)    12640 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      119 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/hello_world_defs.py
+-rw-r--r--   0 root         (0) root         (0)      214 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/hello_world_repository.py
+-rw-r--r--   0 root         (0) root         (0)     8622 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/mysql_instance.py
+-rw-r--r--   0 root         (0) root         (0)      256 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/named_hello_world_repository.py
+-rw-r--r--   0 root         (0) root         (0)     9330 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/postgres_instance.py
+-rw-r--r--   0 root         (0) root         (0)    29928 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/schedule_storage.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.874996 dagster-1.4.0/dagster/_utils/test/toys/
+-rw-r--r--   0 root         (0) root         (0)       83 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/toys/__init__.py
+-rw-r--r--   0 root         (0) root         (0)       84 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/test/toys/single_repository.py
+-rw-r--r--   0 root         (0) root         (0)     2004 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/timing.py
+-rw-r--r--   0 root         (0) root         (0)      170 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/types.py
+-rw-r--r--   0 root         (0) root         (0)     3334 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/typing_api.py
+-rw-r--r--   0 root         (0) root         (0)     4993 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/_utils/yaml_utils.py
+-rw-r--r--   0 root         (0) root         (0)        8 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/py.typed
+-rw-r--r--   0 root         (0) root         (0)       22 2023-07-20 21:53:16.000000 dagster-1.4.0/dagster/version.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-20 21:53:51.794994 dagster-1.4.0/dagster.egg-info/
+-rw-r--r--   0 root         (0) root         (0)     8744 2023-07-20 21:53:51.000000 dagster-1.4.0/dagster.egg-info/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)    25449 2023-07-20 21:53:51.000000 dagster-1.4.0/dagster.egg-info/SOURCES.txt
+-rw-r--r--   0 root         (0) root         (0)        1 2023-07-20 21:53:51.000000 dagster-1.4.0/dagster.egg-info/dependency_links.txt
+-rw-r--r--   0 root         (0) root         (0)       86 2023-07-20 21:53:51.000000 dagster-1.4.0/dagster.egg-info/entry_points.txt
+-rw-r--r--   0 root         (0) root         (0)     1246 2023-07-20 21:53:51.000000 dagster-1.4.0/dagster.egg-info/requires.txt
+-rw-r--r--   0 root         (0) root         (0)        8 2023-07-20 21:53:51.000000 dagster-1.4.0/dagster.egg-info/top_level.txt
+-rw-r--r--   0 root         (0) root         (0)      154 2023-07-20 21:53:51.874996 dagster-1.4.0/setup.cfg
+-rw-r--r--   0 root         (0) root         (0)     6282 2023-07-20 21:53:16.000000 dagster-1.4.0/setup.py
```

### Comparing `dagster-1.3.9rc0/COPYING` & `dagster-1.4.0/COPYING`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/LICENSE` & `dagster-1.4.0/LICENSE`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/PKG-INFO` & `dagster-1.4.0/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: dagster
-Version: 1.3.9rc0
+Version: 1.4.0
 Summary: The data orchestration platform built for productivity.
 Author: Elementl
 Author-email: hello@elementl.com
 License: Apache-2.0
 Project-URL: Homepage, https://dagster.io
 Project-URL: GitHub, https://github.com/dagster-io/dagster
 Project-URL: Changelog, https://github.com/dagster-io/dagster/releases
@@ -15,15 +15,14 @@
 Project-URL: Blog, https://dagster.io/blog
 Project-URL: Newsletter, https://dagster.io/newsletter-signup
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Environment :: Console
 Classifier: Environment :: Web Environment
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: System Administrators
-Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Topic :: System :: Monitoring
 Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
@@ -62,15 +61,15 @@
     <img src="https://img.shields.io/badge/slack-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=slack" />
   </a>
   <a target="_blank" href="https://linkedin.com/showcase/dagster" style="background:none">
     <img src="https://img.shields.io/badge/linkedin-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=linkedin" />
   </a>
 </div>
 
-__Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.__
+**Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.**
 
 It is designed for **developing and maintaining data assets**, such as tables, data sets, machine learning models, and reports.
 
 With Dagster, you declareas Python functionsthe data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.
 
 Here is an example of a graph of three assets defined in Python:
 
@@ -93,27 +92,28 @@
 
 @asset
 def continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -> DataFrame:
     result = country_populations.groupby("continent").sum()
     result["pop_change_factor"] = continent_change_model.coef_
     return result
 ```
+
 The graph loaded into Dagster's web UI:
 
 <p align="center">
   <img width="400px" alt="An example asset graph as rendered in the Dagster UI" src="https://user-images.githubusercontent.com/654855/183537484-48dde394-91f2-4de0-9b17-a70b3e9a3823.png">
 </p>
 
 Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.
 
 ## Quick Start:
 
 If you're new to Dagster, we recommend reading about its [core concepts](https://docs.dagster.io/concepts) or learning with the hands-on [tutorial](https://docs.dagster.io/tutorial).
 
-Dagster is available on PyPI and officially supports Python 3.7+.
+Dagster is available on PyPI and officially supports Python 3.8+.
 
 ```bash
 pip install dagster dagit
 ```
 
 This installs two modules:
 
@@ -131,20 +131,23 @@
 ## Key Features:
 
   <p align="center">
     <img width="100%" alt="image" src=".github/key-features-cards.svg">
   </p>
 
 ### Dagster as a productivity platform
+
 Identify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.
 
 ### Dagster as a robust orchestration engine
+
 Put your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.
 
 ### Dagster as a unified control plane
+
 Maintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.
 
 <hr />
 
 ## Master the Modern Data Stack with integrations
 
 Dagster provides a growing library of integrations for todays most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.
```

### Comparing `dagster-1.3.9rc0/README.md` & `dagster-1.4.0/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -23,15 +23,15 @@
     <img src="https://img.shields.io/badge/slack-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=slack" />
   </a>
   <a target="_blank" href="https://linkedin.com/showcase/dagster" style="background:none">
     <img src="https://img.shields.io/badge/linkedin-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=linkedin" />
   </a>
 </div>
 
-__Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.__
+**Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.**
 
 It is designed for **developing and maintaining data assets**, such as tables, data sets, machine learning models, and reports.
 
 With Dagster, you declareas Python functionsthe data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.
 
 Here is an example of a graph of three assets defined in Python:
 
@@ -54,27 +54,28 @@
 
 @asset
 def continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -> DataFrame:
     result = country_populations.groupby("continent").sum()
     result["pop_change_factor"] = continent_change_model.coef_
     return result
 ```
+
 The graph loaded into Dagster's web UI:
 
 <p align="center">
   <img width="400px" alt="An example asset graph as rendered in the Dagster UI" src="https://user-images.githubusercontent.com/654855/183537484-48dde394-91f2-4de0-9b17-a70b3e9a3823.png">
 </p>
 
 Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.
 
 ## Quick Start:
 
 If you're new to Dagster, we recommend reading about its [core concepts](https://docs.dagster.io/concepts) or learning with the hands-on [tutorial](https://docs.dagster.io/tutorial).
 
-Dagster is available on PyPI and officially supports Python 3.7+.
+Dagster is available on PyPI and officially supports Python 3.8+.
 
 ```bash
 pip install dagster dagit
 ```
 
 This installs two modules:
 
@@ -92,20 +93,23 @@
 ## Key Features:
 
   <p align="center">
     <img width="100%" alt="image" src=".github/key-features-cards.svg">
   </p>
 
 ### Dagster as a productivity platform
+
 Identify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.
 
 ### Dagster as a robust orchestration engine
+
 Put your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.
 
 ### Dagster as a unified control plane
+
 Maintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.
 
 <hr />
 
 ## Master the Modern Data Stack with integrations
 
 Dagster provides a growing library of integrations for todays most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.
```

### Comparing `dagster-1.3.9rc0/dagster/__init__.py` & `dagster-1.4.0/dagster/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -110,17 +110,14 @@
 from dagster._config.source import (
     BoolSource as BoolSource,
     IntSource as IntSource,
     StringSource as StringSource,
 )
 from dagster._core.definitions.asset_in import AssetIn as AssetIn
 from dagster._core.definitions.asset_out import AssetOut as AssetOut
-from dagster._core.definitions.asset_reconciliation_sensor import (
-    build_asset_reconciliation_sensor as build_asset_reconciliation_sensor,
-)
 from dagster._core.definitions.asset_selection import AssetSelection as AssetSelection
 from dagster._core.definitions.asset_sensor_definition import (
     AssetSensorDefinition as AssetSensorDefinition,
 )
 from dagster._core.definitions.assets import AssetsDefinition as AssetsDefinition
 from dagster._core.definitions.auto_materialize_policy import (
     AutoMaterializePolicy as AutoMaterializePolicy,
@@ -409,28 +406,34 @@
 )
 from dagster._core.events.log import EventLogEntry as EventLogEntry
 from dagster._core.execution.api import (
     ReexecutionOptions as ReexecutionOptions,
     execute_job as execute_job,
 )
 from dagster._core.execution.build_resources import build_resources as build_resources
-from dagster._core.execution.context.compute import OpExecutionContext as OpExecutionContext
+from dagster._core.execution.context.compute import (
+    AssetExecutionContext as AssetExecutionContext,
+    OpExecutionContext as OpExecutionContext,
+)
 from dagster._core.execution.context.hook import (
     HookContext as HookContext,
     build_hook_context as build_hook_context,
 )
 from dagster._core.execution.context.init import (
     InitResourceContext as InitResourceContext,
     build_init_resource_context as build_init_resource_context,
 )
 from dagster._core.execution.context.input import (
     InputContext as InputContext,
     build_input_context as build_input_context,
 )
-from dagster._core.execution.context.invocation import build_op_context as build_op_context
+from dagster._core.execution.context.invocation import (
+    build_asset_context as build_asset_context,
+    build_op_context as build_op_context,
+)
 from dagster._core.execution.context.logger import InitLoggerContext as InitLoggerContext
 from dagster._core.execution.context.output import (
     OutputContext as OutputContext,
     build_output_context as build_output_context,
 )
 from dagster._core.execution.context.system import (
     DagsterTypeLoaderContext as DagsterTypeLoaderContext,
@@ -474,30 +477,29 @@
 from dagster._core.storage.fs_io_manager import (
     FilesystemIOManager as FilesystemIOManager,
     custom_path_fs_io_manager as custom_path_fs_io_manager,
     fs_io_manager as fs_io_manager,
 )
 from dagster._core.storage.input_manager import (
     InputManager as InputManager,
+    InputManagerDefinition as InputManagerDefinition,
     input_manager as input_manager,
 )
 from dagster._core.storage.io_manager import (
     IOManager as IOManager,
     IOManagerDefinition as IOManagerDefinition,
     io_manager as io_manager,
 )
 from dagster._core.storage.mem_io_manager import (
     InMemoryIOManager as InMemoryIOManager,
     mem_io_manager as mem_io_manager,
 )
 from dagster._core.storage.memoizable_io_manager import MemoizableIOManager as MemoizableIOManager
-from dagster._core.storage.root_input_manager import (
-    RootInputManager as RootInputManager,
-    RootInputManagerDefinition as RootInputManagerDefinition,
-    root_input_manager as root_input_manager,
+from dagster._core.storage.partition_status_cache import (
+    AssetPartitionStatus as AssetPartitionStatus,
 )
 from dagster._core.storage.tags import (
     MAX_RUNTIME_SECONDS_TAG as MAX_RUNTIME_SECONDS_TAG,
     MEMOIZED_RUN_TAG as MEMOIZED_RUN_TAG,
 )
 from dagster._core.storage.upath_io_manager import UPathIOManager as UPathIOManager
 from dagster._core.types.config_schema import (
@@ -532,15 +534,15 @@
     make_email_on_run_failure_sensor as make_email_on_run_failure_sensor,
 )
 from dagster._utils.backcompat import ExperimentalWarning as ExperimentalWarning
 from dagster._utils.dagster_type import check_dagster_type as check_dagster_type
 from dagster._utils.log import get_dagster_logger as get_dagster_logger
 from dagster.version import __version__ as __version__
 
-# isort: split
+# ruff: isort: split
 
 # ########################
 # ##### DYNAMIC IMPORTS
 # ########################
 
 import importlib
 from typing import (
@@ -561,35 +563,29 @@
 # in `_DEPRECATED` is required  for us to generate the deprecation warning.
 
 if TYPE_CHECKING:
     ##### EXAMPLE
     # from dagster.some.module import (
     #     Foo as Foo,
     # )
-
-    # JobExecutionResult used to be called ExecuteJobResult because it was only returned from
-    # `execute_job`.
-    from dagster._core.execution.job_execution_result import (
-        JobExecutionResult as ExecuteJobResult,  # noqa: F401
-    )
+    pass
 
 
 _DEPRECATED: Final[Mapping[str, TypingTuple[str, str, str]]] = {
     ##### EXAMPLE
     # "Foo": (
     #     "dagster.some.module",
     #     "1.1.0",  # breaking version
     #     "Use Bar instead.",
     # ),
 }
 
 _DEPRECATED_RENAMED: Final[Mapping[str, TypingTuple[Callable, str]]] = {
     ##### EXAMPLE
     # "Foo": (Bar, "1.1.0"),
-    "ExecuteJobResult": (JobExecutionResult, "1.4.0"),
 }
 
 
 def __getattr__(name: str) -> TypingAny:
     if name in _DEPRECATED:
         module, breaking_version, additional_warn_text = _DEPRECATED[name]
         value = getattr(importlib.import_module(module), name)
```

### Comparing `dagster-1.3.9rc0/dagster/_annotations.py` & `dagster-1.4.0/dagster/_annotations.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/get_server_id.py` & `dagster-1.4.0/dagster/_api/get_server_id.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/list_repositories.py` & `dagster-1.4.0/dagster/_api/list_repositories.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/notebook_data.py` & `dagster-1.4.0/dagster/_api/notebook_data.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/snapshot_execution_plan.py` & `dagster-1.4.0/dagster/_api/snapshot_execution_plan.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/snapshot_job.py` & `dagster-1.4.0/dagster/_api/snapshot_job.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/snapshot_partition.py` & `dagster-1.4.0/dagster/_api/snapshot_partition.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/snapshot_repository.py` & `dagster-1.4.0/dagster/_api/snapshot_repository.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/snapshot_schedule.py` & `dagster-1.4.0/dagster/_api/snapshot_schedule.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_api/snapshot_sensor.py` & `dagster-1.4.0/dagster/_api/snapshot_sensor.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_check/README.md` & `dagster-1.4.0/dagster/_check/README.md`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_check/__init__.py` & `dagster-1.4.0/dagster/_check/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1580,15 +1580,15 @@
                     additional_message = (
                         " Did you pass a class where you were expecting an instance of the class?"
                     )
                 else:
                     additional_message = ""
                 raise CheckError(
                     f"Member of tuple mismatches type at index {i}. Expected {of_shape_i}. Got "
-                    f"{repr(obj)} of type {type(obj)}.{additional_message}"
+                    f"{obj!r} of type {type(obj)}.{additional_message}"
                 )
 
     elif of_type is not None:
         _check_iterable_items(obj_tuple, of_type, "tuple")
 
     return obj_tuple
 
@@ -1702,35 +1702,34 @@
     value: object,
     ddict: Mapping,
     ttype: TypeOrTupleOfTypes,
     additional_message: Optional[str] = None,
 ) -> ElementCheckError:
     additional_message = " " + additional_message if additional_message else ""
     return ElementCheckError(
-        f"Value {repr(value)} from key {key} is not a {repr(ttype)}. Dict: {repr(ddict)}."
-        f"{additional_message}"
+        f"Value {value!r} from key {key} is not a {ttype!r}. Dict: {ddict!r}.{additional_message}"
     )
 
 
 def _param_type_mismatch_exception(
     obj: object,
     ttype: TypeOrTupleOfTypes,
     param_name: str,
     additional_message: Optional[str] = None,
 ) -> ParameterCheckError:
     additional_message = " " + additional_message if additional_message else ""
     if isinstance(ttype, tuple):
         type_names = sorted([t.__name__ for t in ttype])
         return ParameterCheckError(
-            f'Param "{param_name}" is not one of {type_names}. Got {repr(obj)} which is type'
+            f'Param "{param_name}" is not one of {type_names}. Got {obj!r} which is type'
             f" {type(obj)}.{additional_message}"
         )
     else:
         return ParameterCheckError(
-            f'Param "{param_name}" is not a {ttype.__name__}. Got {repr(obj)} which is type'
+            f'Param "{param_name}" is not a {ttype.__name__}. Got {obj!r} which is type'
             f" {type(obj)}.{additional_message}"
         )
 
 
 def _param_class_mismatch_exception(
     obj: object,
     param_name: str,
@@ -1738,15 +1737,15 @@
     optional: bool,
     additional_message: Optional[str] = None,
 ) -> ParameterCheckError:
     additional_message = " " + additional_message if additional_message else ""
     opt_clause = optional and "be None or" or ""
     subclass_clause = superclass and f"that inherits from {superclass.__name__}" or ""
     return ParameterCheckError(
-        f'Param "{param_name}" must {opt_clause}be a class{subclass_clause}. Got {repr(obj)} of'
+        f'Param "{param_name}" must {opt_clause}be a class{subclass_clause}. Got {obj!r} of'
         f" type {type(obj)}.{additional_message}"
     )
 
 
 def _type_mismatch_error(
     obj: object, ttype: TypeOrTupleOfTypes, additional_message: Optional[str] = None
 ) -> CheckError:
@@ -1764,15 +1763,15 @@
 
 
 def _param_not_callable_exception(
     obj: Any, param_name: str, additional_message: Optional[str] = None
 ) -> ParameterCheckError:
     additional_message = " " + additional_message if additional_message else ""
     return ParameterCheckError(
-        f'Param "{param_name}" is not callable. Got {repr(obj)} with type {type(obj)}.'
+        f'Param "{param_name}" is not callable. Got {obj!r} with type {type(obj)}.'
         f"{additional_message}"
     )
 
 
 def _param_invariant_exception(param_name: str, desc: Optional[str] = None) -> ParameterCheckError:
     return ParameterCheckError(
         f"Invariant violation for parameter {param_name}. Description: {desc}"
@@ -1791,15 +1790,15 @@
                 additional_message = (
                     " Did you pass a class where you were expecting an instance of the class?"
                 )
             else:
                 additional_message = ""
             raise CheckError(
                 f"Member of {collection_name} mismatches type. Expected {of_type}. Got"
-                f" {repr(obj)} of type {type(obj)}.{additional_message}"
+                f" {obj!r} of type {type(obj)}.{additional_message}"
             )
 
     return obj_iter
 
 
 W = TypeVar("W", bound=Mapping)
 X = TypeVar("X", bound=Mapping)
@@ -1813,22 +1812,22 @@
     value_check: Callable = isinstance,
     mapping_type: Type = collections.abc.Mapping,
 ) -> W:
     """Enforces that the keys/values conform to the types specified by key_type, value_type."""
     for key, value in obj.items():
         if key_type and not key_check(key, key_type):
             raise CheckError(
-                f"Key in {mapping_type.__name__} mismatches type. Expected {repr(key_type)}. Got"
-                f" {repr(key)}"
+                f"Key in {mapping_type.__name__} mismatches type. Expected {key_type!r}. Got"
+                f" {key!r}"
             )
 
         if value_type and not value_check(value, value_type):
             raise CheckError(
                 f"Value in {mapping_type.__name__} mismatches expected type for key {key}. Expected"
-                f" value of type {repr(value_type)}. Got value {value} of type {type(value)}."
+                f" value of type {value_type!r}. Got value {value} of type {type(value)}."
             )
 
     return obj
 
 
 def _check_two_dim_mapping_entries(
     obj: W,
```

### Comparing `dagster-1.3.9rc0/dagster/_cli/__init__.py` & `dagster-1.4.0/dagster/_cli/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/api.py` & `dagster-1.4.0/dagster/_cli/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -505,15 +505,15 @@
     "--lazy-load-user-code",
     is_flag=True,
     required=False,
     default=False,
     help=(
         "Wait until the first LoadRepositories call to actually load the repositories, instead of"
         " waiting to load them when the server is launched. Useful for surfacing errors when the"
-        " server is managed directly from Dagit"
+        " server is managed directly from the Dagster UI."
     ),
     envvar="DAGSTER_LAZY_LOAD_USER_CODE",
 )
 @python_origin_target_argument
 @click.option(
     "--use-python-environment-entry-point",
     is_flag=True,
```

### Comparing `dagster-1.3.9rc0/dagster/_cli/asset.py` & `dagster-1.4.0/dagster/_cli/asset.py`

 * *Files 1% similar despite different names*

```diff
@@ -162,15 +162,15 @@
 
 
 @asset_cli.command(name="wipe-partitions-status-cache")
 @click.argument("key", nargs=-1)
 @click.option("--all", is_flag=True, help="Wipe partitions status cache of all asset keys")
 @click.option("--noprompt", is_flag=True)
 def asset_wipe_cache_command(key, **cli_args):
-    r"""Clears the asset partitions status cache, which is used by Dagit to load partition
+    r"""Clears the asset partitions status cache, which is used by the webserver to load partition
     pages more quickly. The cache will be rebuilt the next time the partition pages are loaded,
     if caching is enabled.
 
     \b
     Usage:
       dagster asset wipe-cache --all
       dagster asset wipe-cache <unstructured_asset_key_name>
```

### Comparing `dagster-1.3.9rc0/dagster/_cli/code_server.py` & `dagster-1.4.0/dagster/_cli/code_server.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/config_scaffolder.py` & `dagster-1.4.0/dagster/_cli/config_scaffolder.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/debug.py` & `dagster-1.4.0/dagster/_cli/debug.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,16 +34,16 @@
 @click.group(name="debug")
 def debug_cli():
     """Commands for helping debug Dagster issues by dumping or loading artifacts from specific runs.
 
     This can be used to send a file to someone like the Dagster team who doesn't have direct access
     to your instance to allow them to view the events and details of a specific run.
 
-    Debug files can be viewed using `dagit-debug` cli.
-    Debug files can also be downloaded from dagit.
+    Debug files can be viewed using `dagster-webserver-debug` cli.
+    Debug files can also be downloaded from the Dagster UI.
     """
 
 
 @debug_cli.command(
     name="export",
     help="Export the relevant artifacts for a job run from the current instance in to a file.",
 )
```

### Comparing `dagster-1.3.9rc0/dagster/_cli/dev.py` & `dagster-1.4.0/dagster/_cli/dev.py`

 * *Files 12% similar despite different names*

```diff
@@ -37,46 +37,58 @@
         working_directory_option(),
     )
 
 
 @click.command(
     name="dev",
     help=(
-        "Start a local deployment of Dagster, including dagit running on localhost and the"
-        " dagster-daemon running in the background"
+        "Start a local deployment of Dagster, including dagster-webserver running on localhost and"
+        " the dagster-daemon running in the background"
     ),
     context_settings=dict(
         max_content_width=120,
-        help_option_names=["--help"],  # Don't show '-h' since that's the dagit host
+        help_option_names=["--help"],  # Don't show '-h' since that's the webserver host
     ),
 )
 @dev_command_options
 @click.option(
     "--code-server-log-level",
     help="Set the log level for code servers spun up by dagster services.",
     show_default=True,
     default="warning",
     type=click.Choice(["critical", "error", "warning", "info", "debug"], case_sensitive=False),
 )
-@click.option("--dagit-port", "-p", help="Port to use for the Dagit UI.", required=False)
-@click.option("--dagit-host", "-h", help="Host to use for the Dagit UI.", required=False)
+@click.option(
+    "--port",
+    "--dagit-port",
+    "-p",
+    help="Port to use for the Dagster webserver.",
+    required=False,
+)
+@click.option(
+    "--host",
+    "--dagit-host",
+    "-h",
+    help="Host to use for the Dagster webserver.",
+    required=False,
+)
 def dev_command(
     code_server_log_level: str,
-    dagit_port: Optional[str],
-    dagit_host: Optional[str],
+    port: Optional[str],
+    host: Optional[str],
     **kwargs: ClickArgValue,
 ) -> None:
-    # check if dagit installed, crash if not
+    # check if dagster-webserver installed, crash if not
     try:
-        import dagit  #  # noqa: F401
+        import dagster_webserver  #  # noqa: F401
     except ImportError:
         raise click.UsageError(
-            "The dagit Python package must be installed in order to use the dagster dev command. If"
-            ' you\'re using pip, you can install the dagit package by running "pip install dagit"'
-            " in your Python environment."
+            "The dagster-webserver Python package must be installed in order to use the dagster dev"
+            " command. If you're using pip, you can install the dagster-webserver package by"
+            ' running "pip install dagster-webserver" in your Python environment.'
         )
 
     configure_loggers()
     logger = logging.getLogger("dagster")
 
     # Sanity check workspace args
     get_workspace_load_target(kwargs)
@@ -116,49 +128,51 @@
         if kwargs.get("module_name"):
             for module_name in check.tuple_elem(kwargs, "module_name"):
                 args.extend(["--module-name", module_name])
 
         if kwargs.get("working_directory"):
             args.extend(["--working-directory", check.str_elem(kwargs, "working_directory")])
 
-        dagit_process = open_ipc_subprocess(
-            [sys.executable, "-m", "dagit"]
-            + (["--port", dagit_port] if dagit_port else [])
-            + (["--host", dagit_host] if dagit_host else [])
+        webserver_process = open_ipc_subprocess(
+            [sys.executable, "-m", "dagster_webserver"]
+            + (["--port", port] if port else [])
+            + (["--host", host] if host else [])
             + args
         )
         daemon_process = open_ipc_subprocess(
             [sys.executable, "-m", "dagster._daemon", "run"] + args
         )
         try:
             while True:
                 time.sleep(_CHECK_SUBPROCESS_INTERVAL)
 
-                if dagit_process.poll() is not None:
+                if webserver_process.poll() is not None:
                     raise Exception(
-                        "Dagit process shut down unexpectedly with return code"
-                        f" {dagit_process.returncode}"
+                        "dagster-webserver process shut down unexpectedly with return code"
+                        f" {webserver_process.returncode}"
                     )
 
                 if daemon_process.poll() is not None:
                     raise Exception(
                         "dagster-daemon process shut down unexpectedly with return code"
                         f" {daemon_process.returncode}"
                     )
 
         except:
             logger.info("Shutting down Dagster services...")
             interrupt_ipc_subprocess(daemon_process)
-            interrupt_ipc_subprocess(dagit_process)
+            interrupt_ipc_subprocess(webserver_process)
 
             try:
-                dagit_process.wait(timeout=_SUBPROCESS_WAIT_TIMEOUT)
+                webserver_process.wait(timeout=_SUBPROCESS_WAIT_TIMEOUT)
             except subprocess.TimeoutExpired:
-                logger.warning("dagit process did not terminate cleanly, killing the process")
-                dagit_process.kill()
+                logger.warning(
+                    "dagster-webserver process did not terminate cleanly, killing the process"
+                )
+                webserver_process.kill()
 
             try:
                 daemon_process.wait(timeout=_SUBPROCESS_WAIT_TIMEOUT)
             except subprocess.TimeoutExpired:
                 logger.warning(
                     "dagster-daemon process did not terminate cleanly, killing the process"
                 )
```

### Comparing `dagster-1.3.9rc0/dagster/_cli/instance.py` & `dagster-1.4.0/dagster/_cli/instance.py`

 * *Files 11% similar despite different names*

```diff
@@ -45,17 +45,28 @@
         home = os.environ.get("DAGSTER_HOME")
         if not home:
             click.echo("$DAGSTER_HOME is not set; ephemeral instances do not need to be migrated.")
             return
 
         click.echo(f"$DAGSTER_HOME: {home}\n")
 
+        click.echo("\nInstance configuration:\n-----------------------")
+        click.echo(instance.info_str())
+
+        click.echo(
+            "\nStorage schema state before migration:\n--------------------------------------"
+        )
+        click.echo(instance.schema_str())
+
+        click.echo("\nRunning migration:\n------------------")
+
         instance.upgrade(click.echo)
 
-        click.echo(instance.info_str())
+        click.echo("\nStorage schema state after migration:\n-------------------------------------")
+        click.echo(instance.schema_str())
 
 
 @instance_cli.command(name="reindex", help="Rebuild index over historical runs for performance.")
 def reindex_command():
     with get_instance_for_cli() as instance:
         home = os.environ.get("DAGSTER_HOME")
 
@@ -70,15 +81,15 @@
         click.echo(f"$DAGSTER_HOME: {home}\n")
 
         instance.reindex(click.echo)
 
 
 @instance_cli.group(name="concurrency")
 def concurrency_cli():
-    """Commands for working with the instance-wide op concurrency."""
+    """Commands for working with the instance-wide op concurrency (Experimental)."""
 
 
 @concurrency_cli.command(name="get", help="Get op concurrency limits")
 @click.option(
     "--all",
     required=False,
     is_flag=True,
```

### Comparing `dagster-1.3.9rc0/dagster/_cli/job.py` & `dagster-1.4.0/dagster/_cli/job.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/load_handle.py` & `dagster-1.4.0/dagster/_cli/load_handle.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/project.py` & `dagster-1.4.0/dagster/_cli/project.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 import os
 import sys
-from typing import Sequence
+from typing import Optional, Sequence
 
 import click
 
 from dagster._generate import (
     download_example_from_github,
     generate_code_location,
     generate_project,
     generate_repository,
 )
 from dagster._generate.download import AVAILABLE_EXAMPLES
+from dagster.version import __version__ as dagster_version
 
 
 @click.group(name="project")
 def project_cli():
     """Commands for bootstrapping new Dagster projects and code locations."""
 
 
@@ -129,38 +130,47 @@
 @project_cli.command(
     name="from-example",
     short_help=from_example_command_help_text,
     help=from_example_command_help_text,
 )
 @click.option(
     "--name",
-    required=True,
     type=click.STRING,
     help="Name of the new Dagster project",
 )
 @click.option(
     "--example",
     required=True,
     type=click.STRING,
     help=(
         "Name of the example to bootstrap with. You can use an example name from the official "
         "examples in Dagster repo: https://github.com/dagster-io/dagster/tree/master/examples. "
         "You can also find the available examples via `dagster project list-examples`."
     ),
 )
-def from_example_command(name: str, example: str):
-    dir_abspath = os.path.abspath(name)
+@click.option(
+    "--version",
+    type=click.STRING,
+    help="Which version of the example to download, defaults to same version as installed dagster.",
+    default=dagster_version,
+    show_default=True,
+)
+def from_example_command(name: Optional[str], example: str, version: str):
+    name = name or example
+    dir_abspath = os.path.abspath(name) + "/"
     if os.path.isdir(dir_abspath) and os.path.exists(dir_abspath):
         click.echo(
             click.style(f"The directory {dir_abspath} already exists. ", fg="red")
             + "\nPlease delete the contents of this path or choose another location."
         )
         sys.exit(1)
+    else:
+        os.mkdir(dir_abspath)
 
-    download_example_from_github(dir_abspath, example)
+    download_example_from_github(dir_abspath, example, version)
 
     click.echo(_styled_success_statement(name, dir_abspath))
 
 
 @project_cli.command(
     name="list-examples",
     short_help=list_examples_command_help_text,
```

### Comparing `dagster-1.3.9rc0/dagster/_cli/run.py` & `dagster-1.4.0/dagster/_cli/run.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/schedule.py` & `dagster-1.4.0/dagster/_cli/schedule.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/sensor.py` & `dagster-1.4.0/dagster/_cli/sensor.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/utils.py` & `dagster-1.4.0/dagster/_cli/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_cli/workspace/cli_target.py` & `dagster-1.4.0/dagster/_cli/workspace/cli_target.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/__init__.py` & `dagster-1.4.0/dagster/_config/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/config_schema.py` & `dagster-1.4.0/dagster/_config/config_schema.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/config_type.py` & `dagster-1.4.0/dagster/_config/config_type.py`

 * *Files 1% similar despite different names*

```diff
@@ -123,15 +123,19 @@
 
 
 # Scalars, Composites, Selectors, Lists, Optional, Any
 
 
 class ConfigScalar(ConfigType):
     def __init__(
-        self, key: str, given_name: Optional[str], scalar_kind: ConfigScalarKind, **kwargs: object
+        self,
+        key: str,
+        given_name: Optional[str],
+        scalar_kind: ConfigScalarKind,
+        **kwargs: typing.Any,
     ):
         self.scalar_kind = check.inst_param(scalar_kind, "scalar_kind", ConfigScalarKind)
         super(ConfigScalar, self).__init__(
             key, kind=ConfigTypeKind.SCALAR, given_name=given_name, **kwargs
         )
 
 
@@ -226,15 +230,16 @@
             key=f"Array.{self.inner_type.key}",
             type_params=[self.inner_type],
             kind=ConfigTypeKind.ARRAY,
         )
 
     @public
     @property
-    def description(self):
+    def description(self) -> str:
+        """A human-readable description of this Array type."""
         return f"List of {self.key}"
 
     def type_iterator(self) -> Iterator["ConfigType"]:
         yield from self.inner_type.type_iterator()
         yield from super().type_iterator()
```

### Comparing `dagster-1.3.9rc0/dagster/_config/errors.py` & `dagster-1.4.0/dagster/_config/errors.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/evaluate_value_result.py` & `dagster-1.4.0/dagster/_config/evaluate_value_result.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/field.py` & `dagster-1.4.0/dagster/_config/field.py`

 * *Files 1% similar despite different names*

```diff
@@ -352,14 +352,18 @@
                     )
                 self._default_value = evr.value
         self._is_required = is_required
 
     @public
     @property
     def is_required(self) -> bool:
+        """Whether a value for this field must be provided at runtime.
+
+        Cannot be True if a default value is provided.
+        """
         return self._is_required
 
     @public
     @property
     def default_provided(self) -> bool:
         """Was a default value provided.
 
@@ -367,20 +371,25 @@
             bool: Yes or no
         """
         return self._default_value != FIELD_NO_DEFAULT_PROVIDED
 
     @public
     @property
     def default_value(self) -> Any:
+        """The default value for the field.
+
+        Raises an exception if no default value was provided.
+        """
         check.invariant(self.default_provided, "Asking for default value when none was provided")
         return self._default_value
 
     @public
     @property
     def description(self) -> Optional[str]:
+        """A human-readable description of this config field, if provided."""
         return self._description
 
     @property
     def default_value_as_json_str(self) -> str:
         check.invariant(self.default_provided, "Asking for default value when none was provided")
         return serialize_value(self.default_value)
```

### Comparing `dagster-1.3.9rc0/dagster/_config/field_utils.py` & `dagster-1.4.0/dagster/_config/field_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # encoding: utf-8
 import hashlib
-from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Mapping, Sequence
+from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Mapping, Optional, Sequence
 
 import dagster._check as check
 from dagster._annotations import public
 from dagster._core.errors import DagsterInvalidConfigDefinitionError
 
 from .config_type import Array, ConfigType, ConfigTypeKind
 
@@ -202,15 +202,16 @@
             given_name=key_label_name,
             type_params=[self.key_type, self.inner_type],
             kind=ConfigTypeKind.MAP,
         )
 
     @public
     @property
-    def key_label_name(self):
+    def key_label_name(self) -> Optional[str]:
+        """Name which describes the role of keys in the map, if provided."""
         return self.given_name
 
     def type_iterator(self) -> Iterator["ConfigType"]:
         yield from self.key_type.type_iterator()
         yield from self.inner_type.type_iterator()
         yield from super().type_iterator()
 
@@ -397,15 +398,15 @@
     key = list(the_dict.keys())[0]
     key_type = _convert_potential_type(original_root, key, stack)
     if not key_type or not key_type.kind == ConfigTypeKind.SCALAR:
         raise DagsterInvalidConfigDefinitionError(
             original_root,
             the_dict,
             stack,
-            f"Map dict must have a scalar type as its only key. Got key {repr(key)}",
+            f"Map dict must have a scalar type as its only key. Got key {key!r}",
         )
 
     inner_type = _convert_potential_type(original_root, the_dict[key], stack)
     if not inner_type:
         raise DagsterInvalidConfigDefinitionError(
             original_root,
             the_dict,
```

### Comparing `dagster-1.3.9rc0/dagster/_config/post_process.py` & `dagster-1.4.0/dagster/_config/post_process.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/primitive_mapping.py` & `dagster-1.4.0/dagster/_config/primitive_mapping.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/pythonic_config/__init__.py` & `dagster-1.4.0/dagster/_config/pythonic_config/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,26 +5,28 @@
 from typing import (
     AbstractSet,
     Any,
     Callable,
     Dict,
     Generator,
     Generic,
+    List,
     Mapping,
     NamedTuple,
     Optional,
     Set,
+    Tuple,
     Type,
     TypeVar,
     Union,
     cast,
 )
 
 from pydantic import ConstrainedFloat, ConstrainedInt, ConstrainedStr
-from typing_extensions import TypeAlias, TypeGuard, get_args
+from typing_extensions import TypeAlias, TypeGuard, get_args, get_origin
 
 from dagster import (
     Enum as DagsterEnum,
     Field as DagsterField,
 )
 from dagster._annotations import deprecated
 from dagster._config.config_type import (
@@ -442,14 +444,16 @@
 
 class AllowDelayedDependencies:
     _nested_partial_resources: Mapping[str, ResourceDefinition] = {}
 
     def _resolve_required_resource_keys(
         self, resource_mapping: Mapping[int, str]
     ) -> AbstractSet[str]:
+        from dagster._core.execution.build_resources import wrap_resource_for_execution
+
         # All dependent resources which are not fully configured
         # must be specified to the Definitions object so that the
         # resource can be configured at runtime by the user
         nested_partial_resource_keys = {
             attr_name: resource_mapping.get(id(resource_def))
             for attr_name, resource_def in self._nested_partial_resources.items()
         }
@@ -464,19 +468,21 @@
         # Recursively get all nested resource keys
         nested_resource_required_keys: Set[str] = set()
         for v in self._nested_partial_resources.values():
             nested_resource_required_keys.update(
                 _resolve_required_resource_keys_for_resource(v, resource_mapping)
             )
 
-        resources, _ = separate_resource_params(self.__dict__)
+        resources, _ = separate_resource_params(
+            cast(Type[BaseModel], self.__class__), self.__dict__
+        )
         for v in resources.values():
             nested_resource_required_keys.update(
                 _resolve_required_resource_keys_for_resource(
-                    coerce_to_resource(v), resource_mapping
+                    wrap_resource_for_execution(v), resource_mapping
                 )
             )
 
         out = set(cast(Set[str], nested_partial_resource_keys.values())).union(
             nested_resource_required_keys
         )
         return out
@@ -600,68 +606,66 @@
 ]
 
 
 def is_coercible_to_resource(val: Any) -> TypeGuard[CoercibleToResource]:
     return isinstance(val, (ResourceDefinition, ConfigurableResourceFactory, PartialResource))
 
 
-def coerce_to_resource(
-    coercible_to_resource: CoercibleToResource,
-) -> ResourceDefinition:
-    if isinstance(coercible_to_resource, (ConfigurableResourceFactory, PartialResource)):
-        return coercible_to_resource.get_resource_definition()
-    return coercible_to_resource
-
-
 class ConfigurableResourceFactoryResourceDefinition(ResourceDefinition, AllowDelayedDependencies):
     def __init__(
         self,
         configurable_resource_cls: Type,
         resource_fn: ResourceFunction,
         config_schema: Any,
         description: Optional[str],
         resolve_resource_keys: Callable[[Mapping[int, str]], AbstractSet[str]],
-        nested_resources: Mapping[str, CoercibleToResource],
+        nested_resources: Mapping[str, Any],
+        dagster_maintained: bool = False,
     ):
         super().__init__(
             resource_fn=resource_fn,
             config_schema=config_schema,
             description=description,
         )
         self._configurable_resource_cls = configurable_resource_cls
         self._resolve_resource_keys = resolve_resource_keys
         self._nested_resources = nested_resources
+        self._dagster_maintained = dagster_maintained
 
     @property
     def configurable_resource_cls(self) -> Type:
         return self._configurable_resource_cls
 
     @property
     def nested_resources(
         self,
-    ) -> Mapping[str, CoercibleToResource]:
+    ) -> Mapping[str, Any]:
         return self._nested_resources
 
     def _resolve_required_resource_keys(
         self, resource_mapping: Mapping[int, str]
     ) -> AbstractSet[str]:
         return self._resolve_resource_keys(resource_mapping)
 
+    def _is_dagster_maintained(self) -> bool:
+        return self._dagster_maintained
+
 
 class ConfigurableIOManagerFactoryResourceDefinition(IOManagerDefinition, AllowDelayedDependencies):
     def __init__(
         self,
         configurable_resource_cls: Type,
         resource_fn: ResourceFunction,
         config_schema: Any,
         description: Optional[str],
         resolve_resource_keys: Callable[[Mapping[int, str]], AbstractSet[str]],
-        nested_resources: Mapping[str, CoercibleToResource],
+        nested_resources: Mapping[str, Any],
         input_config_schema: Optional[Union[CoercableToConfigSchema, Type[Config]]] = None,
         output_config_schema: Optional[Union[CoercableToConfigSchema, Type[Config]]] = None,
+        dagster_maintained: bool = False,
     ):
         input_config_schema_resolved: CoercableToConfigSchema = (
             cast(Type[Config], input_config_schema).to_config_schema()
             if safe_is_subclass(input_config_schema, Config)
             else cast(CoercableToConfigSchema, input_config_schema)
         )
         output_config_schema_resolved: CoercableToConfigSchema = (
@@ -675,37 +679,38 @@
             description=description,
             input_config_schema=input_config_schema_resolved,
             output_config_schema=output_config_schema_resolved,
         )
         self._resolve_resource_keys = resolve_resource_keys
         self._nested_resources = nested_resources
         self._configurable_resource_cls = configurable_resource_cls
+        self._dagster_maintained = dagster_maintained
 
     @property
     def configurable_resource_cls(self) -> Type:
         return self._configurable_resource_cls
 
     @property
     def nested_resources(
         self,
-    ) -> Mapping[str, CoercibleToResource]:
+    ) -> Mapping[str, Any]:
         return self._nested_resources
 
     def _resolve_required_resource_keys(
         self, resource_mapping: Mapping[int, str]
     ) -> AbstractSet[str]:
         return self._resolve_resource_keys(resource_mapping)
 
 
 class ConfigurableResourceFactoryState(NamedTuple):
-    nested_partial_resources: Mapping[str, CoercibleToResource]
+    nested_partial_resources: Mapping[str, Any]
     resolved_config_dict: Dict[str, Any]
     config_schema: DefinitionConfigSchema
     schema: DagsterField
-    nested_resources: Dict[str, CoercibleToResource]
+    nested_resources: Dict[str, Any]
     resource_context: Optional[InitResourceContext]
 
 
 class ConfigurableResourceFactory(
     Generic[TResValue],
     Config,
     TypecheckAllowPartialResourceInitParams,
@@ -755,15 +760,15 @@
             assets=[asset_that_uses_database],
             resources={"database": DatabaseResource(connection_uri="some_uri")},
         )
 
     """
 
     def __init__(self, **data: Any):
-        resource_pointers, data_without_resources = separate_resource_params(data)
+        resource_pointers, data_without_resources = separate_resource_params(self.__class__, data)
 
         schema = infer_schema_from_config_class(
             self.__class__, fields_to_omit=set(resource_pointers.keys())
         )
 
         # Populate config values
         Config.__init__(self, **{**data_without_resources, **resource_pointers})
@@ -808,14 +813,20 @@
         return self._state__internal__.nested_resources
 
     @property
     def _resolved_config_dict(self):
         return self._state__internal__.resolved_config_dict
 
     @classmethod
+    def _is_dagster_maintained(cls) -> bool:
+        """This should be overridden to return True by all dagster maintained resources and IO managers.
+        """
+        return False
+
+    @classmethod
     def _is_cm_resource_cls(cls: Type["ConfigurableResourceFactory"]) -> bool:
         return (
             cls.yield_for_execution != ConfigurableResourceFactory.yield_for_execution
             or cls.teardown_after_execution != ConfigurableResourceFactory.teardown_after_execution
         )
 
     @property
@@ -830,28 +841,29 @@
         return ConfigurableResourceFactoryResourceDefinition(
             self.__class__,
             resource_fn=self._get_initialize_and_run_fn(),
             config_schema=self._config_schema,
             description=self.__doc__,
             resolve_resource_keys=self._resolve_required_resource_keys,
             nested_resources=self.nested_resources,
+            dagster_maintained=self._is_dagster_maintained(),
         )
 
     @abstractmethod
     def create_resource(self, context: InitResourceContext) -> TResValue:
         """Returns the object that this resource hands to user code, accessible by ops or assets
         through the context or resource parameters. This works like the function decorated
         with @resource when using function-based resources.
         """
         raise NotImplementedError()
 
     @property
     def nested_resources(
         self,
-    ) -> Mapping[str, CoercibleToResource]:
+    ) -> Mapping[str, Any]:
         return self._nested_resources
 
     @classmethod
     def configure_at_launch(cls: "Type[Self]", **kwargs) -> "PartialResource[Self]":
         """Returns a partially initialized copy of the resource, with remaining config fields
         set at runtime.
         """
@@ -879,14 +891,16 @@
     ) -> Generator["ConfigurableResourceFactory[TResValue]", None, None]:
         """Updates any nested resources with the resource values from the context.
         In this case, populating partially configured resources or
         resources that return plain Python types.
 
         Returns a new instance of the resource.
         """
+        from dagster._core.execution.build_resources import wrap_resource_for_execution
+
         partial_resources_to_update: Dict[str, Any] = {}
         if self._nested_partial_resources:
             context_with_mapping = cast(
                 InitResourceContextWithKeyMapping,
                 check.inst(
                     context,
                     InitResourceContextWithKeyMapping,
@@ -900,18 +914,18 @@
             partial_resources_to_update = {
                 attr_name: context_with_mapping.resources_by_id[id(resource)]
                 for attr_name, resource in self._nested_partial_resources.items()
             }
 
         # Also evaluate any resources that are not partial
         with contextlib.ExitStack() as stack:
-            resources_to_update, _ = separate_resource_params(self.__dict__)
+            resources_to_update, _ = separate_resource_params(self.__class__, self.__dict__)
             resources_to_update = {
                 attr_name: _call_resource_fn_with_default(
-                    stack, coerce_to_resource(resource), context
+                    stack, wrap_resource_for_execution(resource), context
                 )
                 for attr_name, resource in resources_to_update.items()
                 if attr_name not in partial_resources_to_update
             }
 
             to_update = {**resources_to_update, **partial_resources_to_update}
             yield self._with_updated_values(to_update)
@@ -1101,46 +1115,48 @@
         For ConfigurableResource, this function will return itself, passing
         the actual ConfigurableResource object to user code.
         """
         return cast(TResValue, self)
 
 
 def _is_fully_configured(resource: CoercibleToResource) -> bool:
-    actual_resource = coerce_to_resource(resource)
+    from dagster._core.execution.build_resources import wrap_resource_for_execution
+
+    actual_resource = wrap_resource_for_execution(resource)
     res = (
         validate_config(
             actual_resource.config_schema.config_type,
             actual_resource.config_schema.default_value
             if actual_resource.config_schema.default_provided
             else {},
         ).success
         is True
     )
 
     return res
 
 
 class PartialResourceState(NamedTuple):
-    nested_partial_resources: Dict[str, CoercibleToResource]
+    nested_partial_resources: Dict[str, Any]
     config_schema: DagsterField
     resource_fn: Callable[[InitResourceContext], Any]
     description: Optional[str]
-    nested_resources: Dict[str, CoercibleToResource]
+    nested_resources: Dict[str, Any]
 
 
 class PartialResource(Generic[TResValue], AllowDelayedDependencies, MakeConfigCacheable):
     data: Dict[str, Any]
     resource_cls: Type[ConfigurableResourceFactory[TResValue]]
 
     def __init__(
         self,
         resource_cls: Type[ConfigurableResourceFactory[TResValue]],
         data: Dict[str, Any],
     ):
-        resource_pointers, _data_without_resources = separate_resource_params(data)
+        resource_pointers, _data_without_resources = separate_resource_params(resource_cls, data)
 
         MakeConfigCacheable.__init__(self, data=data, resource_cls=resource_cls)  # type: ignore  # extends BaseModel, takes kwargs
 
         def resource_fn(context: InitResourceContext):
             instantiated = resource_cls(
                 **{**data, **context.resource_config}
             )  # So that collisions are resolved in favor of the latest provided run config
@@ -1160,32 +1176,33 @@
             nested_resources={k: v for k, v in resource_pointers.items()},
         )
 
     # to make AllowDelayedDependencies work
     @property
     def _nested_partial_resources(
         self,
-    ) -> Mapping[str, CoercibleToResource]:
+    ) -> Mapping[str, Any]:
         return self._state__internal__.nested_partial_resources
 
     @property
     def nested_resources(
         self,
-    ) -> Mapping[str, CoercibleToResource]:
+    ) -> Mapping[str, Any]:
         return self._state__internal__.nested_resources
 
     @cached_method
     def get_resource_definition(self) -> ConfigurableResourceFactoryResourceDefinition:
         return ConfigurableResourceFactoryResourceDefinition(
-            self.__class__,
+            self.resource_cls,
             resource_fn=self._state__internal__.resource_fn,
             config_schema=self._state__internal__.config_schema,
             description=self._state__internal__.description,
             resolve_resource_keys=self._resolve_required_resource_keys,
             nested_resources=self.nested_resources,
+            dagster_maintained=self.resource_cls._is_dagster_maintained(),  # noqa: SLF001
         )
 
 
 ResourceOrPartial: TypeAlias = Union[
     ConfigurableResourceFactory[TResValue], PartialResource[TResValue]
 ]
 ResourceOrPartialOrValue: TypeAlias = Union[
@@ -1247,14 +1264,15 @@
         return ConfigurableResourceFactoryResourceDefinition(
             self.__class__,
             resource_fn=self.wrapped_resource.resource_fn,
             config_schema=self._config_schema,
             description=self.__doc__,
             resolve_resource_keys=self._resolve_required_resource_keys,
             nested_resources=self.nested_resources,
+            dagster_maintained=self._is_dagster_maintained(),
         )
 
     def __call__(self, *args, **kwargs):
         return self.wrapped_resource(*args, **kwargs)
 
 
 class ConfigurableIOManagerFactory(ConfigurableResourceFactory[TIOManagerValue]):
@@ -1327,14 +1345,15 @@
             resource_fn=self._get_initialize_and_run_fn(),
             config_schema=self._config_schema,
             description=self.__doc__,
             resolve_resource_keys=self._resolve_required_resource_keys,
             nested_resources=self.nested_resources,
             input_config_schema=self.__class__.input_config_schema(),
             output_config_schema=self.__class__.output_config_schema(),
+            dagster_maintained=self._is_dagster_maintained(),
         )
 
     @classmethod
     def input_config_schema(
         cls,
     ) -> Optional[Union[CoercableToConfigSchema, Type[Config]]]:
         return None
@@ -1362,22 +1381,23 @@
             factory_cls: Type[ConfigurableIOManagerFactory] = cast(
                 Type[ConfigurableIOManagerFactory], self.resource_cls
             )
             input_config_schema = factory_cls.input_config_schema()
             output_config_schema = factory_cls.output_config_schema()
 
         return ConfigurableIOManagerFactoryResourceDefinition(
-            self.__class__,
+            self.resource_cls,
             resource_fn=self._state__internal__.resource_fn,
             config_schema=self._state__internal__.config_schema,
             description=self._state__internal__.description,
             resolve_resource_keys=self._resolve_required_resource_keys,
             nested_resources=self._state__internal__.nested_resources,
             input_config_schema=input_config_schema,
             output_config_schema=output_config_schema,
+            dagster_maintained=self.resource_cls._is_dagster_maintained(),  # noqa: SLF001
         )
 
 
 class ConfigurableIOManager(ConfigurableIOManagerFactory, IOManager):
     """Base class for Dagster IO managers that utilize structured config.
 
     This class is a subclass of both :py:class:`IOManagerDefinition`, :py:class:`Config`,
@@ -1625,14 +1645,15 @@
         return ConfigurableIOManagerFactoryResourceDefinition(
             self.__class__,
             resource_fn=self.wrapped_io_manager.resource_fn,
             config_schema=self._config_schema,
             description=self.__doc__,
             resolve_resource_keys=self._resolve_required_resource_keys,
             nested_resources=self.nested_resources,
+            dagster_maintained=self._is_dagster_maintained(),
         )
 
 
 def _convert_pydantic_descriminated_union_field(pydantic_field: ModelField) -> Field:
     """Builds a Selector config field from a Pydantic field which is a descriminated union.
 
     For example:
@@ -1749,26 +1770,44 @@
 
     docstring = model_cls.__doc__.strip() if model_cls.__doc__ else None
 
     return Field(config=shape_cls(fields), description=description or docstring)
 
 
 class SeparatedResourceParams(NamedTuple):
-    resources: Dict[str, CoercibleToResource]
+    resources: Dict[str, Any]
     non_resources: Dict[str, Any]
 
 
-def separate_resource_params(data: Dict[str, Any]) -> SeparatedResourceParams:
+def _is_annotated_as_resource_type(annotation: Type) -> bool:
+    """Determines if a field in a structured config class is annotated as a resource type or not."""
+    is_annotated_as_resource_dependency = get_origin(annotation) == ResourceDependency or getattr(
+        annotation, "__metadata__", None
+    ) == ("resource_dependency",)
+
+    return is_annotated_as_resource_dependency or safe_is_subclass(
+        annotation, (ResourceDefinition, ConfigurableResourceFactory)
+    )
+
+
+def separate_resource_params(cls: Type[BaseModel], data: Dict[str, Any]) -> SeparatedResourceParams:
     """Separates out the key/value inputs of fields in a structured config Resource class which
-    are themselves Resources and those which are not.
+    are marked as resources (ie, using ResourceDependency) from those which are not.
     """
-    return SeparatedResourceParams(
-        resources={k: v for k, v in data.items() if is_coercible_to_resource(v)},
-        non_resources={k: v for k, v in data.items() if not is_coercible_to_resource(v)},
+    keys_by_alias = {field.alias: field for field in cls.__fields__.values()}
+    data_with_annotation: List[Tuple[str, Any, Type]] = [
+        (k, v, keys_by_alias[k].annotation) for k, v in data.items() if k in keys_by_alias
+    ]
+    out = SeparatedResourceParams(
+        resources={k: v for k, v, t in data_with_annotation if _is_annotated_as_resource_type(t)},
+        non_resources={
+            k: v for k, v, t in data_with_annotation if not _is_annotated_as_resource_type(t)
+        },
     )
+    return out
 
 
 def _call_resource_fn_with_default(
     stack: contextlib.ExitStack, obj: ResourceDefinition, context: InitResourceContext
 ) -> Any:
     from dagster._config.validate import process_config
 
@@ -1788,22 +1827,22 @@
             raise DagsterInvalidConfigError(
                 "Error in config for nested resource ",
                 evr.errors,
                 unprocessed_config,
             )
         context = context.replace_config(cast(dict, evr.value)["config"])
 
-    is_fn_generator = inspect.isgenerator(obj.resource_fn) or isinstance(
-        obj.resource_fn, contextlib.ContextDecorator
-    )
-    if has_at_least_one_parameter(obj.resource_fn):  # type: ignore[unreachable]
+    if has_at_least_one_parameter(obj.resource_fn):
         result = cast(ResourceFunctionWithContext, obj.resource_fn)(context)
     else:
         result = cast(ResourceFunctionWithoutContext, obj.resource_fn)()
 
+    is_fn_generator = inspect.isgenerator(obj.resource_fn) or isinstance(
+        obj.resource_fn, contextlib.ContextDecorator
+    )
     if is_fn_generator:
         return stack.enter_context(cast(contextlib.AbstractContextManager, result))
     else:
         return result
 
 
 LateBoundTypesForResourceTypeChecking.set_actual_types_for_type_checking(
```

### Comparing `dagster-1.3.9rc0/dagster/_config/pythonic_config/attach_other_object_to_context.py` & `dagster-1.4.0/dagster/_config/pythonic_config/attach_other_object_to_context.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,30 +12,30 @@
     .. code-block:: python
 
         @resource(config_schema={"inner_string": str})
         def my_string_resource(context) -> str:
             return context.resource_config["inner_string"]
 
         @asset(required_resource_keys={"my_string"})
-        def my_unconverted_asset(context: OpExecutionContext) -> str:
+        def my_unconverted_asset(context: AssetExecutionContext) -> str:
             return context.resources.my_string
 
     Adapted to a class-style resource, we can ensure our new ConfigurableResource is compatible
     with the asset by implementing this interface:
 
     .. code-block:: python
 
         class MyStringResource(ConfigurableResource, IAttachDifferentObjectToOpContext):
             inner_string: str
 
             def get_object_to_set_on_execution_context(self) -> str:
                 return self.inner_string
 
         @asset(required_resource_keys={"my_string"})
-        def my_unconverted_asset(context: OpExecutionContext) -> str:
+        def my_unconverted_asset(context: AssetExecutionContext) -> str:
             return context.resources.my_string
 
         @asset
         def my_converted_asset(my_string: MyStringResource) -> str:
             return my_string.inner_string
     """
```

### Comparing `dagster-1.3.9rc0/dagster/_config/pythonic_config/typing_utils.py` & `dagster-1.4.0/dagster/_config/pythonic_config/typing_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import TYPE_CHECKING, Any, Generic, Optional, Type, TypeVar, Union, cast
 
 import pydantic
 from pydantic import Field
-from typing_extensions import dataclass_transform, get_origin
+from typing_extensions import Annotated, dataclass_transform, get_origin
 
 from dagster._core.errors import DagsterInvalidDagsterTypeInPythonicConfigDefinitionError
 
 from .utils import safe_is_subclass
 
 if TYPE_CHECKING:
     from dagster._config.pythonic_config import PartialResource
@@ -106,24 +106,28 @@
                 if (
                     get_origin(annotations[field])
                     == LateBoundTypesForResourceTypeChecking.get_resource_rep_type()
                 ):
                     # arg = get_args(annotations[field])[0]
                     # If so, we treat it as a Union of a PartialResource and a Resource
                     # for Pydantic's sake.
-                    annotations[field] = Any
+                    annotations[field] = Annotated[Any, "resource_dependency"]
                 elif safe_is_subclass(
                     annotations[field], LateBoundTypesForResourceTypeChecking.get_resource_type()
                 ):
                     # If the annotation is a Resource, we treat it as a Union of a PartialResource
                     # and a Resource for Pydantic's sake, so that a user can pass in a partially
                     # configured resource.
                     base = annotations[field]
-                    annotations[field] = Union[
-                        LateBoundTypesForResourceTypeChecking.get_partial_resource_type(base), base
+                    annotations[field] = Annotated[
+                        Union[
+                            LateBoundTypesForResourceTypeChecking.get_partial_resource_type(base),
+                            base,
+                        ],
+                        "resource_dependency",
                     ]
 
         namespaces["__annotations__"] = annotations
         return super().__new__(cls, name, bases, namespaces, **kwargs)
 
 
 Self = TypeVar("Self", bound="TypecheckAllowPartialResourceInitParams")
```

### Comparing `dagster-1.3.9rc0/dagster/_config/pythonic_config/utils.py` & `dagster-1.4.0/dagster/_config/pythonic_config/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/snap.py` & `dagster-1.4.0/dagster/_config/snap.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/source.py` & `dagster-1.4.0/dagster/_config/source.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/stack.py` & `dagster-1.4.0/dagster/_config/stack.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/traversal_context.py` & `dagster-1.4.0/dagster/_config/traversal_context.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/type_printer.py` & `dagster-1.4.0/dagster/_config/type_printer.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_config/validate.py` & `dagster-1.4.0/dagster/_config/validate.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/assets.py` & `dagster-1.4.0/dagster/_core/assets.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/code_pointer.py` & `dagster-1.4.0/dagster/_core/code_pointer.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/container_context/config.py` & `dagster-1.4.0/dagster/_core/container_context/config.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/debug.py` & `dagster-1.4.0/dagster/_core/debug.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/__init__.py` & `dagster-1.4.0/dagster/_core/definitions/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -109,15 +109,15 @@
 )
 from .sensor_definition import (
     DefaultSensorStatus as DefaultSensorStatus,
     SensorDefinition as SensorDefinition,
     SensorEvaluationContext as SensorEvaluationContext,
 )
 
-# isort: split
+# ruff: isort: split
 from .asset_in import AssetIn as AssetIn
 from .asset_out import AssetOut as AssetOut
 from .asset_selection import AssetSelection as AssetSelection
 from .assets import AssetsDefinition as AssetsDefinition
 from .assets_job import build_assets_job as build_assets_job
 from .decorators import (
     asset as asset,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_graph.py` & `dagster-1.4.0/dagster/_core/definitions/asset_graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,51 +29,69 @@
 from dagster._core.selector.subset_selector import DependencyGraph, generate_asset_dep_graph
 from dagster._utils.cached_method import cached_method
 
 from .assets import AssetsDefinition
 from .events import AssetKey, AssetKeyPartitionKey
 from .freshness_policy import FreshnessPolicy
 from .partition import PartitionsDefinition, PartitionsSubset
-from .partition_mapping import PartitionMapping, infer_partition_mapping
+from .partition_mapping import PartitionMapping, UpstreamPartitionsResult, infer_partition_mapping
 from .source_asset import SourceAsset
 from .time_window_partitions import (
     TimeWindowPartitionsDefinition,
     get_time_partition_key,
     get_time_partitions_def,
-    has_one_dimension_time_window_partitioning,
 )
 
 if TYPE_CHECKING:
     from dagster._core.definitions.asset_graph_subset import AssetGraphSubset
 
 
+class ParentsPartitionsResult(NamedTuple):
+    """Represents the result of mapping an asset partition to its upstream parent partitions.
+
+    parent_partitions (AbstractSet[AssetKeyPartitionKey]): The existent parent partitions that are
+        upstream of the asset partition. Filters out nonexistent partitions.
+    required_but_nonexistent_parents_partitions (AbstractSet[AssetKeyPartitionKey]): The required
+        upstream asset partitions that were mapped to but do not exist.
+    """
+
+    parent_partitions: AbstractSet[AssetKeyPartitionKey]
+    required_but_nonexistent_parents_partitions: AbstractSet[AssetKeyPartitionKey]
+
+
 class AssetGraph:
     def __init__(
         self,
         asset_dep_graph: DependencyGraph[AssetKey],
         source_asset_keys: AbstractSet[AssetKey],
         partitions_defs_by_key: Mapping[AssetKey, Optional[PartitionsDefinition]],
         partition_mappings_by_key: Mapping[AssetKey, Optional[Mapping[AssetKey, PartitionMapping]]],
         group_names_by_key: Mapping[AssetKey, Optional[str]],
         freshness_policies_by_key: Mapping[AssetKey, Optional[FreshnessPolicy]],
         auto_materialize_policies_by_key: Mapping[AssetKey, Optional[AutoMaterializePolicy]],
         required_multi_asset_sets_by_key: Optional[Mapping[AssetKey, AbstractSet[AssetKey]]],
         code_versions_by_key: Mapping[AssetKey, Optional[str]],
         is_observable_by_key: Mapping[AssetKey, bool],
+        auto_observe_interval_minutes_by_key: Mapping[AssetKey, Optional[float]],
     ):
         self._asset_dep_graph = asset_dep_graph
         self._source_asset_keys = source_asset_keys
         self._partitions_defs_by_key = partitions_defs_by_key
         self._partition_mappings_by_key = partition_mappings_by_key
         self._group_names_by_key = group_names_by_key
         self._freshness_policies_by_key = freshness_policies_by_key
         self._auto_materialize_policies_by_key = auto_materialize_policies_by_key
         self._required_multi_asset_sets_by_key = required_multi_asset_sets_by_key
         self._code_versions_by_key = code_versions_by_key
         self._is_observable_by_key = is_observable_by_key
+        self._auto_observe_interval_minutes_by_key = auto_observe_interval_minutes_by_key
+        # source assets keys can sometimes appear in the upstream dict
+        self._materializable_asset_keys = (
+            self._asset_dep_graph["upstream"].keys() - self.source_asset_keys
+        )
 
     @property
     def asset_dep_graph(self) -> DependencyGraph[AssetKey]:
         return self._asset_dep_graph
 
     @property
     def group_names_by_key(self) -> Mapping[AssetKey, Optional[str]]:
@@ -84,26 +102,29 @@
         return self._source_asset_keys
 
     @property
     def root_asset_keys(self) -> AbstractSet[AssetKey]:
         """Non-source asset keys that have no non-source parents."""
         from .asset_selection import AssetSelection
 
-        return AssetSelection.keys(*self.non_source_asset_keys).roots().resolve(self)
+        return AssetSelection.keys(*self.materializable_asset_keys).roots().resolve(self)
 
     @property
     def freshness_policies_by_key(self) -> Mapping[AssetKey, Optional[FreshnessPolicy]]:
         return self._freshness_policies_by_key
 
     @property
     def auto_materialize_policies_by_key(
         self,
     ) -> Mapping[AssetKey, Optional[AutoMaterializePolicy]]:
         return self._auto_materialize_policies_by_key
 
+    def get_auto_observe_interval_minutes(self, asset_key: AssetKey) -> Optional[float]:
+        return self._auto_observe_interval_minutes_by_key.get(asset_key)
+
     @staticmethod
     def from_assets(
         all_assets: Iterable[Union[AssetsDefinition, SourceAsset]]
     ) -> "InternalAssetGraph":
         assets_defs: List[AssetsDefinition] = []
         source_assets: List[SourceAsset] = []
         partitions_defs_by_key: Dict[AssetKey, Optional[PartitionsDefinition]] = {}
@@ -112,21 +133,25 @@
         ] = {}
         group_names_by_key: Dict[AssetKey, Optional[str]] = {}
         freshness_policies_by_key: Dict[AssetKey, Optional[FreshnessPolicy]] = {}
         auto_materialize_policies_by_key: Dict[AssetKey, Optional[AutoMaterializePolicy]] = {}
         required_multi_asset_sets_by_key: Dict[AssetKey, AbstractSet[AssetKey]] = {}
         code_versions_by_key: Dict[AssetKey, Optional[str]] = {}
         is_observable_by_key: Dict[AssetKey, bool] = {}
+        auto_observe_interval_minutes_by_key: Dict[AssetKey, Optional[float]] = {}
 
         for asset in all_assets:
             if isinstance(asset, SourceAsset):
                 source_assets.append(asset)
                 partitions_defs_by_key[asset.key] = asset.partitions_def
                 group_names_by_key[asset.key] = asset.group_name
                 is_observable_by_key[asset.key] = asset.is_observable
+                auto_observe_interval_minutes_by_key[
+                    asset.key
+                ] = asset.auto_observe_interval_minutes
             else:  # AssetsDefinition
                 assets_defs.append(asset)
                 partition_mappings_by_key.update(
                     {key: asset.partition_mappings for key in asset.keys}
                 )
                 partitions_defs_by_key.update({key: asset.partitions_def for key in asset.keys})
                 group_names_by_key.update(asset.group_names_by_key)
@@ -146,23 +171,24 @@
             freshness_policies_by_key=freshness_policies_by_key,
             auto_materialize_policies_by_key=auto_materialize_policies_by_key,
             required_multi_asset_sets_by_key=required_multi_asset_sets_by_key,
             assets=assets_defs,
             source_assets=source_assets,
             code_versions_by_key=code_versions_by_key,
             is_observable_by_key=is_observable_by_key,
+            auto_observe_interval_minutes_by_key=auto_observe_interval_minutes_by_key,
         )
 
     @property
-    def all_asset_keys(self) -> AbstractSet[AssetKey]:
-        return self._asset_dep_graph["upstream"].keys()
+    def materializable_asset_keys(self) -> AbstractSet[AssetKey]:
+        return self._materializable_asset_keys
 
     @property
-    def non_source_asset_keys(self) -> AbstractSet[AssetKey]:
-        return self._asset_dep_graph["upstream"].keys() - self._source_asset_keys
+    def all_asset_keys(self) -> AbstractSet[AssetKey]:
+        return self._materializable_asset_keys | self.source_asset_keys
 
     def get_partitions_def(self, asset_key: AssetKey) -> Optional[PartitionsDefinition]:
         return self._partitions_defs_by_key.get(asset_key)
 
     def get_partition_mapping(
         self, asset_key: AssetKey, in_asset_key: AssetKey
     ) -> PartitionMapping:
@@ -195,16 +221,26 @@
         return self._is_observable_by_key.get(asset_key, False)
 
     def get_children(self, asset_key: AssetKey) -> AbstractSet[AssetKey]:
         """Returns all assets that depend on the given asset."""
         return self._asset_dep_graph["downstream"][asset_key]
 
     def get_parents(self, asset_key: AssetKey) -> AbstractSet[AssetKey]:
-        """Returns all assets that the given asset depends on."""
-        return self._asset_dep_graph["upstream"][asset_key]
+        """Returns all first-order dependencies of an asset."""
+        return self._asset_dep_graph["upstream"].get(asset_key) or set()
+
+    def get_ancestors(
+        self, asset_key: AssetKey, include_self: bool = False
+    ) -> AbstractSet[AssetKey]:
+        """Returns all nth-order dependencies of an asset."""
+        ancestors = {asset_key} if include_self else set()
+        parents = self.get_parents(asset_key) - {asset_key}  # remove self-dependencies
+        return ancestors.union(
+            *[self.get_ancestors(parent, include_self=True) for parent in parents]
+        )
 
     def get_children_partitions(
         self,
         dynamic_partitions_store: DynamicPartitionsStore,
         current_time: datetime,
         asset_key: AssetKey,
         partition_key: Optional[str] = None,
@@ -279,41 +315,57 @@
 
     def get_parents_partitions(
         self,
         dynamic_partitions_store: DynamicPartitionsStore,
         current_time: datetime,
         asset_key: AssetKey,
         partition_key: Optional[str] = None,
-    ) -> AbstractSet[AssetKeyPartitionKey]:
+    ) -> ParentsPartitionsResult:
         """Returns every partition in every of the given asset's parents that the given partition of
         that asset depends on.
         """
-        result: Set[AssetKeyPartitionKey] = set()
+        valid_parent_partitions: Set[AssetKeyPartitionKey] = set()
+        required_but_nonexistent_parent_partitions: Set[AssetKeyPartitionKey] = set()
         for parent_asset_key in self.get_parents(asset_key):
             if self.is_partitioned(parent_asset_key):
-                for parent_partition_key in self.get_parent_partition_keys_for_child(
+                mapped_partitions_result = self.get_parent_partition_keys_for_child(
                     partition_key,
                     parent_asset_key,
                     asset_key,
                     dynamic_partitions_store=dynamic_partitions_store,
                     current_time=current_time,
-                ):
-                    result.add(AssetKeyPartitionKey(parent_asset_key, parent_partition_key))
+                )
+
+                valid_parent_partitions.update(
+                    {
+                        AssetKeyPartitionKey(parent_asset_key, valid_partition)
+                        for valid_partition in mapped_partitions_result.partitions_subset.get_partition_keys()
+                    }
+                )
+                required_but_nonexistent_parent_partitions.update(
+                    {
+                        AssetKeyPartitionKey(parent_asset_key, invalid_partition)
+                        for invalid_partition in mapped_partitions_result.required_but_nonexistent_partition_keys
+                    }
+                )
             else:
-                result.add(AssetKeyPartitionKey(parent_asset_key))
-        return result
+                valid_parent_partitions.add(AssetKeyPartitionKey(parent_asset_key))
+
+        return ParentsPartitionsResult(
+            valid_parent_partitions, required_but_nonexistent_parent_partitions
+        )
 
     def get_parent_partition_keys_for_child(
         self,
         partition_key: Optional[str],
         parent_asset_key: AssetKey,
         child_asset_key: AssetKey,
         dynamic_partitions_store: DynamicPartitionsStore,
         current_time: datetime,
-    ) -> Sequence[str]:
+    ) -> UpstreamPartitionsResult:
         """Converts a partition key from one asset to the corresponding partition keys in one of its
         parent assets. Uses the existing partition mapping between the child asset and the parent
         asset.
 
         Args:
             partition_key (Optional[str]): The partition key to convert.
             child_asset_key (AssetKey): The asset key of the child asset, which the provided
@@ -332,28 +384,30 @@
 
         if parent_partitions_def is None:
             raise DagsterInvalidInvocationError(
                 f"Asset key {parent_asset_key} is not partitioned. Cannot get partition keys."
             )
 
         partition_mapping = self.get_partition_mapping(child_asset_key, parent_asset_key)
-        parent_partition_key_subset = partition_mapping.get_upstream_partitions_for_partitions(
-            cast(PartitionsDefinition, child_partitions_def)
-            .empty_subset()
-            .with_partition_keys([partition_key])
+
+        return partition_mapping.get_upstream_mapped_partitions_result_for_partitions(
+            cast(PartitionsDefinition, child_partitions_def).subset_with_partition_keys(
+                [partition_key]
+            )
             if partition_key
             else None,
             upstream_partitions_def=parent_partitions_def,
             dynamic_partitions_store=dynamic_partitions_store,
             current_time=current_time,
         )
-        return list(parent_partition_key_subset.get_partition_keys())
 
     def is_source(self, asset_key: AssetKey) -> bool:
-        return asset_key in self.source_asset_keys or asset_key not in self.all_asset_keys
+        return (
+            asset_key in self.source_asset_keys or asset_key not in self.materializable_asset_keys
+        )
 
     def has_non_source_parents(self, asset_key: AssetKey) -> bool:
         """Determines if an asset has any parents which are not source assets."""
         if self.is_source(asset_key):
             return False
         return any(
             not self.is_source(parent_key)
@@ -576,26 +630,28 @@
         freshness_policies_by_key: Mapping[AssetKey, Optional[FreshnessPolicy]],
         auto_materialize_policies_by_key: Mapping[AssetKey, Optional[AutoMaterializePolicy]],
         required_multi_asset_sets_by_key: Optional[Mapping[AssetKey, AbstractSet[AssetKey]]],
         assets: Sequence[AssetsDefinition],
         source_assets: Sequence[SourceAsset],
         code_versions_by_key: Mapping[AssetKey, Optional[str]],
         is_observable_by_key: Mapping[AssetKey, bool],
+        auto_observe_interval_minutes_by_key: Mapping[AssetKey, Optional[float]],
     ):
         super().__init__(
             asset_dep_graph=asset_dep_graph,
             source_asset_keys=source_asset_keys,
             partitions_defs_by_key=partitions_defs_by_key,
             partition_mappings_by_key=partition_mappings_by_key,
             group_names_by_key=group_names_by_key,
             freshness_policies_by_key=freshness_policies_by_key,
             auto_materialize_policies_by_key=auto_materialize_policies_by_key,
             required_multi_asset_sets_by_key=required_multi_asset_sets_by_key,
             code_versions_by_key=code_versions_by_key,
             is_observable_by_key=is_observable_by_key,
+            auto_observe_interval_minutes_by_key=auto_observe_interval_minutes_by_key,
         )
         self._assets = assets
         self._source_assets = source_assets
 
     @property
     def assets(self) -> Sequence[AssetsDefinition]:
         return self._assets
@@ -669,34 +725,34 @@
             # A sort key such that time window partitions are sorted from oldest to newest
             return pendulum.instance(
                 datetime.strptime(time_partition_key, partitions_def.fmt),
                 tz=partitions_def.timezone,
             ).timestamp()
 
         partitions_def = self._asset_graph.get_partitions_def(asset_key)
+        time_partitions_def = get_time_partitions_def(partitions_def)
         if self._asset_graph.has_self_dependency(asset_key):
-            if partitions_def is None or not has_one_dimension_time_window_partitioning(
-                partitions_def
-            ):
+            if time_partitions_def is None:
                 check.failed(
                     "Assets with self-dependencies must have time-window partitions, but"
                     f" {asset_key} does not."
                 )
 
             # sort self dependencies from oldest to newest, as older partitions must exist before
             # new ones can execute
             partition_sort_key = _sort_key_for_time_window_partition(
-                get_time_partitions_def(partitions_def),
+                time_partitions_def,
                 get_time_partition_key(partitions_def, asset_partition.partition_key),
             )
-        elif isinstance(partitions_def, TimeWindowPartitionsDefinition):
+        elif isinstance(time_partitions_def, TimeWindowPartitionsDefinition):
             # sort non-self dependencies from newest to oldest, as newer partitions are more relevant
             # than older ones
             partition_sort_key = -1 * _sort_key_for_time_window_partition(
-                partitions_def, cast(str, asset_partition.partition_key)
+                time_partitions_def,
+                get_time_partition_key(partitions_def, asset_partition.partition_key),
             )
         else:
             partition_sort_key = None
 
         return ToposortedPriorityQueue.QueueItem(
             level,
             partition_sort_key,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_graph_subset.py` & `dagster-1.4.0/dagster/_core/definitions/asset_graph_subset.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,17 @@
 from typing import AbstractSet, Any, Dict, Iterable, Mapping, Optional, Set, Union, cast
 
 from dagster import _check as check
 from dagster._core.definitions.partition import (
     PartitionsDefinition,
     PartitionsSubset,
 )
-from dagster._core.errors import DagsterDefinitionChangedDeserializationError
+from dagster._core.errors import (
+    DagsterDefinitionChangedDeserializationError,
+)
 from dagster._core.instance import DynamicPartitionsStore
 
 from .asset_graph import AssetGraph
 from .events import AssetKey, AssetKeyPartitionKey
 
 
 class AssetGraphSubset:
@@ -201,14 +203,20 @@
     @classmethod
     def from_storage_dict(
         cls, serialized_dict: Mapping[str, Any], asset_graph: AssetGraph
     ) -> "AssetGraphSubset":
         partitions_subsets_by_asset_key: Dict[AssetKey, PartitionsSubset] = {}
         for key, value in serialized_dict["partitions_subsets_by_asset_key"].items():
             asset_key = AssetKey.from_user_string(key)
+
+            if asset_key not in asset_graph.all_asset_keys:
+                raise DagsterDefinitionChangedDeserializationError(
+                    f"Asset {key} existed at storage-time, but no longer does"
+                )
+
             partitions_def = asset_graph.get_partitions_def(asset_key)
 
             if partitions_def is None:
                 raise DagsterDefinitionChangedDeserializationError(
                     f"Asset {key} had a PartitionsDefinition at storage-time, but no longer does"
                 )
 
@@ -223,15 +231,15 @@
         )
 
     @classmethod
     def all(
         cls, asset_graph: AssetGraph, dynamic_partitions_store: DynamicPartitionsStore
     ) -> "AssetGraphSubset":
         return cls.from_asset_keys(
-            asset_graph.non_source_asset_keys, asset_graph, dynamic_partitions_store
+            asset_graph.materializable_asset_keys, asset_graph, dynamic_partitions_store
         )
 
     @classmethod
     def from_asset_keys(
         cls,
         asset_keys: Iterable[AssetKey],
         asset_graph: AssetGraph,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_in.py` & `dagster-1.4.0/dagster/_core/definitions/asset_in.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,15 +6,14 @@
     AssetKey,
     CoercibleToAssetKey,
     CoercibleToAssetKeyPrefix,
 )
 from dagster._core.definitions.input import NoValueSentinel
 from dagster._core.definitions.metadata import ArbitraryMetadataMapping
 from dagster._core.types.dagster_type import DagsterType, resolve_dagster_type
-from dagster._utils.backcompat import canonicalize_backcompat_args
 
 from .partition_mapping import PartitionMapping
 
 
 class AssetIn(
     NamedTuple(
         "_AssetIn",
@@ -49,20 +48,18 @@
     """
 
     def __new__(
         cls,
         key: Optional[CoercibleToAssetKey] = None,
         metadata: Optional[ArbitraryMetadataMapping] = None,
         key_prefix: Optional[CoercibleToAssetKeyPrefix] = None,
-        asset_key: Optional[CoercibleToAssetKey] = None,
         input_manager_key: Optional[str] = None,
         partition_mapping: Optional[PartitionMapping] = None,
         dagster_type: Union[DagsterType, Type[NoValueSentinel]] = NoValueSentinel,
     ):
-        key = canonicalize_backcompat_args(key, "key", asset_key, "asset_key", "1.0.0")
         if isinstance(key_prefix, str):
             key_prefix = [key_prefix]
 
         check.invariant(
             not (key and key_prefix), "key and key_prefix cannot both be set on AssetIn"
         )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_layer.py` & `dagster-1.4.0/dagster/_core/definitions/asset_layer.py`

 * *Files 0% similar despite different names*

```diff
@@ -315,15 +315,15 @@
                     dep_node_output_handles_by_node,
                     node_output_handle,
                 )
 
                 dep_node_outputs_by_asset_key[asset_key].extend(dep_node_output_handles)
 
     # handle internal_asset_deps within graph-backed assets
-    for _, assets_def in assets_defs_by_node_handle.items():
+    for assets_def in assets_defs_by_node_handle.values():
         for asset_key, dep_asset_keys in assets_def.asset_deps.items():
             if asset_key not in assets_def.keys:
                 continue
             for dep_asset_key in [key for key in dep_asset_keys if key in assets_def.keys]:
                 if len(dep_node_outputs_by_asset_key[asset_key]) == 0:
                     # This case occurs when the asset is not yielded from a graph-backed asset
                     continue
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_out.py` & `dagster-1.4.0/dagster/_core/definitions/asset_out.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_reconciliation_sensor.py` & `dagster-1.4.0/dagster/_core/definitions/asset_reconciliation_sensor.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,57 +3,59 @@
 import json
 from collections import defaultdict
 from typing import (
     TYPE_CHECKING,
     AbstractSet,
     Callable,
     Dict,
+    FrozenSet,
     Iterable,
     Mapping,
     NamedTuple,
     Optional,
     Sequence,
     Set,
     Tuple,
-    Union,
     cast,
 )
 
 import pendulum
 
 import dagster._check as check
 from dagster._annotations import experimental
-from dagster._core.definitions.asset_graph_subset import AssetGraphSubset
 from dagster._core.definitions.auto_materialize_policy import AutoMaterializePolicy
 from dagster._core.definitions.data_time import CachingDataTimeResolver
 from dagster._core.definitions.events import AssetKey, AssetKeyPartitionKey
-from dagster._core.definitions.freshness_policy import FreshnessPolicy
-from dagster._core.definitions.multi_dimensional_partitions import MultiPartitionsDefinition
 from dagster._core.definitions.time_window_partitions import (
-    TimeWindow,
     TimeWindowPartitionsDefinition,
-    has_one_dimension_time_window_partitioning,
+    get_time_partitions_def,
 )
 from dagster._serdes.serdes import whitelist_for_serdes
 from dagster._utils.backcompat import deprecation_warning
-from dagster._utils.schedules import cron_string_iterator
 
-from .asset_selection import AssetGraph, AssetSelection
+from .asset_graph import AssetGraph
+from .asset_selection import AssetSelection
 from .auto_materialize_condition import (
     AutoMaterializeCondition,
     AutoMaterializeDecisionType,
-    DownstreamFreshnessAutoMaterializeCondition,
-    FreshnessAutoMaterializeCondition,
     MaxMaterializationsExceededAutoMaterializeCondition,
     MissingAutoMaterializeCondition,
     ParentMaterializedAutoMaterializeCondition,
     ParentOutdatedAutoMaterializeCondition,
 )
 from .decorators.sensor_decorator import sensor
-from .partition import PartitionsDefinition, PartitionsSubset, SerializedPartitionsSubset
+from .freshness_based_auto_materialize import (
+    determine_asset_partitions_to_auto_materialize_for_freshness,
+)
+from .partition import (
+    PartitionsDefinition,
+    PartitionsSubset,
+    ScheduleType,
+    SerializedPartitionsSubset,
+)
 from .run_request import RunRequest
 from .sensor_definition import DefaultSensorStatus, SensorDefinition
 from .utils import check_valid_name
 
 if TYPE_CHECKING:
     from dagster._core.instance import DagsterInstance, DynamicPartitionsStore
     from dagster._utils.caching_instance_queryer import CachingInstanceQueryer  # expensive import
@@ -74,14 +76,15 @@
     asset_key: AssetKey
     partition_subsets_by_condition: Sequence[
         Tuple[AutoMaterializeCondition, Optional[SerializedPartitionsSubset]]
     ]
     num_requested: int
     num_skipped: int
     num_discarded: int
+    run_ids: Set[str] = set()
 
     @staticmethod
     def from_conditions(
         asset_graph: AssetGraph,
         asset_key: AssetKey,
         conditions_by_asset_partition: Mapping[
             AssetKeyPartitionKey, AbstractSet[AutoMaterializeCondition]
@@ -89,22 +92,21 @@
         dynamic_partitions_store: "DynamicPartitionsStore",
     ) -> "AutoMaterializeAssetEvaluation":
         num_requested = 0
         num_skipped = 0
         num_discarded = 0
 
         for conditions in conditions_by_asset_partition.values():
-            decision_types = {condition.decision_type for condition in conditions}
-            if AutoMaterializeDecisionType.DISCARD in decision_types:
-                num_discarded += 1
-            elif AutoMaterializeDecisionType.SKIP in decision_types:
-                num_skipped += 1
-            else:
-                check.invariant(AutoMaterializeDecisionType.MATERIALIZE in decision_types)
+            decision_type = _decision_type_for_conditions(conditions)
+            if decision_type == AutoMaterializeDecisionType.MATERIALIZE:
                 num_requested += 1
+            elif decision_type == AutoMaterializeDecisionType.SKIP:
+                num_skipped += 1
+            elif decision_type == AutoMaterializeDecisionType.DISCARD:
+                num_discarded += 1
 
         partitions_def = asset_graph.get_partitions_def(asset_key)
         if partitions_def is None:
             return AutoMaterializeAssetEvaluation(
                 asset_key=asset_key,
                 partition_subsets_by_condition=[
                     (condition, None)
@@ -146,395 +148,337 @@
 
 def get_implicit_auto_materialize_policy(
     asset_graph: AssetGraph, asset_key: AssetKey
 ) -> Optional[AutoMaterializePolicy]:
     """For backcompat with pre-auto materialize policy graphs, assume a default scope of 1 day."""
     auto_materialize_policy = asset_graph.get_auto_materialize_policy(asset_key)
     if auto_materialize_policy is None:
+        time_partitions_def = get_time_partitions_def(asset_graph.get_partitions_def(asset_key))
+        if time_partitions_def is None:
+            max_materializations_per_minute = None
+        elif time_partitions_def.schedule_type == ScheduleType.HOURLY:
+            max_materializations_per_minute = 24
+        else:
+            max_materializations_per_minute = 1
         return AutoMaterializePolicy(
             on_missing=True,
             on_new_parent_data=not bool(
                 asset_graph.get_downstream_freshness_policies(asset_key=asset_key)
             ),
             for_freshness=True,
-            time_window_partition_scope_minutes=24 * 60,
-            max_materializations_per_minute=None,
+            max_materializations_per_minute=max_materializations_per_minute,
         )
     return auto_materialize_policy
 
 
-def reconciliation_window_for_time_window_partitions(
-    partitions_def: Union[TimeWindowPartitionsDefinition, MultiPartitionsDefinition],
-    time_window_partition_scope: Optional[datetime.timedelta],
-    current_time: datetime.datetime,
-) -> Optional[TimeWindow]:
-    if isinstance(partitions_def, MultiPartitionsDefinition):
-        time_partitions_def = cast(
-            TimeWindowPartitionsDefinition, partitions_def.time_window_dimension.partitions_def
-        )
-    else:
-        time_partitions_def = partitions_def
-
-    latest_partition_window = time_partitions_def.get_last_partition_window(
-        current_time=current_time
-    )
-    earliest_partition_window = time_partitions_def.get_first_partition_window(
-        current_time=current_time
-    )
-    if latest_partition_window is None or earliest_partition_window is None:
-        return None
-
-    allowable_start_time = (
-        max(
-            earliest_partition_window.start,
-            latest_partition_window.start
-            - time_window_partition_scope
-            + datetime.timedelta.resolution,
-        )
-        if time_window_partition_scope is not None
-        else earliest_partition_window.start
-    )
-    return TimeWindow(allowable_start_time, latest_partition_window.end)
-
-
-def can_reconcile_time_window_partition(
-    partitions_def: Union[TimeWindowPartitionsDefinition, MultiPartitionsDefinition],
-    partition_key: Optional[str],
-    time_window_partition_scope: Optional[datetime.timedelta],
-    current_time: datetime.datetime,
-) -> bool:
-    if partition_key is None:
-        return False
-    if time_window_partition_scope is None:
-        return True
-    reconciliation_window = reconciliation_window_for_time_window_partitions(
-        partitions_def=partitions_def,
-        time_window_partition_scope=time_window_partition_scope,
-        current_time=current_time,
-    )
-    if reconciliation_window is None:
-        return False
-    key_window = partitions_def.time_window_for_partition_key(partition_key)
-    return (
-        key_window.start >= reconciliation_window.start
-        and key_window.end <= reconciliation_window.end
-    )
-
-
 class AssetReconciliationCursor(NamedTuple):
-    """Attributes:
-    latest_storage_id: The latest observed storage ID across all assets. Useful for
-        finding out what has happened since the last tick.
-    materialized_or_requested_root_asset_keys: Every entry is a non-partitioned asset with no
-        parents that has been requested by this sensor or has been materialized (even if not by
-        this sensor).
-    materialized_or_requested_root_partitions_by_asset_key: Every key is a partitioned root
-        asset. Every value is the set of that asset's partitoins that have been requested by
-        this sensor or have been materialized (even if not by this sensor).
+    """State that's saved between reconciliation evaluations.
+
+    Attributes:
+        latest_storage_id:
+            The latest observed storage ID across all assets. Useful for finding out what has
+            happened since the last tick.
+        handled_root_asset_keys:
+            Every entry is a non-partitioned asset with no parents that has been requested by this
+            sensor, discarded by this sensor, or has been materialized (even if not by this sensor).
+        handled_root_partitions_by_asset_key:
+            Every key is a partitioned root asset. Every value is the set of that asset's partitions
+            that have been requested by this sensor, discarded by this sensor,
+            or have been materialized (even if not by this sensor).
+        last_observe_request_timestamp_by_asset_key:
+            Every key is an observable source asset that has been auto-observed. The value is the
+            timestamp of the tick that requested the observation.
     """
 
     latest_storage_id: Optional[int]
-    materialized_or_requested_root_asset_keys: AbstractSet[AssetKey]
-    materialized_or_requested_root_partitions_by_asset_key: Mapping[AssetKey, PartitionsSubset]
+    handled_root_asset_keys: AbstractSet[AssetKey]
+    handled_root_partitions_by_asset_key: Mapping[AssetKey, PartitionsSubset]
     evaluation_id: int
+    last_observe_request_timestamp_by_asset_key: Mapping[AssetKey, float]
 
-    def was_previously_materialized_or_requested(self, asset_key: AssetKey) -> bool:
-        return asset_key in self.materialized_or_requested_root_asset_keys
+    def was_previously_handled(self, asset_key: AssetKey) -> bool:
+        return asset_key in self.handled_root_asset_keys
 
-    def get_never_requested_never_materialized_partitions(
+    def get_unhandled_partitions(
         self,
         asset_key: AssetKey,
         asset_graph,
         dynamic_partitions_store: "DynamicPartitionsStore",
         current_time: datetime.datetime,
-        time_window_partition_scope: Optional[datetime.timedelta],
     ) -> Iterable[str]:
         partitions_def = asset_graph.get_partitions_def(asset_key)
 
-        materialized_or_requested_subset = (
-            self.materialized_or_requested_root_partitions_by_asset_key.get(
-                asset_key, partitions_def.empty_subset()
-            )
+        handled_subset = self.handled_root_partitions_by_asset_key.get(
+            asset_key, partitions_def.empty_subset()
         )
 
-        if isinstance(partitions_def, TimeWindowPartitionsDefinition):
-            # for performance, only iterate over keys within the allowable time window
-            reconciliation_window = reconciliation_window_for_time_window_partitions(
-                partitions_def=partitions_def,
-                time_window_partition_scope=time_window_partition_scope,
-                current_time=current_time,
-            )
-            if reconciliation_window is None:
-                return []
-            # for performance, only iterate over keys within the allowable time window
-            return [
-                partition_key
-                for partition_key in partitions_def.get_partition_keys_in_time_window(
-                    reconciliation_window
-                )
-                if partition_key not in materialized_or_requested_subset
-            ]
-        else:
-            return materialized_or_requested_subset.get_partition_keys_not_in_subset(
-                current_time=current_time,
-                dynamic_partitions_store=dynamic_partitions_store,
-            )
+        return handled_subset.get_partition_keys_not_in_subset(
+            current_time=current_time,
+            dynamic_partitions_store=dynamic_partitions_store,
+        )
 
     def with_updates(
         self,
         latest_storage_id: Optional[int],
-        run_requests: Sequence[RunRequest],
+        conditions_by_asset_partition: Mapping[
+            AssetKeyPartitionKey, AbstractSet[AutoMaterializeCondition]
+        ],
         newly_materialized_root_asset_keys: AbstractSet[AssetKey],
         newly_materialized_root_partitions_by_asset_key: Mapping[AssetKey, AbstractSet[str]],
         evaluation_id: int,
         asset_graph: AssetGraph,
+        newly_observe_requested_asset_keys: Sequence[AssetKey],
+        observe_request_timestamp: float,
     ) -> "AssetReconciliationCursor":
         """Returns a cursor that represents this cursor plus the updates that have happened within the
         tick.
         """
-        requested_root_partitions_by_asset_key: Dict[AssetKey, Set[str]] = defaultdict(set)
-        requested_non_partitioned_root_assets: Set[AssetKey] = set()
+        handled_root_partitions_by_asset_key: Dict[AssetKey, Set[str]] = defaultdict(set)
+        handled_non_partitioned_root_assets: Set[AssetKey] = set()
 
-        for run_request in run_requests:
-            for asset_key in cast(Iterable[AssetKey], run_request.asset_selection):
-                if not asset_graph.has_non_source_parents(asset_key):
-                    if run_request.partition_key:
-                        requested_root_partitions_by_asset_key[asset_key].add(
-                            run_request.partition_key
-                        )
-                    else:
-                        requested_non_partitioned_root_assets.add(asset_key)
+        for asset_partition, conditions in conditions_by_asset_partition.items():
+            if (
+                # only consider root assets
+                not asset_graph.has_non_source_parents(asset_partition.asset_key)
+                # which were discarded or materialized
+                and (
+                    _decision_type_for_conditions(conditions)
+                    in (
+                        AutoMaterializeDecisionType.DISCARD,
+                        AutoMaterializeDecisionType.MATERIALIZE,
+                    )
+                )
+            ):
+                if asset_partition.partition_key:
+                    handled_root_partitions_by_asset_key[asset_partition.asset_key].add(
+                        asset_partition.partition_key
+                    )
+                else:
+                    handled_non_partitioned_root_assets.add(asset_partition.asset_key)
 
-        result_materialized_or_requested_root_partitions_by_asset_key = {
-            **self.materialized_or_requested_root_partitions_by_asset_key
-        }
+        result_handled_root_partitions_by_asset_key = {**self.handled_root_partitions_by_asset_key}
         for asset_key in set(newly_materialized_root_partitions_by_asset_key.keys()) | set(
-            requested_root_partitions_by_asset_key.keys()
+            handled_root_partitions_by_asset_key.keys()
         ):
-            prior_materialized_partitions = (
-                self.materialized_or_requested_root_partitions_by_asset_key.get(asset_key)
-            )
+            prior_materialized_partitions = self.handled_root_partitions_by_asset_key.get(asset_key)
             if prior_materialized_partitions is None:
                 prior_materialized_partitions = cast(
                     PartitionsDefinition, asset_graph.get_partitions_def(asset_key)
                 ).empty_subset()
 
-            result_materialized_or_requested_root_partitions_by_asset_key[
+            result_handled_root_partitions_by_asset_key[
                 asset_key
             ] = prior_materialized_partitions.with_partition_keys(
                 itertools.chain(
                     newly_materialized_root_partitions_by_asset_key[asset_key],
-                    requested_root_partitions_by_asset_key[asset_key],
+                    handled_root_partitions_by_asset_key[asset_key],
                 )
             )
 
-        result_materialized_or_requested_root_asset_keys = (
-            self.materialized_or_requested_root_asset_keys
+        result_handled_root_asset_keys = (
+            self.handled_root_asset_keys
             | newly_materialized_root_asset_keys
-            | requested_non_partitioned_root_assets
+            | handled_non_partitioned_root_assets
         )
 
+        result_last_observe_request_timestamp_by_asset_key = {
+            **self.last_observe_request_timestamp_by_asset_key
+        }
+        for asset_key in newly_observe_requested_asset_keys:
+            result_last_observe_request_timestamp_by_asset_key[
+                asset_key
+            ] = observe_request_timestamp
+
         if latest_storage_id and self.latest_storage_id:
             check.invariant(
                 latest_storage_id >= self.latest_storage_id,
                 "Latest storage ID should be >= previous latest storage ID",
             )
 
         return AssetReconciliationCursor(
             latest_storage_id=latest_storage_id or self.latest_storage_id,
-            materialized_or_requested_root_asset_keys=result_materialized_or_requested_root_asset_keys,
-            materialized_or_requested_root_partitions_by_asset_key=result_materialized_or_requested_root_partitions_by_asset_key,
+            handled_root_asset_keys=result_handled_root_asset_keys,
+            handled_root_partitions_by_asset_key=result_handled_root_partitions_by_asset_key,
             evaluation_id=evaluation_id,
+            last_observe_request_timestamp_by_asset_key=result_last_observe_request_timestamp_by_asset_key,
         )
 
     @classmethod
     def empty(cls) -> "AssetReconciliationCursor":
         return AssetReconciliationCursor(
             latest_storage_id=None,
-            materialized_or_requested_root_partitions_by_asset_key={},
-            materialized_or_requested_root_asset_keys=set(),
+            handled_root_partitions_by_asset_key={},
+            handled_root_asset_keys=set(),
             evaluation_id=0,
+            last_observe_request_timestamp_by_asset_key={},
         )
 
     @classmethod
     def from_serialized(cls, cursor: str, asset_graph: AssetGraph) -> "AssetReconciliationCursor":
         data = json.loads(cursor)
-        check.invariant(len(data) in [3, 4], "Invalid serialized cursor")
 
-        (
-            latest_storage_id,
-            serialized_materialized_or_requested_root_asset_keys,
-            serialized_materialized_or_requested_root_partitions_by_asset_key,
-        ) = data[:3]
+        if isinstance(data, list):  # backcompat
+            check.invariant(len(data) in [3, 4], "Invalid serialized cursor")
+            (
+                latest_storage_id,
+                serialized_handled_root_asset_keys,
+                serialized_handled_root_partitions_by_asset_key,
+            ) = data[:3]
 
-        evaluation_id = data[3] if len(data) == 4 else 0
+            evaluation_id = data[3] if len(data) == 4 else 0
+            serialized_last_observe_request_timestamp_by_asset_key = {}
+        else:
+            latest_storage_id = data["latest_storage_id"]
+            serialized_handled_root_asset_keys = data["handled_root_asset_keys"]
+            serialized_handled_root_partitions_by_asset_key = data[
+                "handled_root_partitions_by_asset_key"
+            ]
+            evaluation_id = data["evaluation_id"]
+            serialized_last_observe_request_timestamp_by_asset_key = data.get(
+                "last_observe_request_timestamp_by_asset_key", {}
+            )
 
-        materialized_or_requested_root_partitions_by_asset_key = {}
+        handled_root_partitions_by_asset_key = {}
         for (
             key_str,
             serialized_subset,
-        ) in serialized_materialized_or_requested_root_partitions_by_asset_key.items():
+        ) in serialized_handled_root_partitions_by_asset_key.items():
             key = AssetKey.from_user_string(key_str)
-            if key not in asset_graph.non_source_asset_keys:
+            if key not in asset_graph.materializable_asset_keys:
                 continue
 
             partitions_def = asset_graph.get_partitions_def(key)
             if partitions_def is None:
                 continue
 
             try:
                 # in the case that the partitions def has changed, we may not be able to deserialize
                 # the corresponding subset. in this case, we just use an empty subset
-                materialized_or_requested_root_partitions_by_asset_key[
-                    key
-                ] = partitions_def.deserialize_subset(serialized_subset)
+                handled_root_partitions_by_asset_key[key] = partitions_def.deserialize_subset(
+                    serialized_subset
+                )
             except:
-                materialized_or_requested_root_partitions_by_asset_key[
-                    key
-                ] = partitions_def.empty_subset()
+                handled_root_partitions_by_asset_key[key] = partitions_def.empty_subset()
         return cls(
             latest_storage_id=latest_storage_id,
-            materialized_or_requested_root_asset_keys={
-                AssetKey.from_user_string(key_str)
-                for key_str in serialized_materialized_or_requested_root_asset_keys
+            handled_root_asset_keys={
+                AssetKey.from_user_string(key_str) for key_str in serialized_handled_root_asset_keys
             },
-            materialized_or_requested_root_partitions_by_asset_key=materialized_or_requested_root_partitions_by_asset_key,
+            handled_root_partitions_by_asset_key=handled_root_partitions_by_asset_key,
             evaluation_id=evaluation_id,
+            last_observe_request_timestamp_by_asset_key={
+                AssetKey.from_user_string(key_str): timestamp
+                for key_str, timestamp in serialized_last_observe_request_timestamp_by_asset_key.items()
+            },
         )
 
     @classmethod
     def get_evaluation_id_from_serialized(cls, cursor: str) -> Optional[int]:
         data = json.loads(cursor)
-        check.invariant(len(data) in [3, 4], "Invalid serialized cursor")
-        return data[3] if len(data) == 4 else None
+        if isinstance(data, list):  # backcompat
+            check.invariant(len(data) in [3, 4], "Invalid serialized cursor")
+            return data[3] if len(data) == 4 else None
+        else:
+            return data["evaluation_id"]
 
     def serialize(self) -> str:
-        serializable_materialized_or_requested_root_partitions_by_asset_key = {
+        serializable_handled_root_partitions_by_asset_key = {
             key.to_user_string(): subset.serialize()
-            for key, subset in self.materialized_or_requested_root_partitions_by_asset_key.items()
+            for key, subset in self.handled_root_partitions_by_asset_key.items()
         }
         serialized = json.dumps(
-            (
-                self.latest_storage_id,
-                [key.to_user_string() for key in self.materialized_or_requested_root_asset_keys],
-                serializable_materialized_or_requested_root_partitions_by_asset_key,
-                self.evaluation_id,
-            )
+            {
+                "latest_storage_id": self.latest_storage_id,
+                "handled_root_asset_keys": [
+                    key.to_user_string() for key in self.handled_root_asset_keys
+                ],
+                "handled_root_partitions_by_asset_key": serializable_handled_root_partitions_by_asset_key,
+                "evaluation_id": self.evaluation_id,
+                "last_observe_request_timestamp_by_asset_key": {
+                    key.to_user_string(): timestamp
+                    for key, timestamp in self.last_observe_request_timestamp_by_asset_key.items()
+                },
+            }
         )
         return serialized
 
 
-def get_active_backfill_target_asset_graph_subset(
-    instance: "DagsterInstance", asset_graph: AssetGraph
-) -> AssetGraphSubset:
-    """Returns an AssetGraphSubset representing the set of assets that are currently targeted by
-    an active asset backfill.
-    """
-    from dagster._core.execution.asset_backfill import AssetBackfillData
-    from dagster._core.execution.backfill import BulkActionStatus
-
-    asset_backfills = [
-        backfill
-        for backfill in instance.get_backfills(status=BulkActionStatus.REQUESTED)
-        if backfill.is_asset_backfill
-    ]
-
-    result = AssetGraphSubset(asset_graph)
-    for asset_backfill in asset_backfills:
-        if asset_backfill.serialized_asset_backfill_data is None:
-            check.failed("Asset backfill missing serialized_asset_backfill_data")
-
-        asset_backfill_data = AssetBackfillData.from_serialized(
-            asset_backfill.serialized_asset_backfill_data,
-            asset_graph,
-            asset_backfill.backfill_timestamp,
-        )
-
-        result |= asset_backfill_data.target_subset
-
-    return result
-
-
 def find_parent_materialized_asset_partitions(
     instance_queryer: "CachingInstanceQueryer",
     latest_storage_id: Optional[int],
     target_asset_keys: AbstractSet[AssetKey],
     target_asset_keys_and_parents: AbstractSet[AssetKey],
     asset_graph: AssetGraph,
     can_reconcile_fn: Callable[[AssetKeyPartitionKey], bool] = lambda _: True,
+    map_old_time_partitions: bool = True,
 ) -> Tuple[AbstractSet[AssetKeyPartitionKey], Optional[int]]:
     """Finds asset partitions in the given selection whose parents have been materialized since
     latest_storage_id.
 
     Returns:
         - A set of asset partitions.
         - The latest observed storage_id across all relevant assets. Can be used to avoid scanning
             the same events the next time this function is called.
     """
     result_asset_partitions: Set[AssetKeyPartitionKey] = set()
     result_latest_storage_id = latest_storage_id
 
     for asset_key in target_asset_keys_and_parents:
-        if asset_graph.is_source(asset_key):
-            if asset_graph.is_observable(asset_key):
-                # for observable sources, find the storage id of the latest version record that differs
-                # from the previous version (if any)
-                new_version_storage_id = instance_queryer.new_version_storage_id(
-                    observable_source_asset_key=asset_key, after_cursor=latest_storage_id
-                )
-                if new_version_storage_id is not None:
-                    if (
-                        result_latest_storage_id is None
-                        or new_version_storage_id > result_latest_storage_id
-                    ):
-                        result_latest_storage_id = new_version_storage_id
-                    for child in asset_graph.get_children_partitions(
-                        dynamic_partitions_store=instance_queryer,
-                        current_time=instance_queryer.evaluation_time,
-                        asset_key=asset_key,
-                        partition_key=None,
-                    ):
-                        if child.asset_key in target_asset_keys:
-                            result_asset_partitions.add(child)
-
+        if asset_graph.is_source(asset_key) and not asset_graph.is_observable(asset_key):
             continue
 
-        partitions_def = asset_graph.get_partitions_def(asset_key)
-        latest_record = instance_queryer.get_latest_materialization_record(
-            asset_key, after_cursor=latest_storage_id
+        # the set of asset partitions which have been updated since the latest storage id
+        new_asset_partitions = instance_queryer.get_asset_partitions_updated_after_cursor(
+            asset_key=asset_key,
+            asset_partitions=None,
+            after_cursor=latest_storage_id,
         )
-        if latest_record is None:
+        if not new_asset_partitions:
             continue
 
+        partitions_def = asset_graph.get_partitions_def(asset_key)
         if partitions_def is None:
+            latest_record = check.not_none(
+                instance_queryer.get_latest_materialization_or_observation_record(
+                    AssetKeyPartitionKey(asset_key)
+                )
+            )
             for child in asset_graph.get_children_partitions(
                 dynamic_partitions_store=instance_queryer,
                 current_time=instance_queryer.evaluation_time,
                 asset_key=asset_key,
             ):
+                child_partitions_def = asset_graph.get_partitions_def(child.asset_key)
                 if (
                     child.asset_key in target_asset_keys
+                    # when mapping from unpartitioned assets to time partitioned assets, we ignore
+                    # historical time partitions
+                    and (
+                        map_old_time_partitions
+                        or not isinstance(child_partitions_def, TimeWindowPartitionsDefinition)
+                        or child.partition_key
+                        == child_partitions_def.get_last_partition_key(
+                            current_time=instance_queryer.evaluation_time
+                        )
+                    )
                     and not instance_queryer.is_asset_planned_for_run(latest_record.run_id, child)
                 ):
                     result_asset_partitions.add(child)
         else:
-            # for partitioned assets, we want the set of all asset partitions that have been
-            # materialized since the latest_storage_id, not just the most recent
-            materialized_partitions = [
-                partition_key
-                for partition_key in instance_queryer.get_materialized_partitions(
-                    asset_key, after_cursor=latest_storage_id
-                )
-                if partitions_def.has_partition_key(
-                    partition_key, dynamic_partitions_store=instance_queryer
-                )
-            ]
-
             partitions_subset = partitions_def.empty_subset().with_partition_keys(
-                materialized_partitions
+                [
+                    asset_partition.partition_key
+                    for asset_partition in new_asset_partitions
+                    if asset_partition.partition_key is not None
+                    and partitions_def.has_partition_key(
+                        asset_partition.partition_key,
+                        dynamic_partitions_store=instance_queryer,
+                        current_time=instance_queryer.evaluation_time,
+                    )
+                ]
             )
             for child in asset_graph.get_children(asset_key):
                 child_partitions_def = asset_graph.get_partitions_def(child)
                 if child not in target_asset_keys:
                     continue
                 elif not child_partitions_def:
                     result_asset_partitions.add(AssetKeyPartitionKey(child, None))
@@ -552,40 +496,60 @@
                     )
                     for child_partition in child_partitions_subset.get_partition_keys():
                         # we need to see if the child is planned for the same run, but this is
                         # expensive, so we try to avoid doing so in as many situations as possible
                         child_asset_partition = AssetKeyPartitionKey(child, child_partition)
                         if not can_reconcile_fn(child_asset_partition):
                             continue
-                        # cannot materialize in the same run if different partitions defs or
-                        # different partition keys
                         elif (
+                            # if child has a different partitions def than the parent, then it must
+                            # have been executed in a different run, so it's a valid candidate
                             child_partitions_def != partitions_def
-                            or child_partition not in materialized_partitions
+                            # if child partition key is not the same as any newly materialized
+                            # parent key, then it could not have been executed in the same run as
+                            # its parent
+                            or child_partition not in partitions_subset
+                            # if child partition is not failed or in progress, then even if it was
+                            # executed in the same run, we can filter it out later with an is_reconciled
+                            # check (cheaper than the below logic)
+                            or child_partition
+                            not in instance_queryer.get_failed_or_in_progress_subset(
+                                asset_key=child
+                            )
                         ):
                             result_asset_partitions.add(child_asset_partition)
                         else:
+                            # manually query to see if this asset partition was intended to be
+                            # executed in the same run as its parent
                             latest_partition_record = check.not_none(
-                                instance_queryer.get_latest_materialization_record(
+                                instance_queryer.get_latest_materialization_or_observation_record(
                                     AssetKeyPartitionKey(asset_key, child_partition),
                                     after_cursor=latest_storage_id,
                                 )
                             )
                             if not instance_queryer.is_asset_planned_for_run(
                                 latest_partition_record.run_id, child
                             ):
                                 result_asset_partitions.add(child_asset_partition)
 
-        if result_latest_storage_id is None or latest_record.storage_id > result_latest_storage_id:
-            result_latest_storage_id = latest_record.storage_id
+        asset_latest_storage_id = (
+            instance_queryer.get_latest_materialization_or_observation_storage_id(
+                AssetKeyPartitionKey(asset_key)
+            )
+        )
+        if (
+            result_latest_storage_id is None
+            or (asset_latest_storage_id or 0) > result_latest_storage_id
+        ):
+            result_latest_storage_id = asset_latest_storage_id
 
     return (result_asset_partitions, result_latest_storage_id)
 
 
-def find_never_materialized_or_requested_root_asset_partitions(
+def find_never_handled_root_asset_partitions(
     instance_queryer: "CachingInstanceQueryer",
     cursor: AssetReconciliationCursor,
     target_asset_keys: AbstractSet[AssetKey],
     asset_graph: AssetGraph,
 ) -> Tuple[
     Iterable[AssetKeyPartitionKey], AbstractSet[AssetKey], Mapping[AssetKey, AbstractSet[str]]
 ]:
@@ -595,50 +559,75 @@
     Returns:
     - Asset (partition)s that have never been materialized or requested.
     - Non-partitioned assets that had never been materialized or requested up to the previous cursor
         but are now materialized.
     - Asset (partition)s that had never been materialized or requested up to the previous cursor but
         are now materialized.
     """
-    never_materialized_or_requested = set()
+    never_handled = set()
     newly_materialized_root_asset_keys = set()
     newly_materialized_root_partitions_by_asset_key = defaultdict(set)
 
     for asset_key in target_asset_keys & asset_graph.root_asset_keys:
         if asset_graph.is_partitioned(asset_key):
-            auto_materialize_policy = check.not_none(
-                get_implicit_auto_materialize_policy(asset_graph, asset_key)
-            )
-            for partition_key in cursor.get_never_requested_never_materialized_partitions(
+            for partition_key in cursor.get_unhandled_partitions(
                 asset_key,
                 asset_graph,
                 dynamic_partitions_store=instance_queryer,
                 current_time=instance_queryer.evaluation_time,
-                time_window_partition_scope=auto_materialize_policy.time_window_partition_scope,
             ):
                 asset_partition = AssetKeyPartitionKey(asset_key, partition_key)
-                if instance_queryer.get_latest_materialization_record(asset_partition, None):
+                if instance_queryer.asset_partition_has_materialization_or_observation(
+                    asset_partition
+                ):
                     newly_materialized_root_partitions_by_asset_key[asset_key].add(partition_key)
                 else:
-                    never_materialized_or_requested.add(asset_partition)
+                    never_handled.add(asset_partition)
         else:
-            if not cursor.was_previously_materialized_or_requested(asset_key):
-                asset = AssetKeyPartitionKey(asset_key)
-                if instance_queryer.get_latest_materialization_record(asset, None):
+            if not cursor.was_previously_handled(asset_key):
+                asset_partition = AssetKeyPartitionKey(asset_key)
+                if instance_queryer.asset_partition_has_materialization_or_observation(
+                    asset_partition
+                ):
                     newly_materialized_root_asset_keys.add(asset_key)
                 else:
-                    never_materialized_or_requested.add(asset)
+                    never_handled.add(asset_partition)
 
     return (
-        never_materialized_or_requested,
+        never_handled,
         newly_materialized_root_asset_keys,
         newly_materialized_root_partitions_by_asset_key,
     )
 
 
+def _decision_type_for_conditions(
+    conditions: Optional[AbstractSet[AutoMaterializeCondition]],
+) -> Optional[AutoMaterializeDecisionType]:
+    """Based on a set of conditions, determine the resulting decision."""
+    if not conditions:
+        return None
+    condition_decision_types = {condition.decision_type for condition in conditions}
+    # precedence of decisions
+    for decision_type in [
+        AutoMaterializeDecisionType.SKIP,
+        AutoMaterializeDecisionType.DISCARD,
+        AutoMaterializeDecisionType.MATERIALIZE,
+    ]:
+        if decision_type in condition_decision_types:
+            return decision_type
+    return None
+
+
+def _will_materialize_for_conditions(
+    conditions: Optional[AbstractSet[AutoMaterializeCondition]],
+) -> bool:
+    """Based on a set of conditions, determine if the asset will be materialized."""
+    return _decision_type_for_conditions(conditions) == AutoMaterializeDecisionType.MATERIALIZE
+
+
 def determine_asset_partitions_to_auto_materialize(
     instance_queryer: "CachingInstanceQueryer",
     cursor: AssetReconciliationCursor,
     target_asset_keys: AbstractSet[AssetKey],
     target_asset_keys_and_parents: AbstractSet[AssetKey],
     asset_graph: AssetGraph,
     current_time: datetime.datetime,
@@ -647,458 +636,230 @@
     ],
 ) -> Tuple[
     Mapping[AssetKeyPartitionKey, AbstractSet[AutoMaterializeCondition]],
     AbstractSet[AssetKey],
     Mapping[AssetKey, AbstractSet[str]],
     Optional[int],
 ]:
-    materialization_requests_by_asset_key: Mapping[
-        AssetKey, Set[AssetKeyPartitionKey]
-    ] = defaultdict(set)
     evaluation_time = instance_queryer.evaluation_time
 
     (
-        never_materialized_or_requested_roots,
+        never_handled_roots,
         newly_materialized_root_asset_keys,
         newly_materialized_root_partitions_by_asset_key,
-    ) = find_never_materialized_or_requested_root_asset_partitions(
+    ) = find_never_handled_root_asset_partitions(
         instance_queryer=instance_queryer,
         cursor=cursor,
         target_asset_keys=target_asset_keys,
         asset_graph=asset_graph,
     )
 
     # initialize conditions with the conditions_for_freshness
     conditions_by_asset_partition: Dict[
         AssetKeyPartitionKey, Set[AutoMaterializeCondition]
     ] = defaultdict(set, conditions_by_asset_partition_for_freshness)
+    materialization_requests_by_asset_key: Dict[AssetKey, Set[AssetKeyPartitionKey]] = defaultdict(
+        set
+    )
 
-    # a filter for eliminating candidates
     def can_reconcile_candidate(candidate: AssetKeyPartitionKey) -> bool:
+        """A filter for eliminating candidates from consideration for auto-materialization."""
         auto_materialize_policy = get_implicit_auto_materialize_policy(
             asset_graph=asset_graph, asset_key=candidate.asset_key
         )
-        partitions_def = asset_graph.get_partitions_def(candidate.asset_key)
 
-        # no policy means no reconciliation
-        if auto_materialize_policy is None:
-            return False
-        # the partition is too old to reconcile
-        elif (
-            partitions_def
-            and has_one_dimension_time_window_partitioning(partitions_def)
-            and not can_reconcile_time_window_partition(
-                partitions_def=cast(
-                    Union[TimeWindowPartitionsDefinition, MultiPartitionsDefinition], partitions_def
-                ),
-                partition_key=candidate.partition_key,
-                time_window_partition_scope=auto_materialize_policy.time_window_partition_scope,
-                current_time=evaluation_time,
+        return not (
+            # must have an auto_materialize_policy
+            auto_materialize_policy is None
+            # must be in the taget set
+            or candidate.asset_key not in target_asset_keys
+            # must not be currently backfilled
+            or candidate in instance_queryer.get_active_backfill_target_asset_graph_subset()
+            # must not have invalid parent partitions
+            or len(
+                asset_graph.get_parents_partitions(
+                    instance_queryer,
+                    instance_queryer.evaluation_time,
+                    candidate.asset_key,
+                    candidate.partition_key,
+                ).required_but_nonexistent_parents_partitions
             )
-        ):
-            return False
-        # the policy does not allow for materializing missing partitions and it's missing
-        elif not auto_materialize_policy.on_missing and not instance_queryer.materialization_exists(
-            candidate
-        ):
-            return False
-
-        return True
+            > 0
+        )
 
     stale_candidates, latest_storage_id = find_parent_materialized_asset_partitions(
         instance_queryer=instance_queryer,
         latest_storage_id=cursor.latest_storage_id,
         target_asset_keys=target_asset_keys,
         target_asset_keys_and_parents=target_asset_keys_and_parents,
         asset_graph=asset_graph,
         can_reconcile_fn=can_reconcile_candidate,
+        map_old_time_partitions=False,
     )
 
-    backfill_target_asset_graph_subset = get_active_backfill_target_asset_graph_subset(
-        asset_graph=asset_graph,
-        instance=instance_queryer.instance,
-    )
-
-    def will_be_materialized_for_freshness(asset_partition: AssetKeyPartitionKey) -> bool:
-        return asset_partition in conditions_by_asset_partition_for_freshness and all(
-            condition.decision_type == AutoMaterializeDecisionType.MATERIALIZE
-            for condition in conditions_by_asset_partition_for_freshness[asset_partition]
-        )
-
-    def parents_will_be_reconciled(
-        asset_graph: AssetGraph,
-        candidate: AssetKeyPartitionKey,
-    ) -> bool:
+    def get_waiting_on_asset_keys(candidate: AssetKeyPartitionKey) -> FrozenSet[AssetKey]:
+        """Returns the set of ancestor asset keys that must be materialized before this asset can be
+        materialized.
+        """
         from dagster._core.definitions.external_asset_graph import ExternalAssetGraph
 
+        unreconciled_ancestors = set()
         for parent in asset_graph.get_parents_partitions(
-            instance_queryer, evaluation_time, candidate.asset_key, candidate.partition_key
-        ):
-            if instance_queryer.is_reconciled(asset_partition=parent, asset_graph=asset_graph):
-                continue
-
+            instance_queryer,
+            instance_queryer.evaluation_time,
+            candidate.asset_key,
+            candidate.partition_key,
+        ).parent_partitions:
+            # parent will not be materialized this tick
             if not (
-                (
-                    parent in conditions_by_asset_partition
-                    and all(
-                        condition.decision_type == AutoMaterializeDecisionType.MATERIALIZE
-                        for condition in conditions_by_asset_partition[parent]
-                    )
-                )
-                # if they don't have the same partitioning, then we can't launch a run that
-                # targets both, so we need to wait until the parent is reconciled before
-                # launching a run for the child
+                _will_materialize_for_conditions(conditions_by_asset_partition.get(parent))
+                # the parent must have the same partitioning / partition key to be materialized
+                # alongside the candidate
                 and asset_graph.have_same_partitioning(parent.asset_key, candidate.asset_key)
                 and parent.partition_key == candidate.partition_key
+                # the parent must be in the same repository to be materialized alongside the candidate
+                and (
+                    not isinstance(asset_graph, ExternalAssetGraph)
+                    or asset_graph.get_repository_handle(candidate.asset_key)
+                    == asset_graph.get_repository_handle(parent.asset_key)
+                )
             ):
-                return False
-
-            if isinstance(asset_graph, ExternalAssetGraph):
-                # if the parent is in a different repository, we can't launch a run that targets both,
-                # so we need to wait
-                if asset_graph.get_repository_handle(
-                    candidate.asset_key
-                ) is not asset_graph.get_repository_handle(parent.asset_key):
-                    return False
-
-        return True
+                unreconciled_ancestors.update(
+                    instance_queryer.get_root_unreconciled_ancestors(asset_partition=parent)
+                )
+        return frozenset(unreconciled_ancestors)
 
     def conditions_for_candidate(
         candidate: AssetKeyPartitionKey,
     ) -> AbstractSet[AutoMaterializeCondition]:
-        auto_materialize_policy = get_implicit_auto_materialize_policy(
-            asset_graph=asset_graph, asset_key=candidate.asset_key
+        """Returns a set of AutoMaterializeConditions that apply to a given candidate."""
+        auto_materialize_policy = check.not_none(
+            get_implicit_auto_materialize_policy(
+                asset_graph=asset_graph, asset_key=candidate.asset_key
+            )
         )
         conditions = set()
-        if auto_materialize_policy is None:
-            return conditions
-        elif auto_materialize_policy.on_missing and not instance_queryer.materialization_exists(
-            asset_partition=candidate
+
+        # if this asset is missing
+        if (
+            auto_materialize_policy.on_missing
+            and not instance_queryer.asset_partition_has_materialization_or_observation(
+                asset_partition=candidate
+            )
         ):
             conditions.add(MissingAutoMaterializeCondition())
-        elif auto_materialize_policy.on_new_parent_data and not instance_queryer.is_reconciled(
-            asset_partition=candidate, asset_graph=asset_graph
+
+        # if the parent has been updated
+        if auto_materialize_policy.on_new_parent_data and not instance_queryer.is_reconciled(
+            asset_partition=candidate
         ):
             conditions.add(ParentMaterializedAutoMaterializeCondition())
 
-        # if there are any conditions that would cause us to materialize this candidate unit, we
-        # need to ensure they would not cause us to exceed the rate limit
-        if conditions:
-            if (
-                # has a rate limit
-                auto_materialize_policy.max_materializations_per_minute is not None
-                # has not already been requested
-                and candidate not in materialization_requests_by_asset_key[candidate.asset_key]
-                # current number of requested asset partitions exceeds the rate limit
-                and len(materialization_requests_by_asset_key[candidate.asset_key])
-                >= auto_materialize_policy.max_materializations_per_minute
-            ):
-                conditions.add(MaxMaterializationsExceededAutoMaterializeCondition())
-            else:
-                materialization_requests_by_asset_key[candidate.asset_key].add(candidate)
+        # if the parents will not be resolved this tick
+        waiting_on_asset_keys = get_waiting_on_asset_keys(candidate)
+        if waiting_on_asset_keys:
+            conditions.add(
+                ParentOutdatedAutoMaterializeCondition(waiting_on_asset_keys=waiting_on_asset_keys)
+            )
+
+        if (
+            # would be materialized
+            _will_materialize_for_conditions(conditions)
+            # has a rate limit
+            and auto_materialize_policy.max_materializations_per_minute is not None
+            # would exceed the rate limit
+            and len(materialization_requests_by_asset_key[candidate.asset_key].union({candidate}))
+            > auto_materialize_policy.max_materializations_per_minute
+        ):
+            conditions.add(MaxMaterializationsExceededAutoMaterializeCondition())
 
         return conditions
 
     def should_reconcile(
         asset_graph: AssetGraph,
         candidates_unit: Iterable[AssetKeyPartitionKey],
         to_reconcile: AbstractSet[AssetKeyPartitionKey],
     ) -> bool:
-        if any(
-            # do not reconcile assets if they are not reconcilable
-            not can_reconcile_candidate(candidate)
-            # do not reconcile assets if an active backfill will update them
-            or candidate in backfill_target_asset_graph_subset
-            # do not reconcile assets if they are not in the target selection
-            or candidate.asset_key not in target_asset_keys
-            for candidate in candidates_unit
-        ):
+        if any(not can_reconcile_candidate(candidate) for candidate in candidates_unit):
             return False
-
-        if all(
-            will_be_materialized_for_freshness(candidate)
-            or parents_will_be_reconciled(asset_graph, candidate)
-            for candidate in candidates_unit
-        ):
-            unit_conditions = set().union(
-                *(conditions_for_candidate(candidate) for candidate in candidates_unit)
-            )
-            # all candidates in the unit share the same conditions
-            if unit_conditions:
-                for candidate in candidates_unit:
-                    conditions_by_asset_partition[candidate].update(unit_conditions)
-                # all conditions be of type MATERIALIZE for an asset partition to be materialized
-                return all(
-                    condition.decision_type == AutoMaterializeDecisionType.MATERIALIZE
-                    for condition in unit_conditions
-                )
-            return False
-        else:
-            for candidate in candidates_unit:
-                conditions_by_asset_partition[candidate].add(
-                    ParentOutdatedAutoMaterializeCondition()
-                )
-        return False
+        # collect all conditions that apply to any candidate in the unit
+        unit_conditions = set().union(
+            *(conditions_for_candidate(candidate) for candidate in candidates_unit)
+        )
+        will_materialize = _will_materialize_for_conditions(unit_conditions)
+        # for now, all candidates in the unit share the same conditions
+        for candidate in candidates_unit:
+            conditions_by_asset_partition[candidate].update(unit_conditions)
+            if will_materialize:
+                materialization_requests_by_asset_key[candidate.asset_key].add(candidate)
+        return will_materialize
 
     # will update conditions
     asset_graph.bfs_filter_asset_partitions(
         instance_queryer,
         lambda candidates_unit, to_reconcile: should_reconcile(
             asset_graph, candidates_unit, to_reconcile
         ),
-        set(itertools.chain(never_materialized_or_requested_roots, stale_candidates)),
+        set(itertools.chain(never_handled_roots, stale_candidates)),
         evaluation_time,
     )
 
     return (
         conditions_by_asset_partition,
         newly_materialized_root_asset_keys,
         newly_materialized_root_partitions_by_asset_key,
         latest_storage_id,
     )
 
 
-def get_execution_period_for_policy(
-    freshness_policy: FreshnessPolicy,
-    effective_data_time: Optional[datetime.datetime],
-    current_time: datetime.datetime,
-) -> pendulum.Period:
-    if effective_data_time is None:
-        return pendulum.Period(start=current_time, end=current_time)
-
-    if freshness_policy.cron_schedule:
-        tick_iterator = cron_string_iterator(
-            start_timestamp=current_time.timestamp(),
-            cron_string=freshness_policy.cron_schedule,
-            execution_timezone=freshness_policy.cron_schedule_timezone,
-        )
-
-        while True:
-            # find the next tick tick that requires data after the current effective data time
-            # (usually, this will be the next tick)
-            tick = next(tick_iterator)
-            required_data_time = tick - freshness_policy.maximum_lag_delta
-            if effective_data_time is None or effective_data_time < required_data_time:
-                return pendulum.Period(start=required_data_time, end=tick)
-
-    else:
-        return pendulum.Period(
-            # we don't want to execute this too frequently
-            start=effective_data_time + 0.9 * freshness_policy.maximum_lag_delta,
-            end=max(effective_data_time + freshness_policy.maximum_lag_delta, current_time),
-        )
-
-
-def get_execution_period_and_conditions_for_policies(
-    local_policy: Optional[FreshnessPolicy],
-    policies: AbstractSet[FreshnessPolicy],
-    effective_data_time: Optional[datetime.datetime],
-    current_time: datetime.datetime,
-) -> Tuple[Optional[pendulum.Period], AbstractSet[AutoMaterializeCondition]]:
-    """Determines a range of times for which you can kick off an execution of this asset to solve
-    the most pressing constraint, alongside a maximum number of additional constraints.
-    """
-    merged_period = None
-    conditions = set()
-    for period, policy in sorted(
-        (
-            (get_execution_period_for_policy(policy, effective_data_time, current_time), policy)
-            for policy in policies
-        ),
-        # sort execution periods by most pressing
-        key=lambda pp: pp[0].end,
-    ):
-        if merged_period is None:
-            merged_period = period
-        elif period.start <= merged_period.end:
-            merged_period = pendulum.Period(
-                start=max(period.start, merged_period.start),
-                end=period.end,
-            )
-        else:
-            break
-
-        if policy == local_policy:
-            conditions.add(FreshnessAutoMaterializeCondition())
-        else:
-            conditions.add(DownstreamFreshnessAutoMaterializeCondition())
-
-    return merged_period, conditions
-
-
-def determine_asset_partitions_to_auto_materialize_for_freshness(
-    data_time_resolver: "CachingDataTimeResolver",
-    asset_graph: AssetGraph,
-    target_asset_keys: AbstractSet[AssetKey],
-    target_asset_keys_and_parents: AbstractSet[AssetKey],
-    current_time: datetime.datetime,
-) -> Mapping[AssetKeyPartitionKey, Set[AutoMaterializeCondition]]:
-    """Returns a set of AssetKeyPartitionKeys to materialize in order to abide by the given
-    FreshnessPolicies, as well as a set of AssetKeyPartitionKeys which will be materialized at
-    some point within the plan window.
-
-    Attempts to minimize the total number of asset executions.
-    """
-    from dagster._core.definitions.external_asset_graph import ExternalAssetGraph
-
-    # now we have a full set of constraints, we can find solutions for them as we move down
-    conditions_by_asset_partition: Mapping[
-        AssetKeyPartitionKey, Set[AutoMaterializeCondition]
-    ] = defaultdict(set)
-    waiting_to_materialize: Set[AssetKey] = set()
-    expected_data_time_by_key: Dict[AssetKey, Optional[datetime.datetime]] = {}
-
-    for level in asset_graph.toposort_asset_keys():
-        for key in level:
-            if (
-                key not in target_asset_keys_and_parents
-                or key not in asset_graph.non_source_asset_keys
-                or not asset_graph.get_downstream_freshness_policies(asset_key=key)
-            ):
-                continue
-
-            parents = asset_graph.get_parents(key)
-
-            if any(p in waiting_to_materialize for p in parents):
-                # we can't materialize this asset yet, because we're waiting on a parent
-                waiting_to_materialize.add(key)
-                continue
-
-            # if we're going to materialize a parent of this asset that's in a different repository,
-            # then we need to wait
-            if isinstance(asset_graph, ExternalAssetGraph):
-                repo = asset_graph.get_repository_handle(key)
-                if any(
-                    AssetKeyPartitionKey(p, None) in conditions_by_asset_partition
-                    and asset_graph.get_repository_handle(p) is not repo
-                    for p in parents
-                ):
-                    waiting_to_materialize.add(key)
-                    continue
-
-            # figure out the current contents of this asset with respect to its constraints
-            current_data_time = data_time_resolver.get_current_data_time(key, current_time)
-
-            # figure out the expected data time of this asset if it were to be executed on this tick
-            # for root assets, this would just be the current time
-            expected_data_time = (
-                min(
-                    (
-                        cast(datetime.datetime, expected_data_time_by_key[k])
-                        for k in parents
-                        if k in expected_data_time_by_key
-                        and expected_data_time_by_key[k] is not None
-                    ),
-                    default=None,
-                )
-                if asset_graph.has_non_source_parents(key)
-                else current_time
-            )
-
-            # currently, freshness logic has no effect on partitioned assets
-            if key in target_asset_keys and not asset_graph.is_partitioned(key):
-                # calculate the data times you would expect after all currently-executing runs
-                # were to successfully complete
-                in_progress_data_time = data_time_resolver.get_in_progress_data_time(
-                    key, current_time
-                )
-
-                # calculate the data times you would have expected if the most recent run succeeded
-                failed_data_time = data_time_resolver.get_ignored_failure_data_time(
-                    key, current_time
-                )
-
-                effective_data_time = max(
-                    filter(None, (current_data_time, in_progress_data_time, failed_data_time)),
-                    default=None,
-                )
-
-                # figure out a time period that you can execute this asset within to solve a maximum
-                # number of constraints
-                (
-                    execution_period,
-                    execution_conditions,
-                ) = get_execution_period_and_conditions_for_policies(
-                    local_policy=asset_graph.freshness_policies_by_key.get(key),
-                    policies=asset_graph.get_downstream_freshness_policies(asset_key=key),
-                    effective_data_time=effective_data_time,
-                    current_time=current_time,
-                )
-            else:
-                execution_period, execution_conditions = None, set()
-
-            # a key may already be in conditions by the time we get here if a required
-            # neighbor was selected to be updated
-            asset_partition = AssetKeyPartitionKey(key, None)
-            if asset_partition in conditions_by_asset_partition and all(
-                condition.decision_type == AutoMaterializeDecisionType.MATERIALIZE
-                for condition in conditions_by_asset_partition[asset_partition]
-            ):
-                expected_data_time_by_key[key] = expected_data_time
-            elif (
-                execution_period is not None
-                and execution_period.start <= current_time
-                and expected_data_time is not None
-                and expected_data_time >= execution_period.start
-                and all(
-                    condition.decision_type == AutoMaterializeDecisionType.MATERIALIZE
-                    for condition in execution_conditions
-                )
-            ):
-                expected_data_time_by_key[key] = expected_data_time
-                conditions_by_asset_partition[asset_partition].update(execution_conditions)
-                # all required neighbors will be updated on the same tick
-                for required_key in asset_graph.get_required_multi_asset_keys(key):
-                    conditions_by_asset_partition[
-                        (AssetKeyPartitionKey(required_key, None))
-                    ].update(execution_conditions)
-            else:
-                # if downstream assets consume this, they should expect data time equal to the
-                # current time for this asset, as it's not going to be updated
-                expected_data_time_by_key[key] = current_data_time
-
-    return conditions_by_asset_partition
-
-
 def reconcile(
     asset_graph: AssetGraph,
     target_asset_keys: AbstractSet[AssetKey],
     instance: "DagsterInstance",
     cursor: AssetReconciliationCursor,
-    run_tags: Optional[Mapping[str, str]],
+    materialize_run_tags: Optional[Mapping[str, str]],
+    observe_run_tags: Optional[Mapping[str, str]],
+    auto_observe: bool,
 ) -> Tuple[
     Sequence[RunRequest],
     AssetReconciliationCursor,
     Sequence[AutoMaterializeAssetEvaluation],
 ]:
     from dagster._utils.caching_instance_queryer import CachingInstanceQueryer  # expensive import
 
     current_time = pendulum.now("UTC")
 
-    instance_queryer = CachingInstanceQueryer(instance=instance, evaluation_time=current_time)
+    instance_queryer = CachingInstanceQueryer(
+        instance=instance, asset_graph=asset_graph, evaluation_time=current_time
+    )
 
     target_parent_asset_keys = {
         parent
         for target_asset_key in target_asset_keys
         for parent in asset_graph.get_parents(target_asset_key)
     }
     target_asset_keys_and_parents = target_asset_keys | target_parent_asset_keys
 
     # fetch some data in advance to batch some queries
     target_asset_keys_and_parents_list = list(target_asset_keys_and_parents)
-    instance_queryer.prefetch_asset_records(target_asset_keys_and_parents_list)
+    instance_queryer.prefetch_asset_records(
+        [key for key in target_asset_keys_and_parents_list if not asset_graph.is_source(key)]
+    )
     instance_queryer.prefetch_asset_partition_counts(
-        target_asset_keys_and_parents_list, after_cursor=cursor.latest_storage_id
+        [
+            key
+            for key in target_asset_keys_and_parents_list
+            if asset_graph.is_partitioned(key) and not asset_graph.is_source(key)
+        ],
+        after_cursor=cursor.latest_storage_id,
     )
 
     conditions_by_asset_partition_for_freshness = (
         determine_asset_partitions_to_auto_materialize_for_freshness(
-            data_time_resolver=CachingDataTimeResolver(
-                instance_queryer=instance_queryer, asset_graph=asset_graph
-            ),
+            data_time_resolver=CachingDataTimeResolver(instance_queryer=instance_queryer),
             asset_graph=asset_graph,
             target_asset_keys=target_asset_keys,
             target_asset_keys_and_parents=target_asset_keys_and_parents,
             current_time=current_time,
         )
     )
 
@@ -1113,36 +874,53 @@
         cursor=cursor,
         target_asset_keys=target_asset_keys,
         target_asset_keys_and_parents=target_asset_keys_and_parents,
         current_time=current_time,
         conditions_by_asset_partition_for_freshness=conditions_by_asset_partition_for_freshness,
     )
 
-    run_requests = build_run_requests(
-        asset_partitions={
-            asset_partition
-            for asset_partition, conditions in conditions_by_asset_partition.items()
-            if all(
-                condition.decision_type == AutoMaterializeDecisionType.MATERIALIZE
-                for condition in conditions
-            )
-        },
-        asset_graph=asset_graph,
-        run_tags=run_tags,
+    observe_request_timestamp = pendulum.now().timestamp()
+    auto_observe_run_requests = (
+        get_auto_observe_run_requests(
+            asset_graph=asset_graph,
+            last_observe_request_timestamp_by_asset_key=cursor.last_observe_request_timestamp_by_asset_key,
+            current_timestamp=observe_request_timestamp,
+            run_tags=observe_run_tags,
+        )
+        if auto_observe
+        else []
     )
+    run_requests = [
+        *build_run_requests(
+            asset_partitions={
+                asset_partition
+                for asset_partition, conditions in conditions_by_asset_partition.items()
+                if _will_materialize_for_conditions(conditions)
+            },
+            asset_graph=asset_graph,
+            run_tags=materialize_run_tags,
+        ),
+        *auto_observe_run_requests,
+    ]
 
     return (
         run_requests,
         cursor.with_updates(
             latest_storage_id=latest_storage_id,
-            run_requests=run_requests,
+            conditions_by_asset_partition=conditions_by_asset_partition,
             asset_graph=asset_graph,
             newly_materialized_root_asset_keys=newly_materialized_root_asset_keys,
             newly_materialized_root_partitions_by_asset_key=newly_materialized_root_partitions_by_asset_key,
             evaluation_id=cursor.evaluation_id + 1,
+            newly_observe_requested_asset_keys=[
+                asset_key
+                for run_request in auto_observe_run_requests
+                for asset_key in cast(Sequence[AssetKey], run_request.asset_selection)
+            ],
+            observe_request_timestamp=observe_request_timestamp,
         ),
         build_auto_materialize_asset_evaluations(
             asset_graph, conditions_by_asset_partition, dynamic_partitions_store=instance_queryer
         ),
     )
 
 
@@ -1240,15 +1018,15 @@
 
     Args:
         asset_selection (AssetSelection): The group of assets you want to keep up-to-date
         name (str): The name to give the sensor.
         minimum_interval_seconds (Optional[int]): The minimum amount of time that should elapse between sensor invocations.
         description (Optional[str]): A description for the sensor.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         run_tags (Optional[Mapping[str, str]): Dictionary of tags to pass to the RunRequests launched by this sensor
 
     Returns:
         SensorDefinition
 
     Example:
         If you have the following asset graph, with no freshness policies:
@@ -1357,14 +1135,43 @@
             )
 
         run_requests, updated_cursor, _ = reconcile(
             asset_graph=asset_graph,
             target_asset_keys=target_asset_keys,
             instance=context.instance,
             cursor=cursor,
-            run_tags=run_tags,
+            materialize_run_tags=run_tags,
+            observe_run_tags=None,
+            auto_observe=False,
         )
 
         context.update_cursor(updated_cursor.serialize())
         return run_requests
 
     return _sensor
+
+
+def get_auto_observe_run_requests(
+    last_observe_request_timestamp_by_asset_key: Mapping[AssetKey, float],
+    current_timestamp: float,
+    asset_graph: AssetGraph,
+    run_tags: Optional[Mapping[str, str]],
+) -> Sequence[RunRequest]:
+    assets_to_auto_observe: Set[AssetKey] = set()
+    for asset_key in asset_graph.source_asset_keys:
+        last_observe_request_timestamp = last_observe_request_timestamp_by_asset_key.get(asset_key)
+        auto_observe_interval_minutes = asset_graph.get_auto_observe_interval_minutes(asset_key)
+
+        if auto_observe_interval_minutes and (
+            last_observe_request_timestamp is None
+            or (
+                last_observe_request_timestamp + auto_observe_interval_minutes * 60
+                < current_timestamp
+            )
+        ):
+            assets_to_auto_observe.add(asset_key)
+
+    return [
+        RunRequest(asset_selection=list(asset_keys), tags=run_tags)
+        for asset_keys in asset_graph.split_asset_keys_by_repository(assets_to_auto_observe)
+        if len(asset_keys) > 0
+    ]
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_selection.py` & `dagster-1.4.0/dagster/_core/definitions/asset_selection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import collections.abc
 import operator
 from abc import ABC, abstractmethod
 from functools import reduce
 from typing import AbstractSet, Iterable, Optional, Sequence, Union, cast
 
 from typing_extensions import TypeAlias
 
@@ -100,36 +101,48 @@
             AssetKey.from_user_string(key) if isinstance(key, str) else AssetKey.from_coercible(key)
             for key in asset_keys
         ]
         return KeysAssetSelection(*_asset_keys)
 
     @public
     @staticmethod
-    def key_prefixes(*key_prefixes: CoercibleToAssetKeyPrefix) -> "KeyPrefixesAssetSelection":
+    def key_prefixes(
+        *key_prefixes: CoercibleToAssetKeyPrefix, include_sources: bool = False
+    ) -> "KeyPrefixesAssetSelection":
         """Returns a selection that includes assets that match any of the provided key prefixes.
 
+        Args:
+            include_sources (bool): If True, then include source assets matching the key prefix(es)
+                in the selection.
+
         Examples:
             .. code-block:: python
 
               # match any asset key where the first segment is equal to "a" or "b"
               # e.g. AssetKey(["a", "b", "c"]) would match, but AssetKey(["abc"]) would not.
               AssetSelection.key_prefixes("a", "b")
 
               # match any asset key where the first two segments are ["a", "b"] or ["a", "c"]
               AssetSelection.key_prefixes(["a", "b"], ["a", "c"])
         """
         _asset_key_prefixes = [key_prefix_from_coercible(key_prefix) for key_prefix in key_prefixes]
-        return KeyPrefixesAssetSelection(*_asset_key_prefixes)
+        return KeyPrefixesAssetSelection(*_asset_key_prefixes, include_sources=include_sources)
 
     @public
     @staticmethod
-    def groups(*group_strs) -> "GroupsAssetSelection":
-        """Returns a selection that includes assets that belong to any of the provided groups."""
+    def groups(*group_strs, include_sources: bool = False) -> "GroupsAssetSelection":
+        """Returns a selection that includes materializable assets that belong to any of the
+        provided groups.
+
+        Args:
+            include_sources (bool): If True, then include source assets matching the group in the
+                selection.
+        """
         check.tuple_param(group_strs, "group_strs", of_type=str)
-        return GroupsAssetSelection(*group_strs)
+        return GroupsAssetSelection(*group_strs, include_sources=include_sources)
 
     @public
     def downstream(
         self, depth: Optional[int] = None, include_self: bool = True
     ) -> "DownstreamAssetSelection":
         """Returns a selection that includes all assets that are downstream of any of the assets in
         this selection, selecting the assets in this selection by default. Iterates through each
@@ -146,20 +159,20 @@
         check.opt_bool_param(include_self, "include_self")
         return DownstreamAssetSelection(self, depth=depth, include_self=include_self)
 
     @public
     def upstream(
         self, depth: Optional[int] = None, include_self: bool = True
     ) -> "UpstreamAssetSelection":
-        """Returns a selection that includes all assets that are upstream of any of the assets in
-        this selection, selecting the assets in this selection by default. Iterates through each
-        asset in this selection and returns the union of all upstream assets.
+        """Returns a selection that includes all materializable assets that are upstream of any of
+        the assets in this selection, selecting the assets in this selection by default. Iterates
+        through each asset in this selection and returns the union of all upstream assets.
 
-        Because mixed selections of source and regular assets are currently not supported, keys
-        corresponding to `SourceAssets` will not be included as upstream of regular assets.
+        Because mixed selections of source and materializable assets are currently not supported,
+        keys corresponding to `SourceAssets` will not be included as upstream of regular assets.
 
         Args:
             depth (Optional[int]): If provided, then only include assets to the given depth. A depth
                 of 2 means all assets that are parents or grandparents of the assets in this
                 selection.
             include_self (bool): If True, then include the assets in this selection in the result.
                 If the include_self flag is False, return each upstream asset that is not part of the
@@ -177,46 +190,61 @@
         A sink asset is an asset that has no downstream dependencies within the asset selection.
         The sink asset can have downstream dependencies outside of the asset selection.
         """
         return SinkAssetSelection(self)
 
     @public
     def required_multi_asset_neighbors(self) -> "RequiredNeighborsAssetSelection":
-        """Given an asset selection in which some assets are output from a mutli-asset compute op
+        """Given an asset selection in which some assets are output from a multi-asset compute op
         which cannot be subset, returns a new asset selection that contains all of the assets
         required to execute the original asset selection.
         """
         return RequiredNeighborsAssetSelection(self)
 
     @public
     def roots(self) -> "RootAssetSelection":
         """Given an asset selection, returns a new asset selection that contains all of the root
         assets within the original asset selection.
 
         A root asset is an asset that has no upstream dependencies within the asset selection.
         The root asset can have downstream dependencies outside of the asset selection.
+
+        Because mixed selections of source and materializable assets are currently not supported,
+        keys corresponding to `SourceAssets` will not be included as roots. To select source assets,
+        use the `upstream_source_assets` method.
         """
         return RootAssetSelection(self)
 
     @public
     @deprecated
     def sources(self) -> "RootAssetSelection":
         """Given an asset selection, returns a new asset selection that contains all of the root
         assets within the original asset selection.
 
-        A root asset is an asset that has no upstream dependencies within the asset selection.
-        The root asset can have downstream dependencies outside of the asset selection.
+        A root asset is a materializable asset that has no upstream dependencies within the asset
+        selection. The root asset can have downstream dependencies outside of the asset selection.
+
+        Because mixed selections of source and materializable assets are currently not supported,
+        keys corresponding to `SourceAssets` will not be included as roots. To select source assets,
+        use the `upstream_source_assets` method.
         """
         deprecation_warning(
             "AssetSelection.sources",
             "2.0",
             "Use AssetSelection.roots instead.",
         )
         return self.roots()
 
+    @public
+    def upstream_source_assets(self) -> "SourceAssetSelection":
+        """Given an asset selection, returns a new asset selection that contains all of the source
+        assets upstream of assets in the original selection.
+        """
+        return SourceAssetSelection(self)
+
     def __or__(self, other: "AssetSelection") -> "OrAssetSelection":
         check.inst_param(other, "other", AssetSelection)
         return OrAssetSelection(self, other)
 
     def __and__(self, other: "AssetSelection") -> "AndAssetSelection":
         check.inst_param(other, "other", AssetSelection)
         return AndAssetSelection(self, other)
@@ -271,43 +299,47 @@
 
     @classmethod
     def from_coercible(cls, selection: CoercibleToAssetSelection) -> "AssetSelection":
         if isinstance(selection, str):
             return cls._selection_from_string(selection)
         elif isinstance(selection, AssetSelection):
             return selection
-        elif isinstance(selection, list) and all(isinstance(el, str) for el in selection):
+        elif isinstance(selection, collections.abc.Sequence) and all(
+            isinstance(el, str) for el in selection
+        ):
             return reduce(
                 operator.or_, [cls._selection_from_string(cast(str, s)) for s in selection]
             )
-        elif isinstance(selection, list) and all(
+        elif isinstance(selection, collections.abc.Sequence) and all(
             isinstance(el, (AssetsDefinition, SourceAsset)) for el in selection
         ):
             return AssetSelection.keys(
                 *(
                     key
                     for el in selection
                     for key in (
                         el.keys if isinstance(el, AssetsDefinition) else [cast(SourceAsset, el).key]
                     )
                 )
             )
-        elif isinstance(selection, list) and all(isinstance(el, AssetKey) for el in selection):
+        elif isinstance(selection, collections.abc.Sequence) and all(
+            isinstance(el, AssetKey) for el in selection
+        ):
             return cls.keys(*cast(Sequence[AssetKey], selection))
         else:
             check.failed(
                 "selection argument must be one of str, Sequence[str], Sequence[AssetKey],"
                 " Sequence[AssetsDefinition], Sequence[SourceAsset], AssetSelection. Was"
                 f" {type(selection)}."
             )
 
 
 class AllAssetSelection(AssetSelection):
     def resolve_inner(self, asset_graph: AssetGraph) -> AbstractSet[AssetKey]:
-        return asset_graph.all_asset_keys
+        return asset_graph.materializable_asset_keys
 
 
 class AndAssetSelection(AssetSelection):
     def __init__(self, left: AssetSelection, right: AssetSelection):
         self._left = left
         self._right = right
 
@@ -383,94 +415,120 @@
                 ],
             ),
             selection if not self.include_self else set(),
         )
 
 
 class GroupsAssetSelection(AssetSelection):
-    def __init__(self, *groups: str):
+    def __init__(self, *groups: str, include_sources: bool):
         self._groups = groups
+        self._include_sources = include_sources
 
     def resolve_inner(self, asset_graph: AssetGraph) -> AbstractSet[AssetKey]:
+        base_set = (
+            asset_graph.all_asset_keys
+            if self._include_sources
+            else asset_graph.materializable_asset_keys
+        )
         return {
             asset_key
             for asset_key, group in asset_graph.group_names_by_key.items()
-            if group in self._groups and asset_key not in asset_graph.source_asset_keys
+            if group in self._groups and asset_key in base_set
         }
 
 
 class KeysAssetSelection(AssetSelection):
     def __init__(self, *keys: AssetKey):
         self._keys = keys
 
     def resolve_inner(self, asset_graph: AssetGraph) -> AbstractSet[AssetKey]:
         specified_keys = set(self._keys)
-        invalid_keys = {
-            key
-            for key in specified_keys
-            if key not in asset_graph.all_asset_keys and key not in asset_graph.source_asset_keys
-        }
+        invalid_keys = {key for key in specified_keys if key not in asset_graph.all_asset_keys}
         if invalid_keys:
             raise DagsterInvalidSubsetError(
                 f"AssetKey(s) {invalid_keys} were selected, but no AssetsDefinition objects supply "
                 "these keys. Make sure all keys are spelled correctly, and all AssetsDefinitions "
                 "are correctly added to the `Definitions`."
             )
         return specified_keys
 
 
 class KeyPrefixesAssetSelection(AssetSelection):
-    def __init__(self, *key_prefixes: Sequence[str]):
+    def __init__(self, *key_prefixes: Sequence[str], include_sources: bool):
         self._key_prefixes = key_prefixes
+        self._include_sources = include_sources
 
     def resolve_inner(self, asset_graph: AssetGraph) -> AbstractSet[AssetKey]:
+        base_set = (
+            asset_graph.all_asset_keys
+            if self._include_sources
+            else asset_graph.materializable_asset_keys
+        )
         return {
-            key
-            for key in asset_graph.all_asset_keys
-            if any(key.has_prefix(prefix) for prefix in self._key_prefixes)
+            key for key in base_set if any(key.has_prefix(prefix) for prefix in self._key_prefixes)
         }
 
 
 class OrAssetSelection(AssetSelection):
     def __init__(self, left: AssetSelection, right: AssetSelection):
         self._left = left
         self._right = right
 
     def resolve_inner(self, asset_graph: AssetGraph) -> AbstractSet[AssetKey]:
         return self._left.resolve_inner(asset_graph) | self._right.resolve_inner(asset_graph)
 
 
+def _fetch_all_upstream(
+    selection: AbstractSet[AssetKey],
+    asset_graph: AssetGraph,
+    depth: Optional[int] = None,
+    include_self: bool = True,
+) -> AbstractSet[AssetKey]:
+    return operator.sub(
+        reduce(
+            operator.or_,
+            [
+                {asset_key}
+                | fetch_connected(
+                    item=asset_key,
+                    graph=asset_graph.asset_dep_graph,
+                    direction="upstream",
+                    depth=depth,
+                )
+                for asset_key in selection
+            ],
+            set(),
+        ),
+        selection if not include_self else set(),
+    )
+
+
 class UpstreamAssetSelection(AssetSelection):
     def __init__(
         self,
         child: AssetSelection,
         *,
         depth: Optional[int] = None,
-        include_self: Optional[bool] = True,
+        include_self: bool = True,
     ):
         self._child = child
         self.depth = depth
         self.include_self = include_self
 
     def resolve_inner(self, asset_graph: AssetGraph) -> AbstractSet[AssetKey]:
         selection = self._child.resolve_inner(asset_graph)
         if len(selection) == 0:
             return selection
-
-        all_upstream = operator.sub(
-            reduce(
-                operator.or_,
-                [
-                    {asset_key}
-                    | fetch_connected(
-                        item=asset_key,
-                        graph=asset_graph.asset_dep_graph,
-                        direction="upstream",
-                        depth=self.depth,
-                    )
-                    for asset_key in selection
-                ],
-                set(),
-            ),
-            selection if not self.include_self else set(),
-        )
+        all_upstream = _fetch_all_upstream(selection, asset_graph, self.depth, self.include_self)
         return {key for key in all_upstream if key not in asset_graph.source_asset_keys}
+
+
+class SourceAssetSelection(AssetSelection):
+    def __init__(self, child: AssetSelection):
+        self._child = child
+
+    def resolve_inner(self, asset_graph: AssetGraph) -> AbstractSet[AssetKey]:
+        selection = self._child.resolve_inner(asset_graph)
+        if len(selection) == 0:
+            return selection
+        all_upstream = _fetch_all_upstream(selection, asset_graph)
+        return {key for key in all_upstream if key in asset_graph.source_asset_keys}
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/asset_sensor_definition.py` & `dagster-1.4.0/dagster/_core/definitions/asset_sensor_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -60,15 +60,15 @@
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]): The job
             object to target with this sensor.
         jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]]):
             (experimental) A list of jobs to be executed when the sensor fires.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
     """
 
     def __init__(
         self,
         name: str,
         asset_key: AssetKey,
         job_name: Optional[str],
@@ -163,13 +163,14 @@
             jobs=jobs,
             default_status=default_status,
             required_resource_keys=combined_required_resource_keys,
         )
 
     @public
     @property
-    def asset_key(self):
+    def asset_key(self) -> AssetKey:
+        """AssetKey: The key of the asset targeted by this sensor."""
         return self._asset_key
 
     @property
     def sensor_type(self) -> SensorType:
         return SensorType.ASSET
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/assets.py` & `dagster-1.4.0/dagster/_core/definitions/assets.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import hashlib
 import json
 import warnings
 from typing import (
     TYPE_CHECKING,
     AbstractSet,
+    Any,
     Dict,
     Iterable,
     Iterator,
     List,
     Mapping,
     Optional,
     Sequence,
@@ -28,15 +29,14 @@
 from dagster._core.definitions.partition_mapping import MultiPartitionMapping
 from dagster._core.definitions.time_window_partition_mapping import TimeWindowPartitionMapping
 from dagster._core.definitions.time_window_partitions import TimeWindowPartitionsDefinition
 from dagster._core.errors import DagsterInvalidDefinitionError, DagsterInvalidInvocationError
 from dagster._utils import IHasInternalInit
 from dagster._utils.backcompat import (
     ExperimentalWarning,
-    deprecation_warning,
     experimental_arg_warning,
 )
 from dagster._utils.merger import merge_dicts
 
 from .dependency import NodeHandle
 from .events import AssetKey, CoercibleToAssetKey, CoercibleToAssetKeyPrefix
 from .node_definition import NodeDefinition
@@ -99,16 +99,16 @@
         can_subset: bool = False,
         resource_defs: Optional[Mapping[str, object]] = None,
         group_names_by_key: Optional[Mapping[AssetKey, str]] = None,
         metadata_by_key: Optional[Mapping[AssetKey, ArbitraryMetadataMapping]] = None,
         freshness_policies_by_key: Optional[Mapping[AssetKey, FreshnessPolicy]] = None,
         auto_materialize_policies_by_key: Optional[Mapping[AssetKey, AutoMaterializePolicy]] = None,
         descriptions_by_key: Optional[Mapping[AssetKey, str]] = None,
-        # if adding new fields, make sure to handle them in the with_attributes
-        # and from_graph methods
+        # if adding new fields, make sure to handle them in the with_attributes, from_graph, and
+        # get_attributes_dict methods
     ):
         from dagster._core.execution.build_resources import wrap_resources_for_execution
 
         from .graph_definition import GraphDefinition
 
         if isinstance(node_def, GraphDefinition):
             _validate_graph_def(node_def)
@@ -612,92 +612,109 @@
             can_subset=can_subset,
             selected_asset_keys=None,  # node has no subselection info
         )
 
     @public
     @property
     def can_subset(self) -> bool:
+        """bool: If True, indicates that this AssetsDefinition may materialize any subset of its
+        asset keys in a given computation (as opposed to being required to materialize all asset
+        keys).
+        """
         return self._can_subset
 
     @public
     @property
     def group_names_by_key(self) -> Mapping[AssetKey, str]:
+        """Mapping[AssetKey, str]: Returns a mapping from the asset keys in this AssetsDefinition
+        to the group names assigned to them. If there is no assigned group name for a given AssetKey,
+        it will not be present in this dictionary.
+        """
         return self._group_names_by_key
 
     @public
     @property
     def descriptions_by_key(self) -> Mapping[AssetKey, str]:
+        """Mapping[AssetKey, str]: Returns a mapping from the asset keys in this AssetsDefinition
+        to the descriptions assigned to them. If there is no assigned description for a given AssetKey,
+        it will not be present in this dictionary.
+        """
         return self._descriptions_by_key
 
     @public
     @property
     def op(self) -> OpDefinition:
+        """OpDefinition: Returns the OpDefinition that is used to materialize the assets in this
+        AssetsDefinition.
+        """
         check.invariant(
             isinstance(self._node_def, OpDefinition),
             "The NodeDefinition for this AssetsDefinition is not of type OpDefinition.",
         )
         return cast(OpDefinition, self._node_def)
 
     @public
     @property
     def node_def(self) -> NodeDefinition:
+        """NodeDefinition: Returns the OpDefinition or GraphDefinition that is used to materialize
+        the assets in this AssetsDefinition.
+        """
         return self._node_def
 
     @public
     @property
     def asset_deps(self) -> Mapping[AssetKey, AbstractSet[AssetKey]]:
         """Maps assets that are produced by this definition to assets that they depend on. The
         dependencies can be either "internal", meaning that they refer to other assets that are
         produced by this definition, or "external", meaning that they refer to assets that aren't
         produced by this definition.
         """
         return self._asset_deps
 
     @property
     def input_names(self) -> Iterable[str]:
+        """Iterable[str]: The set of input names of the underlying NodeDefinition for this
+        AssetsDefinition.
+        """
         return self.keys_by_input_name.keys()
 
     @public
     @property
     def key(self) -> AssetKey:
+        """AssetKey: The asset key associated with this AssetsDefinition. If this AssetsDefinition
+        has more than one asset key, this will produce an error.
+        """
         check.invariant(
             len(self.keys) == 1,
             "Tried to retrieve asset key from an assets definition with multiple asset keys: "
             + ", ".join([str(ak.to_string()) for ak in self._keys_by_output_name.values()]),
         )
 
         return next(iter(self.keys))
 
-    @property
-    def asset_key(self) -> AssetKey:
-        deprecation_warning(
-            "AssetsDefinition.asset_key", "1.0.0", "Use AssetsDefinition.key instead."
-        )
-        return self.key
-
     @public
     @property
     def resource_defs(self) -> Mapping[str, ResourceDefinition]:
+        """Mapping[str, ResourceDefinition]: A mapping from resource name to ResourceDefinition for
+        the resources bound to this AssetsDefinition.
+        """
         return dict(self._resource_defs)
 
     @public
     @property
     def keys(self) -> AbstractSet[AssetKey]:
+        """AbstractSet[AssetKey]: The asset keys associated with this AssetsDefinition."""
         return self._selected_asset_keys
 
-    @property
-    def asset_keys(self) -> AbstractSet[AssetKey]:
-        deprecation_warning(
-            "AssetsDefinition.asset_keys", "1.0.0", "Use AssetsDefinition.keys instead."
-        )
-        return self.keys
-
     @public
     @property
     def dependency_keys(self) -> Iterable[AssetKey]:
+        """Iterable[AssetKey]: The asset keys which are upstream of any asset included in this
+        AssetsDefinition.
+        """
         # the input asset keys that are directly upstream of a selected asset key
         upstream_keys = {dep_key for key in self.keys for dep_key in self.asset_deps[key]}
         input_keys = set(self._keys_by_input_name.values())
         return upstream_keys.intersection(input_keys)
 
     @property
     def node_keys_by_output_name(self) -> Mapping[str, AssetKey]:
@@ -729,14 +746,16 @@
     @property
     def auto_materialize_policies_by_key(self) -> Mapping[AssetKey, AutoMaterializePolicy]:
         return self._auto_materialize_policies_by_key
 
     @public
     @property
     def partitions_def(self) -> Optional[PartitionsDefinition]:
+        """Optional[PartitionsDefinition]: The PartitionsDefinition for this AssetsDefinition (if any).
+        """
         return self._partitions_def
 
     @property
     def metadata_by_key(self) -> Mapping[AssetKey, ArbitraryMetadataMapping]:
         return self._metadata_by_key
 
     @property
@@ -745,14 +764,17 @@
 
     @property
     def partition_mappings(self) -> Mapping[AssetKey, PartitionMapping]:
         return self._partition_mappings
 
     @public
     def get_partition_mapping(self, in_asset_key: AssetKey) -> Optional[PartitionMapping]:
+        """Returns the partition mapping between keys in this AssetsDefinition and a given input
+        asset key (if any).
+        """
         return self._partition_mappings.get(in_asset_key)
 
     def get_partition_mapping_for_input(self, input_name: str) -> Optional[PartitionMapping]:
         return self._partition_mappings.get(self._keys_by_input_name[input_name])
 
     def infer_partition_mapping(
         self, upstream_asset_key: AssetKey, upstream_partitions_def: Optional[PartitionsDefinition]
@@ -777,18 +799,20 @@
         returns the op def within the graph that produces the given asset key.
         """
         output_name = self.get_output_name_for_asset_key(key)
         return self.node_def.resolve_output_to_origin_op_def(output_name)
 
     def with_attributes(
         self,
+        *,
         output_asset_key_replacements: Optional[Mapping[AssetKey, AssetKey]] = None,
         input_asset_key_replacements: Optional[Mapping[AssetKey, AssetKey]] = None,
         group_names_by_key: Optional[Mapping[AssetKey, str]] = None,
         descriptions_by_key: Optional[Mapping[AssetKey, str]] = None,
+        metadata_by_key: Optional[Mapping[AssetKey, ArbitraryMetadataMapping]] = None,
         freshness_policy: Optional[
             Union[FreshnessPolicy, Mapping[AssetKey, FreshnessPolicy]]
         ] = None,
         auto_materialize_policy: Optional[
             Union[AutoMaterializePolicy, Mapping[AssetKey, AutoMaterializePolicy]]
         ] = None,
     ) -> "AssetsDefinition":
@@ -806,14 +830,17 @@
         )
         group_names_by_key = check.opt_mapping_param(
             group_names_by_key, "group_names_by_key", key_type=AssetKey, value_type=str
         )
         descriptions_by_key = check.opt_mapping_param(
             descriptions_by_key, "descriptions_by_key", key_type=AssetKey, value_type=str
         )
+        metadata_by_key = check.opt_mapping_param(
+            metadata_by_key, "metadata_by_key", key_type=AssetKey, value_type=dict
+        )
 
         if group_names_by_key:
             group_name_conflicts = [
                 asset_key
                 for asset_key in group_names_by_key
                 if asset_key in self.group_names_by_key
                 and self.group_names_by_key[asset_key] != DEFAULT_GROUP_NAME
@@ -882,25 +909,31 @@
                 ] = replaced_auto_materialize_policy
 
         replaced_descriptions_by_key = {
             output_asset_key_replacements.get(key, key): description
             for key, description in descriptions_by_key.items()
         }
 
-        return __class__.dagster_internal_init(
+        if not metadata_by_key:
+            metadata_by_key = self.metadata_by_key
+
+        replaced_metadata_by_key = {
+            output_asset_key_replacements.get(key, key): metadata
+            for key, metadata in metadata_by_key.items()
+        }
+
+        replaced_attributes = dict(
             keys_by_input_name={
                 input_name: input_asset_key_replacements.get(key, key)
                 for input_name, key in self._keys_by_input_name.items()
             },
             keys_by_output_name={
                 output_name: output_asset_key_replacements.get(key, key)
                 for output_name, key in self._keys_by_output_name.items()
             },
-            node_def=self.node_def,
-            partitions_def=self.partitions_def,
             partition_mappings={
                 input_asset_key_replacements.get(key, key): partition_mapping
                 for key, partition_mapping in self._partition_mappings.items()
             },
             asset_deps={
                 # replace both the keys and the values in this mapping
                 output_asset_key_replacements.get(key, key): {
@@ -908,32 +941,29 @@
                         upstream_key,
                         output_asset_key_replacements.get(upstream_key, upstream_key),
                     )
                     for upstream_key in value
                 }
                 for key, value in self.asset_deps.items()
             },
-            can_subset=self.can_subset,
             selected_asset_keys={
                 output_asset_key_replacements.get(key, key) for key in self._selected_asset_keys
             },
-            resource_defs=self.resource_defs,
             group_names_by_key={
                 **replaced_group_names_by_key,
                 **group_names_by_key,
             },
-            metadata_by_key={
-                output_asset_key_replacements.get(key, key): value
-                for key, value in self.metadata_by_key.items()
-            },
+            metadata_by_key=replaced_metadata_by_key,
             freshness_policies_by_key=replaced_freshness_policies_by_key,
             auto_materialize_policies_by_key=replaced_auto_materialize_policies_by_key,
             descriptions_by_key=replaced_descriptions_by_key,
         )
 
+        return self.__class__(**merge_dicts(self.get_attributes_dict(), replaced_attributes))
+
     def _subset_graph_backed_asset(
         self,
         selected_asset_keys: AbstractSet[AssetKey],
     ):
         from dagster._core.definitions.graph_definition import GraphDefinition
 
         if not isinstance(self.node_def, GraphDefinition):
@@ -949,18 +979,15 @@
         for asset_key in selected_asset_keys:
             dep_node_handles = dep_node_handles_by_asset_key[asset_key]
             for dep_node_handle in dep_node_handles:
                 op_selection.append(".".join(dep_node_handle.path[1:]))
 
         return get_graph_subset(self.node_def, op_selection)
 
-    def subset_for(
-        self,
-        selected_asset_keys: AbstractSet[AssetKey],
-    ) -> "AssetsDefinition":
+    def subset_for(self, selected_asset_keys: AbstractSet[AssetKey]) -> "AssetsDefinition":
         """Create a subset of this AssetsDefinition that will only materialize the assets in the
         selected set.
 
         Args:
             selected_asset_keys (AbstractSet[AssetKey]): The total set of asset keys
         """
         from dagster._core.definitions.graph_definition import GraphDefinition
@@ -998,57 +1025,35 @@
 
             # An op within the graph-backed asset that yields multiple assets will be run
             # any time any of its output assets are selected. Thus, if an op yields multiple assets
             # and only one of them is selected, the op will still run and potentially unexpectedly
             # materialize the unselected asset.
             #
             # Thus, we include unselected assets that may be accidentally materialized in
-            # keys_by_output_name and asset_deps so that Dagit can populate an warning when this
-            # occurs. This is the same behavior as multi-asset subsetting.
+            # keys_by_output_name and asset_deps so that the webserver can populate an warning when
+            # this occurs. This is the same behavior as multi-asset subsetting.
 
             subsetted_asset_deps = {
                 out_asset_key: set(self._keys_by_input_name.values())
                 for out_asset_key in subsetted_keys_by_output_name.values()
             }
 
-            return AssetsDefinition.dagster_internal_init(
+            replaced_attributes = dict(
                 keys_by_input_name=subsetted_keys_by_input_name,
                 keys_by_output_name=subsetted_keys_by_output_name,
                 node_def=subsetted_node,
-                partitions_def=self.partitions_def,
-                partition_mappings=self._partition_mappings,
                 asset_deps=subsetted_asset_deps,
-                can_subset=self.can_subset,
                 selected_asset_keys=selected_asset_keys & self.keys,
-                resource_defs=self.resource_defs,
-                group_names_by_key=self.group_names_by_key,
-                metadata_by_key=self.metadata_by_key,
-                freshness_policies_by_key=self.freshness_policies_by_key,
-                auto_materialize_policies_by_key=self.auto_materialize_policies_by_key,
-                descriptions_by_key=self.descriptions_by_key,
             )
+
+            return self.__class__(**merge_dicts(self.get_attributes_dict(), replaced_attributes))
         else:
             # multi_asset subsetting
-            return AssetsDefinition.dagster_internal_init(
-                # keep track of the original mapping
-                keys_by_input_name=self._keys_by_input_name,
-                keys_by_output_name=self._keys_by_output_name,
-                node_def=self.node_def,
-                partitions_def=self.partitions_def,
-                partition_mappings=self._partition_mappings,
-                asset_deps=self._asset_deps,
-                can_subset=self.can_subset,
-                selected_asset_keys=asset_subselection,
-                resource_defs=self.resource_defs,
-                group_names_by_key=self.group_names_by_key,
-                metadata_by_key=self.metadata_by_key,
-                freshness_policies_by_key=self.freshness_policies_by_key,
-                auto_materialize_policies_by_key=self.auto_materialize_policies_by_key,
-                descriptions_by_key=self.descriptions_by_key,
-            )
+            replaced_attributes = dict(selected_asset_keys=asset_subselection)
+            return self.__class__(**merge_dicts(self.get_attributes_dict(), replaced_attributes))
 
     @public
     def to_source_assets(self) -> Sequence[SourceAsset]:
         """Returns a SourceAsset for each asset in this definition.
 
         Each produced SourceAsset will have the same key, metadata, io_manager_key, etc. as the
         corresponding asset
@@ -1124,38 +1129,38 @@
         yield from self.node_def.get_resource_requirements()  # type: ignore[attr-defined]
         for source_key, resource_def in self.resource_defs.items():
             yield from resource_def.get_resource_requirements(outer_context=source_key)
 
     @public
     @property
     def required_resource_keys(self) -> Set[str]:
+        """Set[str]: The set of keys for resources that must be provided to this AssetsDefinition.
+        """
         return {requirement.key for requirement in self.get_resource_requirements()}
 
     def __str__(self):
-        if len(self.asset_keys) == 1:
-            return f"AssetsDefinition with key {self.asset_key.to_string()}"
+        if len(self.keys) == 1:
+            return f"AssetsDefinition with key {self.key.to_string()}"
         else:
-            asset_keys = ", ".join(
-                sorted(([asset_key.to_string() for asset_key in self.asset_keys]))
-            )
+            asset_keys = ", ".join(sorted(([asset_key.to_string() for asset_key in self.keys])))
             return f"AssetsDefinition with keys {asset_keys}"
 
     @property
     def unique_id(self) -> str:
         """A unique identifier for the AssetsDefinition that's stable across processes."""
         return hashlib.md5((json.dumps(sorted(self.keys))).encode("utf-8")).hexdigest()
 
     def with_resources(self, resource_defs: Mapping[str, ResourceDefinition]) -> "AssetsDefinition":
         from dagster._core.execution.resources_init import get_transitive_required_resource_keys
 
         overlapping_keys = get_resource_key_conflicts(self.resource_defs, resource_defs)
         if overlapping_keys:
             overlapping_keys_str = ", ".join(sorted(list(overlapping_keys)))
             raise DagsterInvalidInvocationError(
-                f"{str(self)} has conflicting resource "
+                f"{self} has conflicting resource "
                 "definitions with provided resources for the following keys: "
                 f"{overlapping_keys_str}. Either remove the existing "
                 "resources from the asset or change the resource keys so that "
                 "they don't overlap."
             )
 
         merged_resource_defs = merge_dicts(resource_defs, self.resource_defs)
@@ -1170,29 +1175,34 @@
         )
         relevant_resource_defs = {
             key: resource_def
             for key, resource_def in merged_resource_defs.items()
             if key in relevant_keys
         }
 
-        return AssetsDefinition.dagster_internal_init(
+        attributes_dict = self.get_attributes_dict()
+        attributes_dict["resource_defs"] = relevant_resource_defs
+        return self.__class__(**attributes_dict)
+
+    def get_attributes_dict(self) -> Dict[str, Any]:
+        return dict(
             keys_by_input_name=self._keys_by_input_name,
             keys_by_output_name=self._keys_by_output_name,
-            node_def=self.node_def,
+            node_def=self._node_def,
             partitions_def=self._partitions_def,
             partition_mappings=self._partition_mappings,
-            asset_deps=self._asset_deps,
+            asset_deps=self.asset_deps,
             selected_asset_keys=self._selected_asset_keys,
             can_subset=self._can_subset,
-            resource_defs=relevant_resource_defs,
-            group_names_by_key=self.group_names_by_key,
-            metadata_by_key=self.metadata_by_key,
-            freshness_policies_by_key=self.freshness_policies_by_key,
-            auto_materialize_policies_by_key=self.auto_materialize_policies_by_key,
-            descriptions_by_key=self.descriptions_by_key,
+            resource_defs=self._resource_defs,
+            group_names_by_key=self._group_names_by_key,
+            metadata_by_key=self._metadata_by_key,
+            freshness_policies_by_key=self._freshness_policies_by_key,
+            auto_materialize_policies_by_key=self._auto_materialize_policies_by_key,
+            descriptions_by_key=self._descriptions_by_key,
         )
 
 
 def _infer_keys_by_input_names(
     node_def: Union["GraphDefinition", OpDefinition], keys_by_input_name: Mapping[str, AssetKey]
 ) -> Mapping[str, AssetKey]:
     all_input_names = [input_def.name for input_def in node_def.input_defs]
@@ -1262,27 +1272,28 @@
     )
 
     resource_defs = assets_def.resource_defs
     invocation_resources = context.resources._asdict()
     for resource_key in sorted(list(invocation_resources.keys())):
         if resource_key in resource_defs:
             raise DagsterInvalidInvocationError(
-                f"Error when invoking {str(assets_def)}: resource '{resource_key}' "
+                f"Error when invoking {assets_def}: resource '{resource_key}' "
                 "provided on both the definition and invocation context. Please "
                 "provide on only one or the other."
             )
     all_resources = merge_dicts(resource_defs, invocation_resources)
 
     if isinstance(context, UnboundOpExecutionContext):
         return build_op_context(
             resources=all_resources,
             config=context.op_config,
             resources_config=context._resources_config,  # noqa: SLF001
             instance=context._instance,  # noqa: SLF001
             partition_key=context._partition_key,  # noqa: SLF001
+            partition_key_range=context._partition_key_range,  # noqa: SLF001
             mapping_key=context._mapping_key,  # noqa: SLF001
             _assets_def=assets_def,
         )
     else:
         # If user is mocking OpExecutionContext, send it through (we don't know
         # what modifications they might be making, and we don't want to override)
         return context
@@ -1343,17 +1354,18 @@
                     time_window_partition_mapping is not None
                     and (time_window_partition_mapping.start_offset or 0) < 0
                     and (time_window_partition_mapping.end_offset or 0) < 0
                 ):
                     continue
 
             raise DagsterInvalidDefinitionError(
-                "Assets can only depend on themselves if they are:\n(a) time-partitioned and each"
-                " partition depends on earlier partitions\n(b) multipartitioned, with one time"
-                " dimension that depends on earlier time partitions"
+                f'Asset "{input_key.to_user_string()}" depends on itself. Assets can only depend'
+                " on themselves if they are:\n(a) time-partitioned and each partition depends on"
+                " earlier partitions\n(b) multipartitioned, with one time dimension that depends"
+                " on earlier time partitions"
             )
 
 
 def get_self_dep_time_window_partition_mapping(
     partition_mapping: Optional[PartitionMapping], partitions_def: Optional[PartitionsDefinition]
 ) -> Optional[TimeWindowPartitionMapping]:
     """Returns a time window partition mapping dimension of the provided partition mapping,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/assets_job.py` & `dagster-1.4.0/dagster/_core/definitions/assets_job.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/auto_materialize_condition.py` & `dagster-1.4.0/dagster/_core/definitions/auto_materialize_condition.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from enum import Enum
-from typing import NamedTuple, Union
+from typing import FrozenSet, NamedTuple, Optional, Union
 
+from dagster._core.definitions.events import AssetKey
 from dagster._serdes import whitelist_for_serdes
 
 
 @whitelist_for_serdes
 class AutoMaterializeDecisionType(Enum):
     """Represents the set of results of the auto-materialize logic.
 
@@ -55,14 +56,15 @@
 
 @whitelist_for_serdes
 class ParentOutdatedAutoMaterializeCondition(NamedTuple):
     """Indicates that this asset should be skipped because one or more of its parents are outdated.
     """
 
     decision_type: AutoMaterializeDecisionType = AutoMaterializeDecisionType.SKIP
+    waiting_on_asset_keys: Optional[FrozenSet[AssetKey]] = None
 
 
 @whitelist_for_serdes
 class MaxMaterializationsExceededAutoMaterializeCondition(NamedTuple):
     """Indicates that this asset should be discarded because materializing it would exceed the
     maximum number of materializations per minute.
     """
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/auto_materialize_policy.py` & `dagster-1.4.0/dagster/_core/definitions/auto_materialize_policy.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,94 +1,93 @@
-import datetime
 from enum import Enum
 from typing import NamedTuple, Optional
 
 import dagster._check as check
 from dagster._annotations import experimental, public
 from dagster._serdes.serdes import whitelist_for_serdes
 
 
 class AutoMaterializePolicyType(Enum):
     EAGER = "EAGER"
     LAZY = "LAZY"
 
 
 @experimental
-@whitelist_for_serdes
+@whitelist_for_serdes(old_fields={"time_window_partition_scope_minutes": 1e-6})
 class AutoMaterializePolicy(
     NamedTuple(
         "_AutoMaterializePolicy",
         [
             ("on_missing", bool),
             ("on_new_parent_data", bool),
             ("for_freshness", bool),
-            ("time_window_partition_scope_minutes", Optional[float]),
             ("max_materializations_per_minute", Optional[int]),
         ],
     )
 ):
     """An AutoMaterializePolicy specifies how Dagster should attempt to keep an asset up-to-date.
 
-    There are two main modes of reconciliation: eager and lazy.
+    There are two main kinds of auto-materialize policies: eager and lazy. In essence, an asset with
+    an eager policy will try to immediately materialize after upstream changes, while an asset with
+    a lazy policy will only materialize when necessary in order to satisfy the relevant
+    FreshnessPolicies.
 
-    Regardless of this selection an asset / partition will never be materialized if any of its
-    parents are missing.
-
-    For eager reconciliation, an asset / partition will be (re)materialized when:
+    For an asset / partition of an asset with an _eager_ policy to be auto-materialized, at least
+    one of the following must be true:
 
     - it is missing
     - it has a freshness policy that requires more up-to-date data
     - any of its descendants have a freshness policy that require more up-to-date data
-    - any of its parent assets / partitions have been updated more recently than it has
+    - any of its parent assets / partitions have newer data
 
-    For lazy reconciliation, an asset / partition will be (re)materialized when:
+    For an asset / partition of an asset with a _lazy_ policy to be auto-materialized, at least one
+    of the following must be true:
 
-    - it is missing
     - it has a freshness policy that requires more up-to-date data
     - any of its descendants have a freshness policy that require more up-to-date data
 
-    In essence, an asset with eager reconciliation will always incorporate the most up-to-date
-    version of its parents, while an asset with lazy reconciliation will only update when necessary
-    in order to stay in line with the relevant FreshnessPolicies.
+    If an asset / partition meets the above criteria, then it will be auto-materialized only if none
+    of the following are true:
+
+    - any of its parent assets / partitions are missing
+    - any of its ancestor assets / partitions have ancestors of their own with newer data
+
+    Lastly, the `max_materializations_per_minute` parameter, which is set to 1 by default,
+    rate-limits the number of auto-materializations that can occur for a particular asset within
+    a short time interval. This mainly matters for partitioned assets. Its purpose is to provide a
+    safeguard against "surprise backfills", where user-error causes auto-materialize to be
+    accidentally triggered for large numbers of partitions at once.
 
     **Warning:**
 
     Constructing an AutoMaterializePolicy directly is not recommended as the API is subject to change.
     AutoMaterializePolicy.eager() and AutoMaterializePolicy.lazy() are the recommended API.
 
     """
 
     def __new__(
         cls,
         on_missing: bool,
         on_new_parent_data: bool,
         for_freshness: bool,
-        time_window_partition_scope_minutes: Optional[float],
         max_materializations_per_minute: Optional[int] = 1,
     ):
         check.invariant(
             on_new_parent_data or for_freshness,
             "One of on_new_parent_data or for_freshness must be True",
         )
 
         return super(AutoMaterializePolicy, cls).__new__(
             cls,
             on_missing=on_missing,
             on_new_parent_data=on_new_parent_data,
             for_freshness=for_freshness,
-            time_window_partition_scope_minutes=time_window_partition_scope_minutes,
             max_materializations_per_minute=max_materializations_per_minute,
         )
 
-    @property
-    def time_window_partition_scope(self) -> Optional[datetime.timedelta]:
-        if self.time_window_partition_scope_minutes is None:
-            return None
-        return datetime.timedelta(minutes=self.time_window_partition_scope_minutes)
-
     @public
     @staticmethod
     def eager(max_materializations_per_minute: Optional[int] = 1) -> "AutoMaterializePolicy":
         """Constructs an eager AutoMaterializePolicy.
 
         Args:
             max_materializations_per_minute (Optional[int]): The maximum number of
@@ -96,15 +95,14 @@
                 is exceeded, the partitions which would have been materialized will be discarded,
                 and will require manual materialization in order to be updated. Defaults to 1.
         """
         return AutoMaterializePolicy(
             on_missing=True,
             on_new_parent_data=True,
             for_freshness=True,
-            time_window_partition_scope_minutes=datetime.timedelta.resolution.total_seconds() / 60,
             max_materializations_per_minute=check.opt_int_param(
                 max_materializations_per_minute, "max_materializations_per_minute"
             ),
         )
 
     @public
     @staticmethod
@@ -114,18 +112,17 @@
         Args:
             max_materializations_per_minute (Optional[int]): The maximum number of
                 auto-materializations for this asset that may be initiated per minute. If this limit
                 is exceeded, the partitions which would have been materialized will be discarded,
                 and will require manual materialization in order to be updated. Defaults to 1.
         """
         return AutoMaterializePolicy(
-            on_missing=True,
+            on_missing=False,
             on_new_parent_data=False,
             for_freshness=True,
-            time_window_partition_scope_minutes=datetime.timedelta.resolution.total_seconds() / 60,
             max_materializations_per_minute=check.opt_int_param(
                 max_materializations_per_minute, "max_materializations_per_minute"
             ),
         )
 
     @property
     def policy_type(self) -> AutoMaterializePolicyType:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/cacheable_assets.py` & `dagster-1.4.0/dagster/_core/definitions/cacheable_assets.py`

 * *Files 0% similar despite different names*

```diff
@@ -309,28 +309,28 @@
             contents.update(json.dumps(self._prefix_for_all_assets).encode("utf-8"))
         return contents.hexdigest()
 
     def transformed_assets_def(self, assets_def: AssetsDefinition) -> AssetsDefinition:
         group_names_by_key = (
             {
                 k: self._group_name_for_all_assets
-                for k in assets_def.asset_keys
+                for k in assets_def.keys
                 if self._group_name_for_all_assets
             }
             if self._group_name_for_all_assets
             else self._group_names_by_key
         )
         output_asset_key_replacements = (
             {
                 k: AssetKey(
                     path=self._prefix_for_all_assets + list(k.path)
                     if self._prefix_for_all_assets
                     else k.path
                 )
-                for k in assets_def.asset_keys
+                for k in assets_def.keys
             }
             if self._prefix_for_all_assets
             else self._output_asset_key_replacements
         )
         input_asset_key_replacements = (
             {
                 k: AssetKey(
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/composition.py` & `dagster-1.4.0/dagster/_core/definitions/composition.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/config.py` & `dagster-1.4.0/dagster/_core/definitions/config.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/configurable.py` & `dagster-1.4.0/dagster/_core/definitions/configurable.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 from abc import ABC, abstractmethod
-from typing import Any, Callable, Optional, TypeVar, Union
+from typing import Any, Callable, NamedTuple, Optional, Type, TypeVar, Union, cast
 
 from typing_extensions import Self
 
 from dagster import (
     Field,
     _check as check,
 )
 from dagster._config import EvaluateValueResult
 from dagster._config.config_schema import UserConfigSchema
+from dagster._core.decorator_utils import get_function_params
 
 from .definition_config_schema import (
     CoercableToConfigSchema,
     ConfiguredDefinitionConfigSchema,
     IDefinitionConfigSchema,
     convert_user_facing_definition_config_schema,
 )
@@ -74,15 +75,15 @@
         config_schema: CoercableToConfigSchema = None,
         description: Optional[str] = None,
     ) -> Self:
         """Wraps this object in an object of the same type that provides configuration to the inner
         object.
 
         Using ``configured`` may result in config values being displayed in
-        Dagit, so it is not recommended to use this API with sensitive values,
+        the Dagster UI, so it is not recommended to use this API with sensitive values,
         such as secrets.
 
         Args:
             config_or_config_fn (Union[Any, Callable[[Any], Any]]): Either (1) Run configuration
                 that fully satisfies this object's config schema or (2) A function that accepts run
                 configuration and returns run configuration that fully satisfies this object's
                 config schema.  In the latter case, config_schema must be specified.  When
@@ -119,15 +120,15 @@
         config_schema: Optional[UserConfigSchema] = None,
         description: Optional[str] = None,
     ) -> Self:
         """Wraps this object in an object of the same type that provides configuration to the inner
         object.
 
         Using ``configured`` may result in config values being displayed in
-        Dagit, so it is not recommended to use this API with sensitive values,
+        the Dagster UI, so it is not recommended to use this API with sensitive values,
         such as secrets.
 
         Args:
             config_or_config_fn (Union[Any, Callable[[Any], Any]]): Either (1) Run configuration
                 that fully satisfies this object's config schema or (2) A function that accepts run
                 configuration and returns run configuration that fully satisfies this object's
                 config schema.  In the latter case, config_schema must be specified.  When
@@ -188,14 +189,70 @@
 
 
 T_Configurable = TypeVar(
     "T_Configurable", bound=Union["AnonymousConfigurableDefinition", "NamedConfigurableDefinition"]
 )
 
 
+class FunctionAndConfigSchema(NamedTuple):
+    function: Callable[[Any], Any]
+    config_schema: Optional[UserConfigSchema]
+
+
+def _wrap_user_fn_if_pythonic_config(
+    user_fn: Any, config_schema: Optional[UserConfigSchema]
+) -> FunctionAndConfigSchema:
+    """Helper function which allows users to provide a Pythonic config object to a @configurable
+    function. Detects if the function has a single parameter annotated with a Config class.
+    If so, wraps the function to convert the config dictionary into the appropriate Config object.
+    """
+    from dagster._config.pythonic_config import (
+        Config,
+        infer_schema_from_config_annotation,
+        safe_is_subclass,
+    )
+
+    if not isinstance(user_fn, Callable):
+        return FunctionAndConfigSchema(function=user_fn, config_schema=config_schema)
+
+    config_fn_params = get_function_params(user_fn)
+    check.invariant(
+        len(config_fn_params) == 1, "@configured function should have exactly one parameter"
+    )
+
+    param = config_fn_params[0]
+
+    # If the parameter is a subclass of Config, we can infer the config schema from the
+    # type annotation. We'll also wrap the config mapping function to convert the config
+    # dictionary into the appropriate Config object.
+    if not safe_is_subclass(param.annotation, Config):
+        return FunctionAndConfigSchema(function=user_fn, config_schema=config_schema)
+
+    check.invariant(
+        config_schema is None,
+        "Cannot provide config_schema to @configured function with Config-annotated param",
+    )
+
+    config_schema_from_class = infer_schema_from_config_annotation(param.annotation, param.default)
+    config_cls = cast(Type[Config], param.annotation)
+
+    param_name = param.name
+
+    def wrapped_fn(config_as_dict) -> Any:
+        config_input = config_cls(**config_as_dict)
+        output = user_fn(**{param_name: config_input})
+
+        if isinstance(output, Config):
+            return output._convert_to_config_dictionary()  # noqa: SLF001
+        else:
+            return output
+
+    return FunctionAndConfigSchema(function=wrapped_fn, config_schema=config_schema_from_class)
+
+
 def configured(
     configurable: T_Configurable,
     config_schema: Optional[UserConfigSchema] = None,
     **kwargs: Any,
 ) -> Callable[[object], T_Configurable]:
     """A decorator that makes it easy to create a function-configured version of an object.
 
@@ -203,68 +260,93 @@
 
     * :py:class:`GraphDefinition`
     * :py:class:`ExecutorDefinition`
     * :py:class:`LoggerDefinition`
     * :py:class:`ResourceDefinition`
     * :py:class:`OpDefinition`
 
-    Using ``configured`` may result in config values being displayed in Dagit,
+    Using ``configured`` may result in config values being displayed in the Dagster UI,
     so it is not recommended to use this API with sensitive values, such as
     secrets.
 
     If the config that will be supplied to the object is constant, you may alternatively invoke this
     and call the result with a dict of config values to be curried. Examples of both strategies
     below.
 
     Args:
         configurable (ConfigurableDefinition): An object that can be configured.
         config_schema (ConfigSchema): The config schema that the inputs to the decorated function
-            must satisfy.
+            must satisfy. Alternatively, annotate the config parameter to the decorated function
+            with a subclass of :py:class:`Config` and omit this argument.
         **kwargs: Arbitrary keyword arguments that will be passed to the initializer of the returned
             object.
 
     Returns:
         (Callable[[Union[Any, Callable[[Any], Any]]], ConfigurableDefinition])
 
     **Examples:**
 
     .. code-block:: python
 
-        dev_s3 = configured(s3_resource, name="dev_s3")({'bucket': 'dev'})
+        class GreetingConfig(Config):
+            message: str
+
+        @op
+        def greeting_op(config: GreetingConfig):
+            print(config.message)
+
+        class HelloConfig(Config):
+            name: str
+
+        @configured(greeting_op)
+        def hello_op(config: HelloConfig):
+            return GreetingConfig(message=f"Hello, {config.name}!")
+
+    .. code-block:: python
+
+        dev_s3 = configured(S3Resource, name="dev_s3")({'bucket': 'dev'})
 
-        @configured(s3_resource)
+        @configured(S3Resource)
         def dev_s3(_):
             return {'bucket': 'dev'}
 
-        @configured(s3_resource, {'bucket_prefix', str})
+        @configured(S3Resource, {'bucket_prefix', str})
         def dev_s3(config):
             return {'bucket': config['bucket_prefix'] + 'dev'}
+
     """
     _check_configurable_param(configurable)
 
     if isinstance(configurable, NamedConfigurableDefinition):
 
         def _configured(config_or_config_fn: object) -> T_Configurable:
             fn_name = (
                 getattr(config_or_config_fn, "__name__", None)
                 if callable(config_or_config_fn)
                 else None
             )
             name: str = check.not_none(kwargs.get("name") or fn_name)
+
+            updated_fn, new_config_schema = _wrap_user_fn_if_pythonic_config(
+                config_or_config_fn, config_schema
+            )
             return configurable.configured(
-                config_or_config_fn=config_or_config_fn,
+                config_or_config_fn=updated_fn,
                 name=name,
-                config_schema=config_schema,
+                config_schema=new_config_schema,
                 **{k: v for k, v in kwargs.items() if k != "name"},
             )
 
         return _configured
     elif isinstance(configurable, AnonymousConfigurableDefinition):
 
         def _configured(config_or_config_fn: object) -> T_Configurable:
+            updated_fn, new_config_schema = _wrap_user_fn_if_pythonic_config(
+                config_or_config_fn, config_schema
+            )
             return configurable.configured(
-                config_schema=config_schema, config_or_config_fn=config_or_config_fn, **kwargs
+                config_schema=new_config_schema, config_or_config_fn=updated_fn, **kwargs
             )
 
         return _configured
     else:
         check.failed(f"Invalid configurable definition type: {type(configurable)}")
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/data_time.py` & `dagster-1.4.0/dagster/_core/definitions/data_time.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,35 @@
+"""The "data time" of an asset materialization is the timestamp on its earliest ancestor
+materialization.
+
+An asset materialization is parent of another asset materialization if both:
+- The child asset depends on the parent asset.
+- The parent materialization is the latest materialization of the parent asset that occurred before
+    the child materialization.
+
+The idea of data time is: if an asset is downstream of another asset, then the freshness of the data
+in the downstream asset depends on the freshness of the data in the upstream asset. No matter how
+recently you've materialized the downstream asset, it can't be fresher than the upstream
+materialization it was derived from.
+"""
+
 import datetime
 from typing import AbstractSet, Dict, Mapping, Optional, Sequence, Tuple, cast
 
 import pendulum
 
 import dagster._check as check
 from dagster._core.definitions.asset_graph import AssetGraph
 from dagster._core.definitions.asset_selection import AssetSelection
 from dagster._core.definitions.data_version import (
     DATA_VERSION_TAG,
     DataVersion,
     get_input_event_pointer_tag,
 )
-from dagster._core.definitions.events import AssetKey
+from dagster._core.definitions.events import AssetKey, AssetKeyPartitionKey
 from dagster._core.definitions.time_window_partitions import (
     TimeWindowPartitionsDefinition,
     TimeWindowPartitionsSubset,
 )
 from dagster._core.errors import DagsterInvariantViolationError
 from dagster._core.event_api import EventLogRecord
 from dagster._core.storage.dagster_run import FINISHED_STATUSES, DagsterRunStatus, RunsFilter
@@ -24,22 +38,25 @@
 from dagster._utils.caching_instance_queryer import CachingInstanceQueryer
 
 
 class CachingDataTimeResolver:
     _instance_queryer: CachingInstanceQueryer
     _asset_graph: AssetGraph
 
-    def __init__(self, instance_queryer: CachingInstanceQueryer, asset_graph: AssetGraph):
+    def __init__(self, instance_queryer: CachingInstanceQueryer):
         self._instance_queryer = instance_queryer
-        self._asset_graph = asset_graph
 
     @property
     def instance_queryer(self) -> CachingInstanceQueryer:
         return self._instance_queryer
 
+    @property
+    def asset_graph(self) -> AssetGraph:
+        return self.instance_queryer.asset_graph
+
     ####################
     # PARTITIONED DATA TIME
     ####################
 
     def _calculate_data_time_partitioned(
         self,
         asset_key: AssetKey,
@@ -104,14 +121,15 @@
         total_partition_counts = self._instance_queryer.get_materialized_partition_counts(asset_key)
 
         # these are the partitions that did not exist before this record was created
         net_new_partitions = {
             partition_key
             for partition_key, new_count in new_partition_counts.items()
             if new_count == total_partition_counts.get(partition_key)
+            and partitions_def.is_valid_partition_key(partition_key)
         }
 
         # there are new materializations, but they don't fill any new partitions
         if not net_new_partitions:
             return first_filled_time_window.end
 
         # the oldest time window that was newly filled
@@ -138,30 +156,30 @@
         """
         partition_data_time = self._calculate_data_time_partitioned(
             asset_key=asset_key,
             cursor=cursor,
             partitions_def=partitions_def,
         )
 
-        root_keys = AssetSelection.keys(asset_key).upstream().sources().resolve(self._asset_graph)
+        root_keys = AssetSelection.keys(asset_key).upstream().sources().resolve(self.asset_graph)
         return {key: partition_data_time for key in root_keys}
 
     ####################
     # UNPARTITIONED DATA TIME
     ####################
 
     def _upstream_records_by_key(
         self, asset_key: AssetKey, record_id: int, record_tags_dict: Mapping[str, str]
     ) -> Mapping[AssetKey, "EventLogRecord"]:
         upstream_records: Dict[AssetKey, EventLogRecord] = {}
 
-        for parent_key in self._asset_graph.get_parents(asset_key):
+        for parent_key in self.asset_graph.get_parents(asset_key):
             if (
-                parent_key in self._asset_graph.source_asset_keys
-                and not self._asset_graph.is_observable(parent_key)
+                parent_key in self.asset_graph.source_asset_keys
+                and not self.asset_graph.is_observable(parent_key)
             ):
                 continue
 
             input_event_pointer_tag = get_input_event_pointer_tag(parent_key)
             if input_event_pointer_tag not in record_tags_dict:
                 # if the input event id was not recorded (materialized pre-1.1.0), just grab
                 # the most recent event for this parent which happened before the current record
@@ -169,23 +187,19 @@
             elif record_tags_dict[input_event_pointer_tag] != "NULL":
                 # get the upstream event which was consumed when producing this materialization event
                 before_cursor = int(record_tags_dict[input_event_pointer_tag]) + 1
             else:
                 before_cursor = None
 
             if before_cursor is not None:
-                if parent_key in self._asset_graph.source_asset_keys:
-                    parent_record = self.instance_queryer.get_observation_record(
-                        asset_key=parent_key, before_cursor=before_cursor
+                parent_record = (
+                    self._instance_queryer.get_latest_materialization_or_observation_record(
+                        AssetKeyPartitionKey(parent_key), before_cursor=before_cursor
                     )
-                else:
-                    parent_record = self._instance_queryer.get_latest_materialization_record(
-                        parent_key, before_cursor=before_cursor
-                    )
-
+                )
                 if parent_record is not None:
                     upstream_records[parent_key] = parent_record
 
         return upstream_records
 
     @cached_method
     def _calculate_data_time_by_key_unpartitioned(
@@ -199,15 +213,15 @@
     ) -> Mapping[AssetKey, Optional[datetime.datetime]]:
         # find the upstream times of each of the parents of this asset
         record_tags_dict = dict(record_tags)
         upstream_records_by_key = self._upstream_records_by_key(
             asset_key, record_id, record_tags_dict
         )
         if not upstream_records_by_key:
-            if not self._asset_graph.has_non_source_parents(asset_key):
+            if not self.asset_graph.has_non_source_parents(asset_key):
                 return {
                     asset_key: datetime.datetime.fromtimestamp(
                         record_timestamp, tz=datetime.timezone.utc
                     )
                 }
             else:
                 return {}
@@ -286,25 +300,25 @@
         asset_key: AssetKey,
         record_id: Optional[int],
         record_timestamp: Optional[float],
         record_tags: Tuple[Tuple[str, str]],  # for hashability
         current_time: datetime.datetime,
     ) -> Mapping[AssetKey, Optional[datetime.datetime]]:
         if record_id is None:
-            return {key: None for key in self._asset_graph.get_non_source_roots(asset_key)}
+            return {key: None for key in self.asset_graph.get_non_source_roots(asset_key)}
         record_timestamp = check.not_none(record_timestamp)
 
-        partitions_def = self._asset_graph.get_partitions_def(asset_key)
+        partitions_def = self.asset_graph.get_partitions_def(asset_key)
         if isinstance(partitions_def, TimeWindowPartitionsDefinition):
             return self._calculate_data_time_by_key_time_partitioned(
                 asset_key=asset_key,
                 cursor=record_id,
                 partitions_def=partitions_def,
             )
-        elif self._asset_graph.is_observable(asset_key):
+        elif self.asset_graph.is_observable(asset_key):
             return self._calculate_data_time_by_key_observable_source(
                 asset_key=asset_key,
                 record_id=record_id,
                 record_tags=record_tags,
                 current_time=current_time,
             )
         else:
@@ -351,19 +365,19 @@
         # if key is not pending materialization within the run, then downstream assets will generally
         # be expected to consume the current version of the asset
         if asset_key not in planned_keys or asset_key in materialized_keys:
             return self.get_current_data_time(asset_key, current_time=current_time)
 
         # if you're here, then this asset is planned, but not materialized. in the worst case, this
         # asset's data time will be equal to the current time once it finishes materializing
-        if not self._asset_graph.has_non_source_parents(asset_key):
+        if not self.asset_graph.has_non_source_parents(asset_key):
             return current_time
 
         data_time = current_time
-        for parent_key in self._asset_graph.get_parents(asset_key):
+        for parent_key in self.asset_graph.get_parents(asset_key):
             parent_data_time = self._get_in_progress_data_time_in_run(
                 run_id=run_id, asset_key=parent_key, current_time=current_time
             )
             if parent_data_time is None:
                 return None
 
             data_time = min(data_time, parent_data_time)
@@ -440,40 +454,43 @@
     ####################
 
     def get_data_time_by_key_for_record(
         self,
         record: EventLogRecord,
         current_time: Optional[datetime.datetime] = None,
     ) -> Mapping[AssetKey, Optional[datetime.datetime]]:
-        """Method to enable calculating the timestamps of materializations of upstream assets
-        which were relevant to a given AssetMaterialization. These timestamps can be calculated relative
-        to any upstream asset keys.
+        """Method to enable calculating the timestamps of materializations or observations of
+        upstream assets which were relevant to a given AssetMaterialization. These timestamps can
+        be calculated relative to any upstream asset keys.
 
         The heart of this functionality is a recursive method which takes a given asset materialization
         and finds the most recent materialization of each of its parents which happened *before* that
         given materialization event.
         """
-        if record.asset_key is None or record.asset_materialization is None:
+        event = record.asset_materialization or record.asset_observation
+        if record.asset_key is None or event is None:
             raise DagsterInvariantViolationError(
-                "Can only calculate data times for records with a materialization event and an"
-                " asset_key."
+                "Can only calculate data times for records with a materialization / observation "
+                "event and an asset_key."
             )
 
         return self._calculate_data_time_by_key(
             asset_key=record.asset_key,
             record_id=record.storage_id,
             record_timestamp=record.event_log_entry.timestamp,
-            record_tags=make_hashable(record.asset_materialization.tags or {}),
+            record_tags=make_hashable(event.tags or {}),
             current_time=current_time or pendulum.now("UTC"),
         )
 
     def get_current_data_time(
         self, asset_key: AssetKey, current_time: datetime.datetime
     ) -> Optional[datetime.datetime]:
-        latest_record = self.instance_queryer.get_latest_materialization_record(asset_key)
+        latest_record = self.instance_queryer.get_latest_materialization_or_observation_record(
+            AssetKeyPartitionKey(asset_key)
+        )
         if latest_record is None:
             return None
 
         data_times = set(self.get_data_time_by_key_for_record(latest_record, current_time).values())
 
         if None in data_times or not data_times:
             return None
@@ -481,15 +498,15 @@
         return min(cast(AbstractSet[datetime.datetime], data_times), default=None)
 
     def get_current_minutes_late(
         self,
         asset_key: AssetKey,
         evaluation_time: datetime.datetime,
     ) -> Optional[float]:
-        freshness_policy = self._asset_graph.freshness_policies_by_key.get(asset_key)
+        freshness_policy = self.asset_graph.freshness_policies_by_key.get(asset_key)
         if freshness_policy is None:
             raise DagsterInvariantViolationError(
                 "Cannot calculate minutes late for asset without a FreshnessPolicy"
             )
 
         return freshness_policy.minutes_overdue(
             data_time=self.get_current_data_time(asset_key, current_time=evaluation_time),
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/data_version.py` & `dagster-1.4.0/dagster/_core/definitions/data_version.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,17 +21,19 @@
 
 from dagster import _check as check
 from dagster._annotations import deprecated, experimental
 from dagster._utils.cached_method import cached_method
 
 if TYPE_CHECKING:
     from dagster._core.definitions.asset_graph import AssetGraph
-    from dagster._core.definitions.events import AssetKey
+    from dagster._core.definitions.events import AssetKey, AssetMaterialization, AssetObservation
+    from dagster._core.event_api import EventLogRecord
     from dagster._core.events.log import EventLogEntry
     from dagster._core.instance import DagsterInstance
+    from dagster._utils.caching_instance_queryer import CachingInstanceQueryer
 
 
 class UnknownValue:
     pass
 
 
 def foo(x):
@@ -278,25 +280,27 @@
 class CachingStaleStatusResolver:
     """Used to resolve data version information. Avoids redundant database
     calls that would otherwise occur. Intended for use within the scope of a
     single "request" (e.g. GQL request, RunRequest resolution).
     """
 
     _instance: "DagsterInstance"
+    _instance_queryer: Optional["CachingInstanceQueryer"]
     _asset_graph: Optional["AssetGraph"]
     _asset_graph_load_fn: Optional[Callable[[], "AssetGraph"]]
 
     def __init__(
         self,
         instance: "DagsterInstance",
         asset_graph: Union["AssetGraph", Callable[[], "AssetGraph"]],
     ):
         from dagster._core.definitions.asset_graph import AssetGraph
 
         self._instance = instance
+        self._instance_queryer = None
         if isinstance(asset_graph, AssetGraph):
             self._asset_graph = asset_graph
             self._asset_graph_load_fn = None
         else:
             self._asset_graph = None
             self._asset_graph_load_fn = asset_graph
 
@@ -337,15 +341,15 @@
 
     def _get_stale_causes_materialized(self, key: AssetKey) -> Iterator[StaleCause]:
         code_version = self.asset_graph.get_code_version(key)
         provenance = self._get_current_data_provenance(key=key)
         dependency_keys = self.asset_graph.get_parents(key)
 
         # only used if no provenance available
-        materialization = check.not_none(self._get_latest_materialization_event(key=key))
+        materialization = check.not_none(self._get_latest_data_version_record(key=key))
         materialization_time = materialization.timestamp
 
         if provenance:
             if code_version and code_version != provenance.code_version:
                 yield StaleCause(key, StaleCauseCategory.CODE, "has a new code version")
 
             removed_deps = set(provenance.input_data_versions.keys()) - set(dependency_keys)
@@ -401,15 +405,15 @@
                         ],
                     )
             # If no provenance and dep is a materializable asset, then use materialization
             # timestamps instead of versions this should be removable eventually since
             # provenance is on all newer materializations. If dep is a source, then we'll never
             # provide a stale reason here.
             elif not self.asset_graph.is_source(dep_key):
-                dep_materialization = self._get_latest_materialization_event(key=dep_key)
+                dep_materialization = self._get_latest_data_version_record(key=dep_key)
                 if dep_materialization is None:
                     # The input must be new if it has no materialization
                     yield StaleCause(key, StaleCauseCategory.DATA, "has a new input", dep_key)
                 elif dep_materialization.timestamp > materialization_time:
                     yield StaleCause(
                         key,
                         StaleCauseCategory.DATA,
@@ -446,48 +450,52 @@
 
     @property
     def asset_graph(self) -> "AssetGraph":
         if self._asset_graph is None:
             self._asset_graph = check.not_none(self._asset_graph_load_fn)()
         return self._asset_graph
 
+    # This is lazily constructed because it depends on the asset graph, which needs to be lazily
+    # constructed for GQL performance reasons.
+    @property
+    def instance_queryer(self) -> "CachingInstanceQueryer":
+        from dagster._utils.caching_instance_queryer import CachingInstanceQueryer
+
+        if self._instance_queryer is None:
+            self._instance_queryer = CachingInstanceQueryer(self._instance, self.asset_graph)
+        return self._instance_queryer
+
     @cached_method
     def _get_current_data_version(self, *, key: AssetKey) -> DataVersion:
-        is_source = self.asset_graph.is_source(key)
-        event = self._instance.get_latest_data_version_record(
-            key,
-            is_source,
-        )
-        if event is None and is_source:
+        # Currently we can only use asset records, which are fetched in one shot, for non-source
+        # assets. This is because the most recent AssetObservation is not stored on the AssetRecord.
+        record = self._get_latest_data_version_record(key=key)
+        if self.asset_graph.is_source(key) and record is None:
             return DEFAULT_DATA_VERSION
-        elif event is None:
+        elif record is None:
             return NULL_DATA_VERSION
         else:
-            data_version = extract_data_version_from_entry(event.event_log_entry)
+            data_version = extract_data_version_from_entry(record.event_log_entry)
             return data_version or DEFAULT_DATA_VERSION
 
     @cached_method
-    def _get_latest_materialization_event(self, *, key: AssetKey) -> Optional[EventLogEntry]:
-        return self._instance.get_latest_materialization_event(key)
-
-    @cached_method
     def _is_current_data_version_user_provided(self, *, key: AssetKey) -> bool:
         if self.asset_graph.is_source(key):
             return True
         else:
             provenance = self._get_current_data_provenance(key=key)
             return provenance is not None and provenance.is_user_provided
 
     @cached_method
     def _get_current_data_provenance(self, *, key: AssetKey) -> Optional[DataProvenance]:
-        materialization = self._get_latest_materialization_event(key=key)
-        if materialization is None:
+        record = self._get_latest_data_version_record(key=key)
+        if record is None:
             return None
         else:
-            return extract_data_provenance_from_entry(materialization)
+            return extract_data_provenance_from_entry(record.event_log_entry)
 
     @cached_method
     def _is_partitioned_or_downstream(self, *, key: AssetKey) -> bool:
         if self.asset_graph.get_partitions_def(key):
             return True
         elif self.asset_graph.is_source(key):
             return False
@@ -504,7 +512,32 @@
     @cached_method
     def _is_volatile(self, *, key: AssetKey) -> bool:
         if self.asset_graph.is_source(key):
             return self.asset_graph.is_observable(key)
         else:
             deps = self.asset_graph.get_parents(key)
             return len(deps) == 0 or any(self._is_volatile(key=dep_key) for dep_key in deps)
+
+    @cached_method
+    def _get_latest_data_version_event(
+        self, *, key: AssetKey
+    ) -> Optional[Union["AssetMaterialization", "AssetObservation"]]:
+        record = self._get_latest_data_version_record(key=key)
+        if record:
+            entry = record.event_log_entry
+            return entry.asset_materialization or entry.asset_observation
+        else:
+            return None
+
+    @cached_method
+    def _get_latest_data_version_record(self, key: AssetKey) -> Optional["EventLogRecord"]:
+        from dagster._core.definitions.events import AssetKeyPartitionKey
+
+        # If an asset record is cached, all of its ancestors have already been cached.
+        if not self.asset_graph.is_source(
+            key
+        ) and not self.instance_queryer.has_cached_asset_record(key):
+            ancestors = self.asset_graph.get_ancestors(key, include_self=True)
+            self.instance_queryer.prefetch_asset_records(ancestors)
+        return self.instance_queryer.get_latest_materialization_or_observation_record(
+            asset_partition=AssetKeyPartitionKey(key)
+        )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/__init__.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/asset_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/asset_decorator.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 from typing import (
     AbstractSet,
     Any,
     Callable,
     Dict,
     Mapping,
     Optional,
+    Sequence,
     Set,
     Tuple,
     Union,
     cast,
     overload,
 )
 
@@ -20,28 +21,29 @@
 from dagster._core.decorator_utils import get_function_params, get_valid_name_permutations
 from dagster._core.definitions.auto_materialize_policy import AutoMaterializePolicy
 from dagster._core.definitions.freshness_policy import FreshnessPolicy
 from dagster._core.definitions.metadata import ArbitraryMetadataMapping, MetadataUserInput
 from dagster._core.definitions.resource_annotation import (
     get_resource_args,
 )
+from dagster._core.definitions.source_asset import SourceAsset
 from dagster._core.errors import DagsterInvalidDefinitionError
 from dagster._core.types.dagster_type import DagsterType
 from dagster._utils.backcompat import (
     ExperimentalWarning,
     deprecation_warning,
     experimental_arg_warning,
 )
 
 from ..asset_in import AssetIn
 from ..asset_out import AssetOut
 from ..assets import AssetsDefinition
 from ..decorators.graph_decorator import graph
 from ..decorators.op_decorator import _Op
-from ..events import AssetKey, CoercibleToAssetKeyPrefix
+from ..events import AssetKey, CoercibleToAssetKey, CoercibleToAssetKeyPrefix
 from ..input import In
 from ..output import GraphOut, Out
 from ..partition import PartitionsDefinition
 from ..policy import RetryPolicy
 from ..resource_definition import ResourceDefinition
 from ..utils import DEFAULT_IO_MANAGER_KEY, NoValueSentinel
 
@@ -55,15 +57,15 @@
 
 @overload
 def asset(
     *,
     name: Optional[str] = ...,
     key_prefix: Optional[CoercibleToAssetKeyPrefix] = None,
     ins: Optional[Mapping[str, AssetIn]] = ...,
-    non_argument_deps: Optional[Union[Set[AssetKey], Set[str]]] = ...,
+    deps: Optional[Sequence[Union[CoercibleToAssetKey, AssetsDefinition, SourceAsset]]] = ...,
     metadata: Optional[Mapping[str, Any]] = ...,
     description: Optional[str] = ...,
     config_schema: Optional[UserConfigSchema] = None,
     required_resource_keys: Optional[Set[str]] = ...,
     resource_defs: Optional[Mapping[str, object]] = ...,
     io_manager_def: Optional[object] = ...,
     io_manager_key: Optional[str] = ...,
@@ -73,25 +75,27 @@
     op_tags: Optional[Mapping[str, Any]] = ...,
     group_name: Optional[str] = ...,
     output_required: bool = ...,
     freshness_policy: Optional[FreshnessPolicy] = ...,
     auto_materialize_policy: Optional[AutoMaterializePolicy] = ...,
     retry_policy: Optional[RetryPolicy] = ...,
     code_version: Optional[str] = ...,
+    key: Optional[CoercibleToAssetKey] = None,
+    non_argument_deps: Optional[Union[Set[AssetKey], Set[str]]] = ...,
 ) -> Callable[[Callable[..., Any]], AssetsDefinition]:
     ...
 
 
 def asset(
     compute_fn: Optional[Callable] = None,
     *,
     name: Optional[str] = None,
     key_prefix: Optional[CoercibleToAssetKeyPrefix] = None,
     ins: Optional[Mapping[str, AssetIn]] = None,
-    non_argument_deps: Optional[Union[Set[AssetKey], Set[str]]] = None,
+    deps: Optional[Sequence[Union[CoercibleToAssetKey, AssetsDefinition, SourceAsset]]] = None,
     metadata: Optional[ArbitraryMetadataMapping] = None,
     description: Optional[str] = None,
     config_schema: Optional[UserConfigSchema] = None,
     required_resource_keys: Optional[Set[str]] = None,
     resource_defs: Optional[Mapping[str, object]] = None,
     io_manager_def: Optional[object] = None,
     io_manager_key: Optional[str] = None,
@@ -101,14 +105,16 @@
     op_tags: Optional[Mapping[str, Any]] = None,
     group_name: Optional[str] = None,
     output_required: bool = True,
     freshness_policy: Optional[FreshnessPolicy] = None,
     auto_materialize_policy: Optional[AutoMaterializePolicy] = None,
     retry_policy: Optional[RetryPolicy] = None,
     code_version: Optional[str] = None,
+    key: Optional[CoercibleToAssetKey] = None,
+    non_argument_deps: Optional[Union[Set[AssetKey], Set[str]]] = None,
 ) -> Union[AssetsDefinition, Callable[[Callable[..., Any]], AssetsDefinition]]:
     """Create a definition for how to compute an asset.
 
     A software-defined asset is the combination of:
       1. An asset key, e.g. the name of a table.
       2. A function, which can be run to compute the contents of the asset.
       3. A set of upstream assets that are provided as inputs to the function when computing the asset.
@@ -126,29 +132,30 @@
             letters, numbers, and _) and may not contain python reserved keywords.
         key_prefix (Optional[Union[str, Sequence[str]]]): If provided, the asset's key is the
             concatenation of the key_prefix and the asset's name, which defaults to the name of
             the decorated function. Each item in key_prefix must be a valid name in dagster (ie only
             contains letters, numbers, and _) and may not contain python reserved keywords.
         ins (Optional[Mapping[str, AssetIn]]): A dictionary that maps input names to information
             about the input.
-        non_argument_deps (Optional[Union[Set[AssetKey], Set[str]]]): Set of asset keys that are
-            upstream dependencies, but do not pass an input to the asset.
+        deps (Optional[Sequence[Union[AssetsDefinition, SourceAsset, AssetKey, str]]]):
+            The assets that are upstream dependencies, but do not correspond to a parameter of the
+            decorated function.
         config_schema (Optional[ConfigSchema): The configuration schema for the asset's underlying
             op. If set, Dagster will check that config provided for the op matches this schema and fail
             if it does not. If not set, Dagster will accept any config provided for the op.
         metadata (Optional[Dict[str, Any]]): A dict of metadata entries for the asset.
         required_resource_keys (Optional[Set[str]]): Set of resource handles required by the op.
         io_manager_key (Optional[str]): The resource key of the IOManager used
             for storing the output of the op as an asset, and for loading it in downstream ops
             (default: "io_manager"). Only one of io_manager_key and io_manager_def can be provided.
         io_manager_def (Optional[object]): (Experimental) The IOManager used for
             storing the output of the op as an asset,  and for loading it in
             downstream ops. Only one of io_manager_def and io_manager_key can be provided.
         compute_kind (Optional[str]): A string to represent the kind of computation that produces
-            the asset, e.g. "dbt" or "spark". It will be displayed in Dagit as a badge on the asset.
+            the asset, e.g. "dbt" or "spark". It will be displayed in the Dagster UI as a badge on the asset.
         dagster_type (Optional[DagsterType]): Allows specifying type validation functions that
             will be executed on the output of the decorated function after it runs.
         partitions_def (Optional[PartitionsDefinition]): Defines the set of partition keys that
             compose the asset.
         op_tags (Optional[Dict[str, Any]]): A dictionary of tags for the op that computes the asset.
             Frameworks may expect and require certain metadata to be attached to a op. Values that
             are not strings will be json encoded and must meet the criteria that
@@ -166,29 +173,35 @@
             with respect to its root data.
         auto_materialize_policy (AutoMaterializePolicy): (Experimental) Configure Dagster to automatically materialize
             this asset according to its FreshnessPolicy and when upstream dependencies change.
         retry_policy (Optional[RetryPolicy]): The retry policy for the op that computes the asset.
         code_version (Optional[str]): (Experimental) Version of the code that generates this asset. In
             general, versions should be set only for code that deterministically produces the same
             output when given the same inputs.
+        non_argument_deps (Optional[Union[Set[AssetKey], Set[str]]]): Deprecated, use deps instead.
+            Set of asset keys that are upstream dependencies, but do not pass an input to the asset.
 
     Examples:
         .. code-block:: python
 
             @asset
             def my_asset(my_upstream_asset: int) -> int:
                 return my_upstream_asset + 1
     """
 
     def create_asset():
+        upstream_asset_deps = _type_check_deps_and_non_argument_deps(
+            deps=deps, non_argument_deps=non_argument_deps
+        )
+
         return _Asset(
             name=cast(Optional[str], name),  # (mypy bug that it can't infer name is Optional[str])
             key_prefix=key_prefix,
             ins=ins,
-            non_argument_deps=_make_asset_keys(non_argument_deps),
+            deps=_make_asset_keys(upstream_asset_deps),
             metadata=metadata,
             description=description,
             config_schema=config_schema,
             required_resource_keys=required_resource_keys,
             resource_defs=resource_defs,
             io_manager_key=io_manager_key,
             io_manager_def=io_manager_def,
@@ -198,14 +211,15 @@
             op_tags=op_tags,
             group_name=group_name,
             output_required=output_required,
             freshness_policy=freshness_policy,
             auto_materialize_policy=auto_materialize_policy,
             retry_policy=retry_policy,
             code_version=code_version,
+            key=key,
         )
 
     if compute_fn is not None:
         return create_asset()(compute_fn)
 
     def inner(fn: Callable[..., Any]) -> AssetsDefinition:
         check.invariant(
@@ -231,15 +245,15 @@
 
 class _Asset:
     def __init__(
         self,
         name: Optional[str] = None,
         key_prefix: Optional[CoercibleToAssetKeyPrefix] = None,
         ins: Optional[Mapping[str, AssetIn]] = None,
-        non_argument_deps: Optional[Set[AssetKey]] = None,
+        deps: Optional[Set[AssetKey]] = None,
         metadata: Optional[ArbitraryMetadataMapping] = None,
         description: Optional[str] = None,
         config_schema: Optional[UserConfigSchema] = None,
         required_resource_keys: Optional[Set[str]] = None,
         resource_defs: Optional[Mapping[str, object]] = None,
         io_manager_key: Optional[str] = None,
         io_manager_def: Optional[object] = None,
@@ -249,22 +263,23 @@
         op_tags: Optional[Mapping[str, Any]] = None,
         group_name: Optional[str] = None,
         output_required: bool = True,
         freshness_policy: Optional[FreshnessPolicy] = None,
         auto_materialize_policy: Optional[AutoMaterializePolicy] = None,
         retry_policy: Optional[RetryPolicy] = None,
         code_version: Optional[str] = None,
+        key: Optional[CoercibleToAssetKey] = None,
     ):
         self.name = name
 
         if isinstance(key_prefix, str):
             key_prefix = [key_prefix]
         self.key_prefix = key_prefix
         self.ins = ins or {}
-        self.non_argument_deps = non_argument_deps
+        self.deps = deps
         self.metadata = metadata
         self.description = description
         self.required_resource_keys = check.opt_set_param(
             required_resource_keys, "required_resource_keys"
         )
         self.io_manager_key = io_manager_key
         self.io_manager_def = io_manager_def
@@ -277,26 +292,38 @@
         self.group_name = group_name
         self.output_required = output_required
         self.freshness_policy = freshness_policy
         self.retry_policy = retry_policy
         self.auto_materialize_policy = auto_materialize_policy
         self.code_version = code_version
 
+        if (name or key_prefix) and key:
+            raise DagsterInvalidDefinitionError(
+                "Cannot specify a name or key prefix for an asset when the key argument is"
+                " provided."
+            )
+
+        self.key = AssetKey.from_coercible(key) if key is not None else None
+
     def __call__(self, fn: Callable) -> AssetsDefinition:
         from dagster._config.pythonic_config import (
             validate_resource_annotated_function,
         )
         from dagster._core.execution.build_resources import wrap_resources_for_execution
 
         validate_resource_annotated_function(fn)
         asset_name = self.name or fn.__name__
 
-        asset_ins = build_asset_ins(fn, self.ins or {}, self.non_argument_deps)
+        asset_ins = build_asset_ins(fn, self.ins or {}, self.deps)
 
-        out_asset_key = AssetKey(list(filter(None, [*(self.key_prefix or []), asset_name])))
+        out_asset_key = (
+            AssetKey(list(filter(None, [*(self.key_prefix or []), asset_name])))
+            if not self.key
+            else self.key
+        )
         with warnings.catch_warnings():
             warnings.simplefilter("ignore", category=ExperimentalWarning)
 
             arg_resource_keys = {arg.name for arg in get_resource_args(fn)}
 
             bare_required_resource_keys = set(self.required_resource_keys)
 
@@ -382,37 +409,38 @@
             else None,
             auto_materialize_policies_by_key={out_asset_key: self.auto_materialize_policy}
             if self.auto_materialize_policy
             else None,
             asset_deps=None,  # no asset deps in single-asset decorator
             selected_asset_keys=None,  # no subselection in decorator
             can_subset=False,
-            metadata_by_key=None,  # not supported for now
+            metadata_by_key={out_asset_key: self.metadata} if self.metadata else None,
             descriptions_by_key=None,  # not supported for now
         )
 
 
 def multi_asset(
     *,
     outs: Mapping[str, AssetOut],
     name: Optional[str] = None,
     ins: Optional[Mapping[str, AssetIn]] = None,
-    non_argument_deps: Optional[Union[Set[AssetKey], Set[str]]] = None,
+    deps: Optional[Sequence[Union[CoercibleToAssetKey, AssetsDefinition, SourceAsset]]] = None,
     description: Optional[str] = None,
     config_schema: Optional[UserConfigSchema] = None,
     required_resource_keys: Optional[Set[str]] = None,
     compute_kind: Optional[str] = None,
     internal_asset_deps: Optional[Mapping[str, Set[AssetKey]]] = None,
     partitions_def: Optional[PartitionsDefinition] = None,
     op_tags: Optional[Mapping[str, Any]] = None,
     can_subset: bool = False,
     resource_defs: Optional[Mapping[str, object]] = None,
     group_name: Optional[str] = None,
     retry_policy: Optional[RetryPolicy] = None,
     code_version: Optional[str] = None,
+    non_argument_deps: Optional[Union[Set[AssetKey], Set[str]]] = None,
 ) -> Callable[[Callable[..., Any]], AssetsDefinition]:
     """Create a combined definition of multiple assets that are computed using the same op and same
     upstream assets.
 
     Each argument to the decorated function references an upstream asset that this asset depends on.
     The name of the argument designates the name of the upstream asset.
 
@@ -421,22 +449,23 @@
     corresponding to that asset in the `outs` parameter.
 
     Args:
         name (Optional[str]): The name of the op.
         outs: (Optional[Dict[str, AssetOut]]): The AssetOuts representing the produced assets.
         ins (Optional[Mapping[str, AssetIn]]): A dictionary that maps input names to information
             about the input.
-        non_argument_deps (Optional[Union[Set[AssetKey], Set[str]]]): Set of asset keys that are upstream
-            dependencies, but do not pass an input to the multi_asset.
+        deps (Optional[Sequence[Union[AssetsDefinition, SourceAsset, AssetKey, str]]]):
+            The assets that are upstream dependencies, but do not correspond to a parameter of the
+            decorated function.
         config_schema (Optional[ConfigSchema): The configuration schema for the asset's underlying
             op. If set, Dagster will check that config provided for the op matches this schema and fail
             if it does not. If not set, Dagster will accept any config provided for the op.
         required_resource_keys (Optional[Set[str]]): Set of resource handles required by the underlying op.
         compute_kind (Optional[str]): A string to represent the kind of computation that produces
-            the asset, e.g. "dbt" or "spark". It will be displayed in Dagit as a badge on the asset.
+            the asset, e.g. "dbt" or "spark". It will be displayed in the Dagster UI as a badge on the asset.
         internal_asset_deps (Optional[Mapping[str, Set[AssetKey]]]): By default, it is assumed
             that all assets produced by a multi_asset depend on all assets that are consumed by that
             multi asset. If this default is not correct, you pass in a map of output names to a
             corrected set of AssetKeys that they depend on. Any AssetKeys in this list must be either
             used as input to the asset or produced within the op.
         partitions_def (Optional[PartitionsDefinition]): Defines the set of partition keys that
             compose the assets.
@@ -451,14 +480,16 @@
             will be initialized during execution, and can be accessed from the
             context within the body of the function.
         group_name (Optional[str]): A string name used to organize multiple assets into groups. This
             group name will be applied to all assets produced by this multi_asset.
         retry_policy (Optional[RetryPolicy]): The retry policy for the op that computes the asset.
         code_version (Optional[str]): (Experimental) Version of the code encapsulated by the multi-asset. If set,
             this is used as a default code version for all defined assets.
+        non_argument_deps (Optional[Union[Set[AssetKey], Set[str]]]): Deprecated, use deps instead. Set of asset keys that are upstream
+            dependencies, but do not pass an input to the multi_asset.
 
     Examples:
         .. code-block:: python
 
             # Use IO managers to handle I/O:
             @multi_asset(
                 outs={
@@ -472,24 +503,28 @@
 
             # Handle I/O on your own:
             @multi_asset(
                 outs={
                     "asset1": AssetOut(),
                     "asset2": AssetOut(),
                 },
-                non_argument_deps={"asset0"},
+                deps={"asset0"},
             )
             def my_function():
                 asset0_value = load(path="asset0")
                 asset1_result, asset2_result = do_some_transformation(asset0_value)
                 write(asset1_result, path="asset1")
                 write(asset2_result, path="asset2")
     """
     from dagster._core.execution.build_resources import wrap_resources_for_execution
 
+    upstream_asset_deps = _type_check_deps_and_non_argument_deps(
+        deps=deps, non_argument_deps=non_argument_deps
+    )
+
     if resource_defs is not None:
         experimental_arg_warning("resource_defs", "multi_asset")
 
     asset_deps = check.opt_mapping_param(
         internal_asset_deps, "internal_asset_deps", key_type=str, value_type=set
     )
     required_resource_keys = check.opt_set_param(
@@ -505,27 +540,17 @@
         additional_message="Only dicts are supported for asset config_schema.",
     )
 
     bare_required_resource_keys = set(required_resource_keys)
     resource_defs_keys = set(resource_defs.keys())
     required_resource_keys = bare_required_resource_keys | resource_defs_keys
 
-    for out in outs.values():
-        if isinstance(out, Out) and not isinstance(out, AssetOut):
-            deprecation_warning(
-                "Passing Out objects as values for the out argument of @multi_asset",
-                "1.0.0",
-                additional_warn_txt="Use AssetOut instead.",
-            )
-
     def inner(fn: Callable[..., Any]) -> AssetsDefinition:
         op_name = name or fn.__name__
-        asset_ins = build_asset_ins(
-            fn, ins or {}, non_argument_deps=_make_asset_keys(non_argument_deps)
-        )
+        asset_ins = build_asset_ins(fn, ins or {}, deps=_make_asset_keys(upstream_asset_deps))
         asset_outs = build_asset_outs(outs)
 
         arg_resource_keys = {arg.name for arg in get_resource_args(fn)}
         check.param_invariant(
             len(bare_required_resource_keys or []) == 0 or len(arg_resource_keys) == 0,
             (
                 "Cannot specify resource requirements in both @multi_asset decorator and as"
@@ -582,15 +607,15 @@
             output_name: asset_key for asset_key, (output_name, _) in asset_outs.items()
         }
 
         # source group names from the AssetOuts (if any)
         group_names_by_key = {
             keys_by_output_name[output_name]: out.group_name
             for output_name, out in outs.items()
-            if isinstance(out, AssetOut) and out.group_name is not None
+            if out.group_name is not None
         }
         if group_name:
             check.invariant(
                 not group_names_by_key,
                 (
                     "Cannot set group_name parameter on multi_asset if one or more of the AssetOuts"
                     " supplied to this multi_asset have a group_name defined."
@@ -600,26 +625,31 @@
                 asset_key: group_name for asset_key in keys_by_output_name.values()
             }
 
         # source freshness policies from the AssetOuts (if any)
         freshness_policies_by_key = {
             keys_by_output_name[output_name]: out.freshness_policy
             for output_name, out in outs.items()
-            if isinstance(out, AssetOut) and out.freshness_policy is not None
+            if out.freshness_policy is not None
         }
         auto_materialize_policies_by_key = {
             keys_by_output_name[output_name]: out.auto_materialize_policy
             for output_name, out in outs.items()
-            if isinstance(out, AssetOut) and out.auto_materialize_policy is not None
+            if out.auto_materialize_policy is not None
         }
         partition_mappings = {
             keys_by_input_name[input_name]: asset_in.partition_mapping
             for input_name, asset_in in (ins or {}).items()
             if asset_in.partition_mapping is not None
         }
+        metadata_by_key = {
+            keys_by_output_name[output_name]: out.metadata
+            for output_name, out in outs.items()
+            if out.metadata is not None
+        }
 
         return AssetsDefinition.dagster_internal_init(
             keys_by_input_name=keys_by_input_name,
             keys_by_output_name=keys_by_output_name,
             node_def=op,
             asset_deps={keys_by_output_name[name]: asset_deps[name] for name in asset_deps},
             partitions_def=partitions_def,
@@ -627,27 +657,27 @@
             can_subset=can_subset,
             resource_defs=resource_defs,
             group_names_by_key=group_names_by_key,
             freshness_policies_by_key=freshness_policies_by_key,
             auto_materialize_policies_by_key=auto_materialize_policies_by_key,
             selected_asset_keys=None,  # no subselection in decorator
             descriptions_by_key=None,  # not supported for now
-            metadata_by_key=None,  # not supported for now
+            metadata_by_key=metadata_by_key,
         )
 
     return inner
 
 
 def build_asset_ins(
     fn: Callable,
     asset_ins: Mapping[str, AssetIn],
-    non_argument_deps: Optional[AbstractSet[AssetKey]],
+    deps: Optional[AbstractSet[AssetKey]],
 ) -> Mapping[AssetKey, Tuple[str, In]]:
     """Creates a mapping from AssetKey to (name of input, In object)."""
-    non_argument_deps = check.opt_set_param(non_argument_deps, "non_argument_deps", AssetKey)
+    deps = check.opt_set_param(deps, "deps", AssetKey)
 
     params = get_function_params(fn)
     is_context_provided = len(params) > 0 and params[0].name in get_valid_name_permutations(
         "context"
     )
     input_params = params[1:] if is_context_provided else params
 
@@ -697,15 +727,15 @@
         asset_key = asset_key or AssetKey(list(filter(None, [*(key_prefix or []), input_name])))
 
         ins_by_asset_key[asset_key] = (
             input_name.replace("-", "_"),
             In(metadata=metadata, input_manager_key=input_manager_key, dagster_type=dagster_type),
         )
 
-    for asset_key in non_argument_deps:
+    for asset_key in deps:
         stringified_asset_key = "_".join(asset_key.path).replace("-", "_")
         # mypy doesn't realize that Nothing is a valid type here
         ins_by_asset_key[asset_key] = (stringified_asset_key, In(cast(type, Nothing)))
 
     return ins_by_asset_key
 
 
@@ -973,16 +1003,81 @@
         )
 
         outs_by_asset_key[asset_key] = (output_name.replace("-", "_"), out)
 
     return outs_by_asset_key
 
 
-def _make_asset_keys(deps: Optional[Union[Set[AssetKey], Set[str]]]) -> Optional[Set[AssetKey]]:
-    """Convert all str items to AssetKey in the set."""
+def _type_check_deps_and_non_argument_deps(
+    deps: Optional[Sequence[Union[CoercibleToAssetKey, AssetsDefinition, SourceAsset]]],
+    non_argument_deps: Optional[Union[Set[AssetKey], Set[str]]],
+):
+    """Helper function for managing deps and non_argument_deps while non_argument_deps is still an accepted parameter.
+    Ensures:
+    1. only one of deps and non_argument_deps is provided.
+    2. multi assets AssetsDefinition is not passed to deps.
+    3. deprecation warning is fired for non_argument_deps.
+    """
+    if non_argument_deps is not None and deps is not None:
+        raise DagsterInvalidDefinitionError(
+            "Cannot specify both deps and non_argument_deps to @asset. Use only deps instead."
+        )
+
+    upstream_asset_deps: Optional[
+        Sequence[Union[CoercibleToAssetKey, AssetsDefinition, SourceAsset]]
+    ] = None
+    if deps is not None:
+        for dep in deps:
+            if isinstance(dep, AssetsDefinition):
+                # Only AssetsDefinition with a single asset can be passed
+                if len(dep.keys) > 1:
+                    raise DagsterInvalidDefinitionError(
+                        "Cannot pass a multi_asset AssetsDefinition as an argument to deps."
+                        " Instead, specify dependencies on the assets created by the multi_asset"
+                        f" via AssetKeys or strings. For the multi_asset {dep.node_def.name}, the"
+                        f" available keys are: {dep.keys}."
+                    )
+            elif isinstance(dep, SourceAsset):
+                # no additional type checking needed for SourceAssets
+                continue
+            else:
+                # confirm that dep is coercible to AssetKey
+                try:
+                    AssetKey.from_coercible(dep)
+                except check.CheckError:
+                    raise DagsterInvalidDefinitionError(
+                        f"Cannot pass an instance of type {type(dep)} to deps parameter of @asset."
+                        " Instead, pass AssetsDefinitions or AssetKeys."
+                    )
+
+        upstream_asset_deps = deps
+
+    if non_argument_deps is not None:
+        deprecation_warning("non_argument_deps", "2.0.0", "use parameter deps instead")
+        # this set -> list conversion is a side effect of the type changing from
+        # Union[Set[AssetKey], Set[str]] to Sequence[Union[AssetsDefinition, CoercibleToAssetKey, SourceAsset]]
+        check.set_param(non_argument_deps, "non_argument_deps", of_type=(AssetKey, str))
+        upstream_asset_deps = list(non_argument_deps)
+
+    return upstream_asset_deps
+
+
+def _make_asset_keys(
+    deps: Optional[Sequence[Union[CoercibleToAssetKey, AssetsDefinition, SourceAsset]]]
+) -> Optional[Set[AssetKey]]:
+    """Convert all items to AssetKey in a set. By putting all of the AssetKeys in a set, it will also deduplicate them.
+    """
     if deps is None:
         return deps
 
-    deps_asset_keys = {
-        AssetKey.from_user_string(dep) if isinstance(dep, str) else dep for dep in deps
-    }
+    deps_asset_keys: Set[AssetKey] = set()
+    for dep in deps:
+        if isinstance(dep, AssetsDefinition):
+            # this will error if the AssetsDefinition is a multi_asset, but we should have caught that
+            # earlier in execution
+            deps_asset_keys.add(dep.key)
+        elif isinstance(dep, SourceAsset):
+            deps_asset_keys.add(dep.key)
+        else:
+            deps_asset_keys.add(AssetKey.from_coercible(dep))
+
     return deps_asset_keys
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/config_mapping_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/config_mapping_decorator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/graph_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/graph_decorator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/hook_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/hook_decorator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/job_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/job_decorator.py`

 * *Files 0% similar despite different names*

```diff
@@ -174,35 +174,35 @@
             Describes how the job is parameterized at runtime.
 
             If no value is provided, then the schema for the job's run config is a standard
             format based on its ops and resources.
 
             If a dictionary is provided, then it must conform to the standard config schema, and
             it will be used as the job's run config for the job whenever the job is executed.
-            The values provided will be viewable and editable in the Dagit playground, so be
+            The values provided will be viewable and editable in the Dagster UI, so be
             careful with secrets.
 
             If a :py:class:`RunConfig` object is provided, then it will be used directly as the run config
             for the job whenever the job is executed, similar to providing a dictionary.
 
             If a :py:class:`ConfigMapping` object is provided, then the schema for the job's run config is
             determined by the config mapping, and the ConfigMapping, which should return
             configuration in the standard format to configure the job.
 
             If a :py:class:`PartitionedConfig` object is provided, then it defines a discrete set of config
             values that can parameterize the job, as well as a function for mapping those
             values to the base config. The values provided will be viewable and editable in the
-            Dagit playground, so be careful with secrets.
+            Dagster UI, so be careful with secrets.
         tags (Optional[Dict[str, Any]]):
             Arbitrary information that will be attached to the execution of the Job.
             Values that are not strings will be json encoded and must meet the criteria that
             `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag
             values provided at invocation time.
         metadata (Optional[Dict[str, RawMetadataValue]]):
-            Arbitrary information that will be attached to the JobDefinition and be viewable in Dagit.
+            Arbitrary information that will be attached to the JobDefinition and be viewable in the Dagster UI.
             Keys must be strings, and values must be python primitive types or one of the provided
             MetadataValue types
         logger_defs (Optional[Dict[str, LoggerDefinition]]):
             A dictionary of string logger identifiers to their implementations.
         executor_def (Optional[ExecutorDefinition]):
             How this Job will be executed. Defaults to :py:class:`multiprocess_executor` .
         op_retry_policy (Optional[RetryPolicy]): The default retry policy for all ops in this job.
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/op_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/op_decorator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/repository_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/repository_decorator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/schedule_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/schedule_decorator.py`

 * *Files 2% similar despite different names*

```diff
@@ -92,15 +92,15 @@
         execution_timezone (Optional[str]): Timezone in which the schedule should run.
             Supported strings for timezones are the ones provided by the
             `IANA time zone database <https://www.iana.org/time-zones>` - e.g. "America/Los_Angeles".
         description (Optional[str]): A human-readable description of the schedule.
         job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]): The job
             that should execute when this schedule runs.
         default_status (DefaultScheduleStatus): Whether the schedule starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         required_resource_keys (Optional[Set[str]]): The set of resource keys required by the schedule.
     """
 
     def inner(fn: RawScheduleEvaluationFunction) -> ScheduleDefinition:
         from dagster._config.pythonic_config import validate_resource_annotated_function
 
         check.callable_param(fn, "fn")
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/sensor_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/sensor_decorator.py`

 * *Files 4% similar despite different names*

```diff
@@ -57,15 +57,15 @@
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]):
             The job to be executed when the sensor fires.
         jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]]):
             (experimental) A list of jobs to be executed when the sensor fires.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         asset_selection (AssetSelection): (Experimental) an asset selection to launch a run for if
             the sensor condition is met. This can be provided instead of specifying a job.
     """
     check.opt_str_param(name, "name")
 
     def inner(fn: RawSensorEvaluationFunction) -> SensorDefinition:
         check.callable_param(fn, "fn")
@@ -96,14 +96,15 @@
     job_name: Optional[str] = None,
     name: Optional[str] = None,
     minimum_interval_seconds: Optional[int] = None,
     description: Optional[str] = None,
     job: Optional[ExecutableDefinition] = None,
     jobs: Optional[Sequence[ExecutableDefinition]] = None,
     default_status: DefaultSensorStatus = DefaultSensorStatus.STOPPED,
+    required_resource_keys: Optional[Set[str]] = None,
 ) -> Callable[[AssetMaterializationFunction,], AssetSensorDefinition,]:
     """Creates an asset sensor where the decorated function is used as the asset sensor's evaluation
     function.
 
     The decorated function may:
 
     1. Return a `RunRequest` object.
@@ -123,15 +124,15 @@
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]): The
             job to be executed when the sensor fires.
         jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]]):
             (experimental) A list of jobs to be executed when the sensor fires.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
 
 
     Example:
         .. code-block:: python
 
             from dagster import AssetKey, EventLogEntry, SensorEvaluationContext, asset_sensor
 
@@ -182,26 +183,27 @@
                         "{result} of type {type_}.  Should only return SkipReason or "
                         "RunRequest objects."
                     ).format(sensor_name=sensor_name, result=result, type_=type(result))
                 )
 
         # Preserve any resource arguments from the underlying function, for when we inspect the
         # wrapped function later on
-        _wrapped_fn.__signature__ = inspect.signature(fn)
+        _wrapped_fn = update_wrapper(_wrapped_fn, wrapped=fn)
 
         return AssetSensorDefinition(
             name=sensor_name,
             asset_key=asset_key,
             job_name=job_name,
             asset_materialization_fn=_wrapped_fn,
             minimum_interval_seconds=minimum_interval_seconds,
             description=description,
             job=job,
             jobs=jobs,
             default_status=default_status,
+            required_resource_keys=required_resource_keys,
         )
 
     return inner
 
 
 @experimental
 def multi_asset_sensor(
@@ -211,14 +213,15 @@
     name: Optional[str] = None,
     minimum_interval_seconds: Optional[int] = None,
     description: Optional[str] = None,
     job: Optional[ExecutableDefinition] = None,
     jobs: Optional[Sequence[ExecutableDefinition]] = None,
     default_status: DefaultSensorStatus = DefaultSensorStatus.STOPPED,
     request_assets: Optional[AssetSelection] = None,
+    required_resource_keys: Optional[Set[str]] = None,
 ) -> Callable[[MultiAssetMaterializationFunction,], MultiAssetSensorDefinition,]:
     """Creates an asset sensor that can monitor multiple assets.
 
     The decorated function is used as the asset sensor's evaluation
     function.  The decorated function may:
 
     1. Return a `RunRequest` object.
@@ -239,15 +242,15 @@
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]): The
             job to be executed when the sensor fires.
         jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]]):
             (experimental) A list of jobs to be executed when the sensor fires.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         request_assets (Optional[AssetSelection]): (Experimental) an asset selection to launch a run
             for if the sensor condition is met. This can be provided instead of specifying a job.
     """
     check.opt_str_param(name, "name")
 
     if not isinstance(monitored_assets, AssetSelection) and not (
         isinstance(monitored_assets, collections.abc.Sequence)
@@ -269,12 +272,13 @@
             asset_materialization_fn=fn,
             minimum_interval_seconds=minimum_interval_seconds,
             description=description,
             job=job,
             jobs=jobs,
             default_status=default_status,
             request_assets=request_assets,
+            required_resource_keys=required_resource_keys,
         )
         update_wrapper(sensor_def, wrapped=fn)
         return sensor_def
 
     return inner
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/decorators/source_asset_decorator.py` & `dagster-1.4.0/dagster/_core/definitions/decorators/source_asset_decorator.py`

 * *Files 4% similar despite different names*

```diff
@@ -26,14 +26,15 @@
     io_manager_key: Optional[str] = None,
     io_manager_def: Optional[object] = None,
     description: Optional[str] = None,
     group_name: Optional[str] = None,
     required_resource_keys: Optional[AbstractSet[str]] = None,
     resource_defs: Optional[Mapping[str, ResourceDefinition]] = None,
     partitions_def: Optional[PartitionsDefinition] = None,
+    auto_observe_interval_minutes: Optional[float] = None,
 ) -> "_ObservableSourceAsset":
     ...
 
 
 @experimental
 def observable_source_asset(
     observe_fn: Optional[SourceAssetObserveFunction] = None,
@@ -44,14 +45,15 @@
     io_manager_key: Optional[str] = None,
     io_manager_def: Optional[object] = None,
     description: Optional[str] = None,
     group_name: Optional[str] = None,
     required_resource_keys: Optional[AbstractSet[str]] = None,
     resource_defs: Optional[Mapping[str, ResourceDefinition]] = None,
     partitions_def: Optional[PartitionsDefinition] = None,
+    auto_observe_interval_minutes: Optional[float] = None,
 ) -> Union[SourceAsset, "_ObservableSourceAsset"]:
     """Create a `SourceAsset` with an associated observation function.
 
     The observation function of a source asset is wrapped inside of an op and can be executed as
     part of a job. Each execution generates an `AssetObservation` event associated with the source
     asset. The source asset observation function should return a metadata dictionary that will be
     attached to the `AssetObservation`.
@@ -74,14 +76,16 @@
             the name "default" is used.
         required_resource_keys (Optional[Set[str]]): Set of resource keys required by the observe op.
         resource_defs (Optional[Mapping[str, ResourceDefinition]]): (Experimental) resource
             definitions that may be required by the :py:class:`dagster.IOManagerDefinition` provided in
             the `io_manager_def` argument.
         partitions_def (Optional[PartitionsDefinition]): Defines the set of partition keys that
             compose the asset.
+        auto_observe_interval_minutes (Optional[float]): While the asset daemon is turned on, a run
+            of the observation function for this asset will be launched at this interval.
         observe_fn (Optional[SourceAssetObserveFunction]) Observation function for the source asset.
     """
     if observe_fn is not None:
         return _ObservableSourceAsset()(observe_fn)
 
     return _ObservableSourceAsset(
         name,
@@ -90,14 +94,15 @@
         io_manager_key,
         io_manager_def,
         description,
         group_name,
         required_resource_keys,
         resource_defs,
         partitions_def,
+        auto_observe_interval_minutes,
     )
 
 
 class _ObservableSourceAsset:
     def __init__(
         self,
         name: Optional[str] = None,
@@ -106,14 +111,15 @@
         io_manager_key: Optional[str] = None,
         io_manager_def: Optional[object] = None,
         description: Optional[str] = None,
         group_name: Optional[str] = None,
         required_resource_keys: Optional[AbstractSet[str]] = None,
         resource_defs: Optional[Mapping[str, ResourceDefinition]] = None,
         partitions_def: Optional[PartitionsDefinition] = None,
+        auto_observe_interval_minutes: Optional[float] = None,
     ):
         self.name = name
         if isinstance(key_prefix, str):
             key_prefix = [key_prefix]
         elif key_prefix is None:
             key_prefix = []
         self.key_prefix = key_prefix
@@ -121,14 +127,15 @@
         self.io_manager_key = io_manager_key
         self.io_manager_def = io_manager_def
         self.description = description
         self.group_name = group_name
         self.required_resource_keys = required_resource_keys
         self.resource_defs = resource_defs
         self.partitions_def = partitions_def
+        self.auto_observe_interval_minutes = auto_observe_interval_minutes
 
     def __call__(self, observe_fn: SourceAssetObserveFunction) -> SourceAsset:
         source_asset_name = self.name or observe_fn.__name__
         source_asset_key = AssetKey([*self.key_prefix, source_asset_name])
 
         arg_resource_keys = {arg.name for arg in get_resource_args(observe_fn)}
         decorator_resource_keys = set(self.required_resource_keys or [])
@@ -148,8 +155,9 @@
             io_manager_def=self.io_manager_def,
             description=self.description,
             group_name=self.group_name,
             _required_resource_keys=resolved_resource_keys,
             resource_defs=self.resource_defs,
             observe_fn=observe_fn,
             partitions_def=self.partitions_def,
+            auto_observe_interval_minutes=self.auto_observe_interval_minutes,
         )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/definition_config_schema.py` & `dagster-1.4.0/dagster/_core/definitions/definition_config_schema.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/definitions_class.py` & `dagster-1.4.0/dagster/_core/definitions/definitions_class.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 )
 
 import dagster._check as check
 from dagster._annotations import deprecated, experimental, public
 from dagster._config.pythonic_config import (
     attach_resource_id_to_key_mapping,
 )
+from dagster._core.definitions.asset_graph import InternalAssetGraph
 from dagster._core.definitions.events import AssetKey, CoercibleToAssetKey
 from dagster._core.definitions.executor_definition import ExecutorDefinition
 from dagster._core.definitions.logger_definition import LoggerDefinition
 from dagster._core.execution.build_resources import wrap_resources_for_execution
 from dagster._core.execution.with_resources import with_resources
 from dagster._core.executor.base import Executor
 from dagster._core.instance import DagsterInstance
@@ -513,7 +514,11 @@
         self,
     ) -> Union[RepositoryDefinition, PendingRepositoryDefinition]:
         """This method is used internally to access the inner repository during the loading process
         at CLI entry points. We explicitly do not want to resolve the pending repo because the entire
         point is to defer that resolution until later.
         """
         return self._created_pending_or_normal_repo
+
+    def get_asset_graph(self) -> InternalAssetGraph:
+        """Get the AssetGraph for this set of definitions."""
+        return self.get_repository_def().asset_graph
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/dependency.py` & `dagster-1.4.0/dagster/_core/definitions/dependency.py`

 * *Files 0% similar despite different names*

```diff
@@ -664,15 +664,17 @@
             check.str_param(output, "output"),
             check.opt_str_param(description, "description"),
         )
 
     def get_node_dependencies(self) -> Sequence["DependencyDefinition"]:
         return [self]
 
+    @public
     def is_fan_in(self) -> bool:
+        """Return True if the dependency is fan-in (always False for DependencyDefinition)."""
         return False
 
     def get_op_dependencies(self) -> Sequence["DependencyDefinition"]:
         return [self]
 
 
 class MultiDependencyDefinition(
@@ -749,24 +751,28 @@
             else:
                 check.failed(f"Unexpected dependencies entry {dep}")
 
         return super(MultiDependencyDefinition, cls).__new__(cls, deps)
 
     @public
     def get_node_dependencies(self) -> Sequence[DependencyDefinition]:
+        """Return the list of :py:class:`DependencyDefinition` contained by this object."""
         return [dep for dep in self.dependencies if isinstance(dep, DependencyDefinition)]
 
     @public
     def is_fan_in(self) -> bool:
+        """Return `True` if the dependency is fan-in (always True for MultiDependencyDefinition)."""
         return True
 
     @public
     def get_dependencies_and_mappings(
         self,
     ) -> Sequence[Union[DependencyDefinition, Type["MappedInputPlaceholder"]]]:
+        """Return the combined list of dependencies contained by this object, inculding of :py:class:`DependencyDefinition` and :py:class:`MappedInputPlaceholder` objects.
+        """
         return self.dependencies
 
 
 class DynamicCollectDependencyDefinition(
     NamedTuple("_DynamicCollectDependencyDefinition", [("node_name", str), ("output_name", str)]),
     IDependencyDefinition,
 ):
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/events.py` & `dagster-1.4.0/dagster/_core/definitions/events.py`

 * *Files 0% similar despite different names*

```diff
@@ -172,14 +172,18 @@
             return AssetKey(arg)
         else:
             check.failed(f"Unexpected type for AssetKey: {type(arg)}")
 
     def has_prefix(self, prefix: Sequence[str]) -> bool:
         return len(self.path) >= len(prefix) and self.path[: len(prefix)] == prefix
 
+    def with_prefix(self, prefix: "CoercibleToAssetKeyPrefix") -> "AssetKey":
+        prefix = key_prefix_from_coercible(prefix)
+        return AssetKey(list(prefix) + list(self.path))
+
 
 class AssetKeyPartitionKey(NamedTuple):
     """An AssetKey with an (optional) partition key. Refers either to a non-partitioned asset or a
     partition of a partitioned asset.
     """
 
     asset_key: AssetKey
@@ -334,24 +338,28 @@
     @property
     def metadata(self) -> Mapping[str, MetadataValue]:
         return self._metadata
 
     @public
     @property
     def mapping_key(self) -> str:
+        """The mapping_key that was set for this DynamicOutput at instantiation."""
         return self._mapping_key
 
     @public
     @property
     def value(self) -> T:
+        """The value that is returned by the compute function for this DynamicOut."""
         return self._value
 
     @public
     @property
     def output_name(self) -> str:
+        """Name of the :py:class:`DynamicOut` defined on the op that this DynamicOut is associated with.
+        """
         return self._output_name
 
     def __eq__(self, other: object) -> bool:
         return (
             isinstance(other, DynamicOutput)
             and self.value == other.value
             and self.output_name == other.output_name
@@ -468,15 +476,15 @@
     Op compute functions may yield events of this type whenever they wish to indicate to the
     Dagster framework (and the end user) that they have produced a materialized value as a
     side effect of computation. Unlike outputs, asset materializations can not be passed to other
     ops, and their persistence is controlled by op logic, rather than by the Dagster
     framework.
 
     Op authors should use these events to organize metadata about the side effects of their
-    computations, enabling tooling like the Assets dashboard in Dagit.
+    computations, enabling tooling like the Assets dashboard in the Dagster UI.
 
     Args:
         asset_key (Union[str, List[str], AssetKey]): A key to identify the materialized asset across
             job runs
         description (Optional[str]): A longer human-readable description of the materialized value.
         partition (Optional[str]): The name of the partition
             that was materialized.
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/executor_definition.py` & `dagster-1.4.0/dagster/_core/definitions/executor_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -124,14 +124,17 @@
         self, executor_config: Mapping[str, object]
     ) -> Sequence[ExecutorRequirement]:
         return self._requirements_fn(executor_config)
 
     @public
     @property
     def executor_creation_fn(self) -> Optional[ExecutorCreationFunction]:
+        """Callable that takes an :py:class:`InitExecutorContext` and returns an instance of
+        :py:class:`Executor`.
+        """
         return self._executor_creation_fn
 
     def copy_for_configured(self, name, description, config_schema) -> "ExecutorDefinition":
         return ExecutorDefinition(
             name=name,
             config_schema=config_schema,  # type: ignore
             executor_creation_fn=self.executor_creation_fn,
@@ -158,15 +161,15 @@
         config_schema: Optional[UserConfigSchema] = None,
         description: Optional[str] = None,
     ) -> Self:
         """Wraps this object in an object of the same type that provides configuration to the inner
         object.
 
         Using ``configured`` may result in config values being displayed in
-        Dagit, so it is not recommended to use this API with sensitive values,
+        the Dagster UI, so it is not recommended to use this API with sensitive values,
         such as secrets.
 
         Args:
             config_or_config_fn (Union[Any, Callable[[Any], Any]]): Either (1) Run configuration
                 that fully satisfies this object's config schema or (2) A function that accepts run
                 configuration and returns run configuration that fully satisfies this object's
                 config schema.  In the latter case, config_schema must be specified.  When
@@ -440,17 +443,17 @@
 
 def _check_intra_process_job(job: IJob) -> None:
     if not isinstance(job, ReconstructableJob):
         raise DagsterUnmetExecutorRequirementsError(
             "You have attempted to use an executor that uses multiple processes with the job"
             f' "{job.get_definition().name}" that is not reconstructable. Job must be loaded in a'
             " way that allows dagster to reconstruct them in a new process. This means: \n  *"
-            " using the file, module, or repository.yaml arguments of"
-            " dagit/dagster-graphql/dagster\n  * loading the job through the reconstructable()"
-            " function\n"
+            " using the file, module, or workspace.yaml arguments of"
+            " dagster-webserver/dagster-graphql/dagster\n  * loading the job through the"
+            " reconstructable() function\n"
         )
 
 
 def _check_non_ephemeral_instance(instance: "DagsterInstance") -> None:
     if instance.is_ephemeral:
         raise DagsterUnmetExecutorRequirementsError(
             "You have attempted to use an executor that uses multiple processes with an ephemeral"
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/external_asset_graph.py` & `dagster-1.4.0/dagster/_core/definitions/external_asset_graph.py`

 * *Files 3% similar despite different names*

```diff
@@ -40,26 +40,28 @@
         freshness_policies_by_key: Mapping[AssetKey, Optional[FreshnessPolicy]],
         auto_materialize_policies_by_key: Mapping[AssetKey, Optional[AutoMaterializePolicy]],
         required_multi_asset_sets_by_key: Optional[Mapping[AssetKey, AbstractSet[AssetKey]]],
         repo_handles_by_key: Mapping[AssetKey, RepositoryHandle],
         job_names_by_key: Mapping[AssetKey, Sequence[str]],
         code_versions_by_key: Mapping[AssetKey, Optional[str]],
         is_observable_by_key: Mapping[AssetKey, bool],
+        auto_observe_interval_minutes_by_key: Mapping[AssetKey, Optional[float]],
     ):
         super().__init__(
             asset_dep_graph=asset_dep_graph,
             source_asset_keys=source_asset_keys,
             partitions_defs_by_key=partitions_defs_by_key,
             partition_mappings_by_key=partition_mappings_by_key,
             group_names_by_key=group_names_by_key,
             freshness_policies_by_key=freshness_policies_by_key,
             auto_materialize_policies_by_key=auto_materialize_policies_by_key,
             required_multi_asset_sets_by_key=required_multi_asset_sets_by_key,
             code_versions_by_key=code_versions_by_key,
             is_observable_by_key=is_observable_by_key,
+            auto_observe_interval_minutes_by_key=auto_observe_interval_minutes_by_key,
         )
         self._repo_handles_by_key = repo_handles_by_key
         self._materialization_job_names_by_key = job_names_by_key
 
         self._asset_keys_by_job_name: Mapping[str, List[AssetKey]] = defaultdict(list)
         for asset_key, job_names in self._materialization_job_names_by_key.items():
             for job_name in job_names:
@@ -112,38 +114,43 @@
         group_names_by_key = {}
         freshness_policies_by_key = {}
         auto_materialize_policies_by_key = {}
         asset_keys_by_atomic_execution_unit_id: Dict[str, Set[AssetKey]] = defaultdict(set)
         repo_handles_by_key = {
             node.asset_key: repo_handle
             for repo_handle, node in repo_handle_external_asset_nodes
-            if not node.is_source
+            if not node.is_source or node.is_observable
         }
         job_names_by_key = {
             node.asset_key: node.job_names
             for _, node in repo_handle_external_asset_nodes
-            if not node.is_source
+            if not node.is_source or node.is_observable
         }
         code_versions_by_key = {
             node.asset_key: node.code_version
             for _, node in repo_handle_external_asset_nodes
             if not node.is_source
         }
 
         all_non_source_keys = {
             node.asset_key for _, node in repo_handle_external_asset_nodes if not node.is_source
         }
 
         is_observable_by_key = {key: False for key in all_non_source_keys}
+        auto_observe_interval_minutes_by_key = {}
 
         for repo_handle, node in repo_handle_external_asset_nodes:
             if node.is_source:
                 # We need to set this even if the node is a regular asset in another code location.
                 # `is_observable` will only ever be consulted in the source asset context.
                 is_observable_by_key[node.asset_key] = node.is_observable
+                auto_observe_interval_minutes_by_key[
+                    node.asset_key
+                ] = node.auto_observe_interval_minutes
+
                 if node.asset_key in all_non_source_keys:
                     # one location's source is another location's non-source
                     continue
 
                 source_asset_keys.add(node.asset_key)
 
             upstream[node.asset_key] = {dep.upstream_asset_key for dep in node.dependencies}
@@ -168,15 +175,15 @@
 
         downstream: Dict[AssetKey, Set[AssetKey]] = defaultdict(set)
         for asset_key, upstream_keys in upstream.items():
             for upstream_key in upstream_keys:
                 downstream[upstream_key].add(asset_key)
 
         required_multi_asset_sets_by_key: Dict[AssetKey, AbstractSet[AssetKey]] = {}
-        for _, asset_keys in asset_keys_by_atomic_execution_unit_id.items():
+        for asset_keys in asset_keys_by_atomic_execution_unit_id.values():
             if len(asset_keys) > 1:
                 for asset_key in asset_keys:
                     required_multi_asset_sets_by_key[asset_key] = asset_keys
 
         return cls(
             asset_dep_graph={"upstream": upstream, "downstream": downstream},
             source_asset_keys=source_asset_keys,
@@ -186,14 +193,15 @@
             freshness_policies_by_key=freshness_policies_by_key,
             auto_materialize_policies_by_key=auto_materialize_policies_by_key,
             required_multi_asset_sets_by_key=required_multi_asset_sets_by_key,
             repo_handles_by_key=repo_handles_by_key,
             job_names_by_key=job_names_by_key,
             code_versions_by_key=code_versions_by_key,
             is_observable_by_key=is_observable_by_key,
+            auto_observe_interval_minutes_by_key=auto_observe_interval_minutes_by_key,
         )
 
     @property
     def repository_handles_by_key(self) -> Mapping[AssetKey, RepositoryHandle]:
         return self._repo_handles_by_key
 
     def get_repository_handle(self, asset_key: AssetKey) -> RepositoryHandle:
@@ -203,15 +211,15 @@
         """Returns the names of jobs that materialize this asset."""
         return self._materialization_job_names_by_key[asset_key]
 
     def get_materialization_asset_keys_for_job(self, job_name: str) -> Sequence[AssetKey]:
         """Returns asset keys that are targeted for materialization in the given job."""
         return [
             k
-            for k in self.non_source_asset_keys
+            for k in self.materializable_asset_keys
             if job_name in self.get_materialization_job_names(k)
         ]
 
     def get_asset_keys_for_job(self, job_name: str) -> Sequence[AssetKey]:
         return self._asset_keys_by_job_name[job_name]
 
     def get_implicit_job_name_for_assets(self, asset_keys: Iterable[AssetKey]) -> Optional[str]:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/freshness_policy.py` & `dagster-1.4.0/dagster/_core/definitions/freshness_policy.py`

 * *Files 1% similar despite different names*

```diff
@@ -144,14 +144,29 @@
     def __reduce__(self):
         return (self._create, (self.maximum_lag_minutes, self.cron_schedule))
 
     @property
     def maximum_lag_delta(self) -> datetime.timedelta:
         return datetime.timedelta(minutes=self.maximum_lag_minutes)
 
+    def get_evaluation_tick(
+        self,
+        evaluation_time: datetime.datetime,
+    ) -> Optional[datetime.datetime]:
+        if self.cron_schedule:
+            # most recent cron schedule tick
+            schedule_ticks = reverse_cron_string_iterator(
+                end_timestamp=evaluation_time.timestamp(),
+                cron_string=self.cron_schedule,
+                execution_timezone=self.cron_schedule_timezone,
+            )
+            return next(schedule_ticks)
+        else:
+            return evaluation_time
+
     def minutes_overdue(
         self,
         data_time: Optional[datetime.datetime],
         evaluation_time: datetime.datetime,
     ) -> Optional[float]:
         """Returns a number of minutes past the specified freshness policy that this asset currently
         is. If the asset is missing upstream data, or is not materialized at all, then it is unknown
@@ -161,20 +176,12 @@
             data_time (Optional[datetime]): The timestamp of the data that was used to create the
                 current version of this asset.
             evaluation_time (datetime): The time at which we're evaluating the overdueness of this
                 asset. Generally, this is the current time.
         """
         if data_time is None:
             return None
-        if self.cron_schedule:
-            # most recent cron schedule tick
-            schedule_ticks = reverse_cron_string_iterator(
-                end_timestamp=evaluation_time.timestamp(),
-                cron_string=self.cron_schedule,
-                execution_timezone=self.cron_schedule_timezone,
-            )
-            evaluation_tick = next(schedule_ticks)
-        else:
-            evaluation_tick = evaluation_time
-
+        evaluation_tick = self.get_evaluation_tick(evaluation_time)
+        if evaluation_tick is None:
+            return None
         required_time = evaluation_tick - self.maximum_lag_delta
         return max(0.0, (required_time - data_time).total_seconds() / 60)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/freshness_policy_sensor_definition.py` & `dagster-1.4.0/dagster/_core/definitions/freshness_policy_sensor_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -192,15 +192,15 @@
         freshness_policy_sensor_fn (Callable[[FreshnessPolicySensorContext], None]): The core
             evaluation function for the sensor. Takes a :py:class:`~dagster.FreshnessPolicySensorContext`.
         asset_selection (AssetSelection): The asset selection monitored by the sensor.
         minimum_interval_seconds (Optional[int]): The minimum number of seconds that will elapse
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
     """
 
     def __init__(
         self,
         name: str,
         asset_selection: AssetSelection,
         freshness_policy_sensor_fn: Callable[..., None],
@@ -243,18 +243,18 @@
                 new_cursor = FreshnessPolicySensorCursor({})
                 context.update_cursor(new_cursor.to_json())
                 yield SkipReason(f"Initializing {name}.")
                 return
 
             evaluation_time = pendulum.now("UTC")
             asset_graph = context.repository_def.asset_graph
-            instance_queryer = CachingInstanceQueryer(context.instance, evaluation_time)
-            data_time_resolver = CachingDataTimeResolver(
-                instance_queryer=instance_queryer, asset_graph=asset_graph
+            instance_queryer = CachingInstanceQueryer(
+                context.instance, asset_graph, evaluation_time
             )
+            data_time_resolver = CachingDataTimeResolver(instance_queryer=instance_queryer)
             monitored_keys = asset_selection.resolve(asset_graph)
 
             # get the previous status from the cursor
             previous_minutes_late_by_key = FreshnessPolicySensorCursor.from_json(
                 context.cursor
             ).minutes_late_by_key
 
@@ -362,15 +362,15 @@
         name (Optional[str]): The name of the sensor. Defaults to the name of the decorated function.
         freshness_policy_sensor_fn (Callable[[FreshnessPolicySensorContext], None]): The core
             evaluation function for the sensor. Takes a :py:class:`~dagster.FreshnessPolicySensorContext`.
         minimum_interval_seconds (Optional[int]): The minimum number of seconds that will elapse
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
     """
 
     def inner(fn: Callable[..., None]) -> FreshnessPolicySensorDefinition:
         check.callable_param(fn, "fn")
         sensor_name = name or fn.__name__
 
         return FreshnessPolicySensorDefinition(
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/graph_definition.py` & `dagster-1.4.0/dagster/_core/definitions/graph_definition.py`

 * *Files 3% similar despite different names*

```diff
@@ -84,15 +84,15 @@
                 not a node. You have likely forgetten to annotate this function with
                 the @op or @graph decorators.'
                 """.format(
                     name=graph_name, func=node_def.__name__
                 )
             )
         else:
-            raise DagsterInvalidDefinitionError(f"Invalid item in node list: {repr(node_def)}")
+            raise DagsterInvalidDefinitionError(f"Invalid item in node list: {node_def!r}")
 
     return node_defs
 
 
 def create_adjacency_lists(
     nodes: Sequence[Node],
     dep_structure: DependencyStructure,
@@ -200,15 +200,15 @@
             Union[DependencyMapping[str], DependencyMapping[NodeInvocation]]
         ] = None,
         input_mappings: Optional[Sequence[InputMapping]] = None,
         output_mappings: Optional[Sequence[OutputMapping]] = None,
         config: Optional[ConfigMapping] = None,
         tags: Optional[Mapping[str, str]] = None,
         node_input_source_assets: Optional[Mapping[str, Mapping[str, "SourceAsset"]]] = None,
-        **kwargs: object,
+        **kwargs: Any,
     ):
         self._node_defs = _check_node_defs_arg(name, node_defs)
 
         # `dependencies` will be converted to `dependency_structure` and `node_dict`, which may
         # alternatively be passed directly (useful when copying)
         self._dependencies = normalize_dependency_dict(dependencies)
         self._dependency_structure, self._node_dict = create_execution_structure(
@@ -363,24 +363,36 @@
             if isinstance(node, GraphNode):
                 yield from node.definition.iterate_node_handles(cur_node_handle)
             yield cur_node_handle
 
     @public
     @property
     def input_mappings(self) -> Sequence[InputMapping]:
+        """Input mappings for the graph.
+
+        An input mapping is a mapping from an input of the graph to an input of a child node.
+        """
         return self._input_mappings
 
     @public
     @property
     def output_mappings(self) -> Sequence[OutputMapping]:
+        """Output mappings for the graph.
+
+        An output mapping is a mapping from an output of the graph to an output of a child node.
+        """
         return self._output_mappings
 
     @public
     @property
     def config_mapping(self) -> Optional[ConfigMapping]:
+        """The config mapping for the graph, if present.
+
+        By specifying a config mapping function, you can override the configuration for the child nodes contained within a graph.
+        """
         return self._config_mapping
 
     @property
     def has_config_mapping(self) -> bool:
         return self._config_mapping is not None
 
     def all_dagster_types(self) -> Iterable[DagsterType]:
@@ -576,32 +588,32 @@
                 Describes how the job is parameterized at runtime.
 
                 If no value is provided, then the schema for the job's run config is a standard
                 format based on its ops and resources.
 
                 If a dictionary is provided, then it must conform to the standard config schema, and
                 it will be used as the job's run config for the job whenever the job is executed.
-                The values provided will be viewable and editable in the Dagit playground, so be
+                The values provided will be viewable and editable in the Dagster UI, so be
                 careful with secrets.
 
                 If a :py:class:`ConfigMapping` object is provided, then the schema for the job's run config is
                 determined by the config mapping, and the ConfigMapping, which should return
                 configuration in the standard format to configure the job.
 
                 If a :py:class:`PartitionedConfig` object is provided, then it defines a discrete set of config
                 values that can parameterize the job, as well as a function for mapping those
                 values to the base config. The values provided will be viewable and editable in the
-                Dagit playground, so be careful with secrets.
+                Dagster UI, so be careful with secrets.
             tags (Optional[Mapping[str, Any]]):
                 Arbitrary information that will be attached to the execution of the Job.
                 Values that are not strings will be json encoded and must meet the criteria that
                 `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag
                 values provided at invocation time.
             metadata (Optional[Mapping[str, RawMetadataValue]]):
-                Arbitrary information that will be attached to the JobDefinition and be viewable in Dagit.
+                Arbitrary information that will be attached to the JobDefinition and be viewable in the Dagster UI.
                 Keys must be strings, and values must be python primitive types or one of the provided
                 MetadataValue types
             logger_defs (Optional[Mapping[str, LoggerDefinition]]):
                 A dictionary of string logger identifiers to their implementations.
             executor_def (Optional[ExecutorDefinition]):
                 How this Job will be executed. Defaults to :py:class:`multi_or_in_process_executor`,
                 which can be switched between multi-process and in-process modes of execution. The
@@ -742,35 +754,81 @@
 
         for dagster_type in self.all_dagster_types():
             yield from dagster_type.get_resource_requirements()
 
     @public
     @property
     def name(self) -> str:
+        """The name of the graph."""
         return super(GraphDefinition, self).name
 
     @public
     @property
     def tags(self) -> Mapping[str, str]:
+        """The tags associated with the graph."""
         return super(GraphDefinition, self).tags
 
     @public
     def alias(self, name: str) -> "PendingNodeInvocation":
+        """Aliases the graph with a new name.
+
+        Can only be used in the context of a :py:func:`@graph <graph>`, :py:func:`@job <job>`, or :py:func:`@asset_graph <asset_graph>` decorated function.
+
+        **Examples:**
+            .. code-block:: python
+
+                @job
+                def do_it_all():
+                    my_graph.alias("my_graph_alias")
+        """
         return super(GraphDefinition, self).alias(name)
 
     @public
     def tag(self, tags: Optional[Mapping[str, str]]) -> "PendingNodeInvocation":
+        """Attaches the provided tags to the graph immutably.
+
+        Can only be used in the context of a :py:func:`@graph <graph>`, :py:func:`@job <job>`, or :py:func:`@asset_graph <asset_graph>` decorated function.
+
+        **Examples:**
+            .. code-block:: python
+
+                @job
+                def do_it_all():
+                    my_graph.tag({"my_tag": "my_value"})
+        """
         return super(GraphDefinition, self).tag(tags)
 
     @public
     def with_hooks(self, hook_defs: AbstractSet[HookDefinition]) -> "PendingNodeInvocation":
+        """Attaches the provided hooks to the graph immutably.
+
+        Can only be used in the context of a :py:func:`@graph <graph>`, :py:func:`@job <job>`, or :py:func:`@asset_graph <asset_graph>` decorated function.
+
+        **Examples:**
+            .. code-block:: python
+
+                @job
+                def do_it_all():
+                    my_graph.with_hooks({my_hook})
+        """
         return super(GraphDefinition, self).with_hooks(hook_defs)
 
     @public
     def with_retry_policy(self, retry_policy: RetryPolicy) -> "PendingNodeInvocation":
+        """Attaches the provided retry policy to the graph immutably.
+
+        Can only be used in the context of a :py:func:`@graph <graph>`, :py:func:`@job <job>`, or :py:func:`@asset_graph <asset_graph>` decorated function.
+
+        **Examples:**
+            .. code-block:: python
+
+                @job
+                def do_it_all():
+                    my_graph.with_retry_policy(RetryPolicy(max_retries=5))
+        """
         return super(GraphDefinition, self).with_retry_policy(retry_policy)
 
     def resolve_input_to_destinations(
         self, input_handle: NodeInputHandle
     ) -> Sequence[NodeInputHandle]:
         all_destinations: List[NodeInputHandle] = []
         for mapping in self.input_mappings:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/hook_definition.py` & `dagster-1.4.0/dagster/_core/definitions/hook_definition.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/hook_invocation.py` & `dagster-1.4.0/dagster/_core/definitions/hook_invocation.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/inference.py` & `dagster-1.4.0/dagster/_core/definitions/inference.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,18 @@
 from inspect import Parameter, Signature, isgeneratorfunction, signature
-from typing import Any, Callable, Mapping, NamedTuple, Optional, Sequence
+from typing import (
+    Any,
+    Callable,
+    Mapping,
+    NamedTuple,
+    Optional,
+    Sequence,
+)
 
+from dagster._core.decorator_utils import get_type_hints
 from dagster._seven import is_module_available
 
 from .utils import NoValueSentinel
 
 IS_DOCSTRING_PARSER_AVAILABLE = is_module_available("docstring_parser")
 
 
@@ -51,57 +59,60 @@
 
         return docstring.returns.description
     except Exception:
         return None
 
 
 def infer_output_props(fn: Callable) -> InferredOutputProps:
-    sig = signature(fn)
-
-    annotation = Parameter.empty
-    if not isgeneratorfunction(fn):
-        annotation = sig.return_annotation
+    type_hints = get_type_hints(fn)
+    annotation = (
+        type_hints["return"]
+        if not isgeneratorfunction(fn) and "return" in type_hints
+        else Parameter.empty
+    )
 
     return InferredOutputProps(
         annotation=annotation,
         description=_infer_output_description_from_docstring(fn),
     )
 
 
 def has_explicit_return_type(fn: Callable) -> bool:
     sig = signature(fn)
     return sig.return_annotation is not Signature.empty
 
 
 def _infer_inputs_from_params(
     params: Sequence[Parameter],
+    type_hints: Mapping[str, object],
     descriptions: Optional[Mapping[str, Optional[str]]] = None,
 ) -> Sequence[InferredInputProps]:
     _descriptions: Mapping[str, Optional[str]] = descriptions or {}
     input_defs = []
     for param in params:
         if param.default is not Parameter.empty:
             input_def = InferredInputProps(
                 param.name,
-                param.annotation,
+                type_hints.get(param.name, param.annotation),
                 default_value=param.default,
                 description=_descriptions.get(param.name),
             )
         else:
             input_def = InferredInputProps(
                 param.name,
-                param.annotation,
+                type_hints.get(param.name, param.annotation),
                 description=_descriptions.get(param.name),
             )
 
         input_defs.append(input_def)
 
     return input_defs
 
 
 def infer_input_props(fn: Callable, context_arg_provided: bool) -> Sequence[InferredInputProps]:
     sig = signature(fn)
     params = list(sig.parameters.values())
+    type_hints = get_type_hints(fn)
     descriptions = _infer_input_description_from_docstring(fn)
     params_to_infer = params[1:] if context_arg_provided else params
-    defs = _infer_inputs_from_params(params_to_infer, descriptions=descriptions)
+    defs = _infer_inputs_from_params(params_to_infer, type_hints, descriptions=descriptions)
     return defs
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/input.py` & `dagster-1.4.0/dagster/_core/definitions/input.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,15 +23,15 @@
     normalize_metadata,
 )
 from dagster._core.errors import DagsterError, DagsterInvalidDefinitionError
 from dagster._core.types.dagster_type import (  # BuiltinScalarDagsterType,
     DagsterType,
     resolve_dagster_type,
 )
-from dagster._utils.backcompat import deprecation_warning, experimental_arg_warning
+from dagster._utils.backcompat import experimental_arg_warning
 
 from .inference import InferredInputProps
 from .utils import NoValueSentinel, check_valid_name
 
 if TYPE_CHECKING:
     from dagster._core.execution.context.input import InputContext
 
@@ -78,45 +78,43 @@
         name (str): Name of the input.
         dagster_type (Optional[Union[Type, DagsterType]]]): The type of this input.
             Users should provide the Python type of the objects that they expect to be passed for
             this input, or a :py:class:`DagsterType` that defines a runtime check that they want
             to be run on this input. Defaults to :py:class:`Any`.
         description (Optional[str]): Human-readable description of the input.
         default_value (Optional[Any]): The default value to use if no input is provided.
-        root_manager_key (Optional[str]): (Experimental) The resource key for the
-            :py:class:`RootInputManager` used for loading this input when it is not connected to an
-            upstream output.
         metadata (Optional[Dict[str, Any]]): A dict of metadata for the input.
         asset_key (Optional[Union[AssetKey, InputContext -> AssetKey]]): (Experimental) An AssetKey
             (or function that produces an AssetKey from the InputContext) which should be associated
             with this InputDefinition. Used for tracking lineage information through Dagster.
         asset_partitions (Optional[Union[AbstractSet[str], InputContext -> AbstractSet[str]]]): (Experimental) A
             set of partitions of the given asset_key (or a function that produces this list of
             partitions from the InputContext) which should be associated with this InputDefinition.
+        input_manager_key (Optional[str]): (Experimental) The resource key for the
+            :py:class:`InputManager` used for loading this input when it is not connected to an
+            upstream output.
     """
 
     _name: str
     _type_not_set: bool
     _dagster_type: DagsterType
     _description: Optional[str]
     _default_value: Any
     _input_manager_key: Optional[str]
-    _root_manager_key: Optional[str]
     _raw_metadata: ArbitraryMetadataMapping
     _metadata: Mapping[str, MetadataValue]
     _asset_key: Optional[Union[AssetKey, Callable[["InputContext"], AssetKey]]]
     _asset_partitions_fn: Optional[Callable[["InputContext"], Set[str]]]
 
     def __init__(
         self,
         name: str,
         dagster_type: object = None,
         description: Optional[str] = None,
         default_value: object = NoValueSentinel,
-        root_manager_key: Optional[str] = None,
         metadata: Optional[ArbitraryMetadataMapping] = None,
         asset_key: Optional[Union[AssetKey, Callable[["InputContext"], AssetKey]]] = None,
         asset_partitions: Optional[Union[Set[str], Callable[["InputContext"], Set[str]]]] = None,
         input_manager_key: Optional[str] = None
         # when adding new params, make sure to update combine_with_inferred and with_dagster_type below
     ):
         self._name = check_valid_name(name, allow_list=["config"])
@@ -124,29 +122,14 @@
         self._type_not_set = dagster_type is None
         self._dagster_type = check.inst(resolve_dagster_type(dagster_type), DagsterType)
 
         self._description = check.opt_str_param(description, "description")
 
         self._default_value = _check_default_value(self._name, self._dagster_type, default_value)
 
-        if root_manager_key:
-            deprecation_warning(
-                "root_manager_key",
-                "1.0.0",
-                additional_warn_txt="Use an InputManager with input_manager_key instead.",
-            )
-
-        if root_manager_key and input_manager_key:
-            raise DagsterInvalidDefinitionError(
-                f"Can't supply both root input manager key {root_manager_key} and input manager key"
-                f" {input_manager_key} on InputDefinition."
-            )
-
-        self._root_manager_key = check.opt_str_param(root_manager_key, "root_manager_key")
-
         self._input_manager_key = check.opt_str_param(input_manager_key, "input_manager_key")
 
         self._raw_metadata = check.opt_mapping_param(metadata, "metadata", key_type=str)
         self._metadata = normalize_metadata(self._raw_metadata, allow_invalid=True)
 
         if asset_key:
             experimental_arg_warning("asset_key", "InputDefinition.__init__")
@@ -189,18 +172,14 @@
 
     @property
     def default_value(self) -> Any:
         check.invariant(self.has_default_value, "Can only fetch default_value if has_default_value")
         return self._default_value
 
     @property
-    def root_manager_key(self) -> Optional[str]:
-        return self._root_manager_key
-
-    @property
     def input_manager_key(self) -> Optional[str]:
         return self._input_manager_key
 
     @property
     def metadata(self) -> ArbitraryMetadataMapping:
         return self._raw_metadata
 
@@ -308,28 +287,26 @@
             default_value = inferred.default_value
 
         return InputDefinition(
             name=self.name,
             dagster_type=dagster_type,
             description=description,
             default_value=default_value,
-            root_manager_key=self._root_manager_key,
             metadata=self.metadata,
             asset_key=self._asset_key,
             asset_partitions=self._asset_partitions_fn,
             input_manager_key=self._input_manager_key,
         )
 
     def with_dagster_type(self, dagster_type: DagsterType) -> "InputDefinition":
         return InputDefinition(
             name=self.name,
             dagster_type=dagster_type,
             description=self.description,
             default_value=self.default_value if self.has_default_value else NoValueSentinel,
-            root_manager_key=self._root_manager_key,
             metadata=self.metadata,
             asset_key=self._asset_key,
             asset_partitions=self._asset_partitions_fn,
             input_manager_key=self._input_manager_key,
         )
 
 
@@ -449,15 +426,14 @@
 class In(
     NamedTuple(
         "_In",
         [
             ("dagster_type", PublicAttr[Union[DagsterType, Type[NoValueSentinel]]]),
             ("description", PublicAttr[Optional[str]]),
             ("default_value", PublicAttr[Any]),
-            ("root_manager_key", PublicAttr[Optional[str]]),
             ("metadata", PublicAttr[Optional[Mapping[str, Any]]]),
             (
                 "asset_key",
                 PublicAttr[Optional[Union[AssetKey, Callable[["InputContext"], AssetKey]]]],
             ),
             (
                 "asset_partitions",
@@ -474,85 +450,68 @@
 
     Args:
         dagster_type (Optional[Union[Type, DagsterType]]]):
             The type of this input. Should only be set if the correct type can not
             be inferred directly from the type signature of the decorated function.
         description (Optional[str]): Human-readable description of the input.
         default_value (Optional[Any]): The default value to use if no input is provided.
-        root_manager_key (Optional[str]): (Experimental) The resource key for the
-            :py:class:`RootInputManager` used for loading this input when it is not connected to an
-            upstream output.
         metadata (Optional[Dict[str, RawMetadataValue]]): A dict of metadata for the input.
         asset_key (Optional[Union[AssetKey, InputContext -> AssetKey]]): (Experimental) An AssetKey
             (or function that produces an AssetKey from the InputContext) which should be associated
             with this In. Used for tracking lineage information through Dagster.
         asset_partitions (Optional[Union[Set[str], InputContext -> Set[str]]]): (Experimental) A
             set of partitions of the given asset_key (or a function that produces this list of
             partitions from the InputContext) which should be associated with this In.
+        input_manager_key (Optional[str]): (Experimental) The resource key for the
+            :py:class:`InputManager` used for loading this input when it is not connected to an
+            upstream output.
     """
 
     def __new__(
         cls,
         dagster_type: Union[Type, DagsterType] = NoValueSentinel,
         description: Optional[str] = None,
         default_value: Any = NoValueSentinel,
-        root_manager_key: Optional[str] = None,
         metadata: Optional[Mapping[str, RawMetadataValue]] = None,
         asset_key: Optional[Union[AssetKey, Callable[["InputContext"], AssetKey]]] = None,
         asset_partitions: Optional[Union[Set[str], Callable[["InputContext"], Set[str]]]] = None,
         input_manager_key: Optional[str] = None,
     ):
-        if root_manager_key and input_manager_key:
-            raise DagsterInvalidDefinitionError(
-                f"Can't supply both root input manager key {root_manager_key} and input manager key"
-                f" {input_manager_key} on InputDefinition."
-            )
-
-        if root_manager_key:
-            deprecation_warning(
-                "root_manager_key",
-                "1.0.0",
-                additional_warn_txt="Use an InputManager with input_manager_key instead.",
-            )
-
         return super(In, cls).__new__(
             cls,
             dagster_type=NoValueSentinel
             if dagster_type is NoValueSentinel
             else resolve_dagster_type(dagster_type),
             description=check.opt_str_param(description, "description"),
             default_value=default_value,
-            root_manager_key=check.opt_str_param(root_manager_key, "root_manager_key"),
             metadata=check.opt_mapping_param(metadata, "metadata", key_type=str),
             asset_key=check.opt_inst_param(asset_key, "asset_key", (AssetKey, FunctionType)),
             asset_partitions=asset_partitions,
             input_manager_key=check.opt_str_param(input_manager_key, "input_manager_key"),
         )
 
     @staticmethod
     def from_definition(input_def: InputDefinition) -> "In":
         return In(
             dagster_type=input_def.dagster_type,
             description=input_def.description,
             default_value=input_def._default_value,  # noqa: SLF001
-            root_manager_key=input_def.root_manager_key,
             metadata=input_def.metadata,
             asset_key=input_def._asset_key,  # noqa: SLF001
             asset_partitions=input_def._asset_partitions_fn,  # noqa: SLF001
             input_manager_key=input_def.input_manager_key,
         )
 
     def to_definition(self, name: str) -> InputDefinition:
         dagster_type = self.dagster_type if self.dagster_type is not NoValueSentinel else None
         return InputDefinition(
             name=name,
             dagster_type=dagster_type,
             description=self.description,
             default_value=self.default_value,
-            root_manager_key=self.root_manager_key,
             metadata=self.metadata,
             asset_key=self.asset_key,
             asset_partitions=self.asset_partitions,
             input_manager_key=self.input_manager_key,
         )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/instigation_logger.py` & `dagster-1.4.0/dagster/_core/definitions/instigation_logger.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/job_base.py` & `dagster-1.4.0/dagster/_core/definitions/job_base.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/job_definition.py` & `dagster-1.4.0/dagster/_core/definitions/job_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,15 +50,19 @@
     DagsterInvalidSubsetError,
     DagsterInvariantViolationError,
 )
 from dagster._core.selector.subset_selector import (
     AssetSelectionData,
     OpSelectionData,
 )
-from dagster._core.storage.io_manager import IOManagerDefinition, io_manager
+from dagster._core.storage.io_manager import (
+    IOManagerDefinition,
+    dagster_maintained_io_manager,
+    io_manager,
+)
 from dagster._core.storage.tags import MEMOIZED_RUN_TAG
 from dagster._core.types.dagster_type import DagsterType
 from dagster._core.utils import str_format_set
 from dagster._utils import IHasInternalInit
 from dagster._utils.backcompat import deprecation_warning, experimental_class_warning
 from dagster._utils.merger import merge_dicts
 
@@ -320,46 +324,70 @@
     @property
     def dependencies(self) -> DependencyMapping[NodeInvocation]:
         return self._graph_def.dependencies
 
     @public
     @property
     def executor_def(self) -> ExecutorDefinition:
+        """Returns the default :py:class:`ExecutorDefinition` for the job.
+
+        If the user has not specified an executor definition, then this will default to the :py:func:`multi_or_in_process_executor`. If a default is specified on the :py:class:`Definitions` object the job was provided to, then that will be used instead.
+        """
         return self._executor_def or DEFAULT_EXECUTOR_DEF
 
     @public
     @property
     def has_specified_executor(self) -> bool:
+        """Returns True if this job has explicitly specified an executor, and False if the executor was inherited through defaults or the :py:class:`Definitions` object the job was provided to.
+        """
         return self._executor_def is not None
 
     @public
     @property
     def resource_defs(self) -> Mapping[str, ResourceDefinition]:
+        """Returns the set of ResourceDefinition objects specified on the job.
+
+        This may not be the complete set of resources required by the job, since those can also be provided on the :py:class:`Definitions` object the job may be provided to.
+        """
         return self._resource_defs
 
     @public
     @property
     def partitioned_config(self) -> Optional[PartitionedConfig]:
+        """The partitioned config for the job, if it has one.
+
+        A partitioned config defines a way to map partition keys to run config for the job.
+        """
         return self._partitioned_config
 
     @public
     @property
     def config_mapping(self) -> Optional[ConfigMapping]:
+        """The config mapping for the job, if it has one.
+
+        A config mapping defines a way to map a top-level config schema to run config for the job.
+        """
         return self._config_mapping
 
     @public
     @property
     def loggers(self) -> Mapping[str, LoggerDefinition]:
+        """Returns the set of LoggerDefinition objects specified on the job.
+
+        If the user has not specified a mapping of :py:class:`LoggerDefinition` objects, then this will default to the :py:func:`colored_console_logger` under the key `console`. If a default is specified on the :py:class:`Definitions` object the job was provided to, then that will be used instead.
+        """
         from dagster._loggers import default_loggers
 
         return self._loggers or default_loggers()
 
     @public
     @property
     def has_specified_loggers(self) -> bool:
+        """Returns true if the job explicitly set loggers, and False if loggers were inherited through defaults or the :py:class:`Definitions` object the job was provided to.
+        """
         return self._loggers is not None
 
     @property
     def required_resource_keys(self) -> AbstractSet[str]:
         return self._required_resource_keys
 
     @property
@@ -371,14 +399,18 @@
         if self._run_config_schema is None:
             self._run_config_schema = _create_run_config_schema(self, self.required_resource_keys)
         return self._run_config_schema
 
     @public
     @property
     def partitions_def(self) -> Optional[PartitionsDefinition]:
+        """Returns the :py:class:`PartitionsDefinition` for the job, if it has one.
+
+        A partitions definition defines the set of partition keys the job operates on.
+        """
         return None if not self.partitioned_config else self.partitioned_config.partitions_def
 
     @property
     def hook_defs(self) -> AbstractSet[HookDefinition]:
         return self._hook_defs
 
     @property
@@ -770,15 +802,15 @@
         return new_job
 
     def _get_job_def_for_op_selection(self, op_selection: Iterable[str]) -> Self:
         try:
             sub_graph = get_graph_subset(self.graph, op_selection)
 
             # if explicit config was passed the config_mapping that resolves the defaults implicitly is
-            # very unlikely to work. The job will still present the default config in dagit.
+            # very unlikely to work. The job will still present the default config in the Dagster UI.
             config = (
                 None
                 if self.run_config is not None
                 else self.config_mapping or self.partitioned_config
             )
 
             return self._copy(
@@ -984,14 +1016,15 @@
         updated_resources = dict(resources)
         updated_resources[DEFAULT_IO_MANAGER_KEY] = mem_io_manager
         return updated_resources
 
     return resources
 
 
+@dagster_maintained_io_manager
 @io_manager(
     description="Built-in filesystem IO manager that stores and retrieves values using pickling."
 )
 def default_job_io_manager(init_context: "InitResourceContext"):
     # support overriding the default io manager via environment variables
     module_name = os.getenv("DAGSTER_DEFAULT_IO_MANAGER_MODULE")
     attribute_name = os.getenv("DAGSTER_DEFAULT_IO_MANAGER_ATTRIBUTE")
@@ -1024,14 +1057,15 @@
     # normally, default to the fs_io_manager
     from dagster._core.storage.fs_io_manager import PickledObjectFilesystemIOManager
 
     instance = check.not_none(init_context.instance)
     return PickledObjectFilesystemIOManager(base_dir=instance.storage_directory())
 
 
+@dagster_maintained_io_manager
 @io_manager(
     description="Built-in filesystem IO manager that stores and retrieves values using pickling.",
     config_schema={"base_dir": Field(StringSource, is_required=False)},
 )
 def default_job_io_manager_with_fs_io_manager_schema(init_context: "InitResourceContext"):
     # support overriding the default io manager via environment variables
     module_name = os.getenv("DAGSTER_DEFAULT_IO_MANAGER_MODULE")
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/load_assets_from_modules.py` & `dagster-1.4.0/dagster/_core/definitions/load_assets_from_modules.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/logger_definition.py` & `dagster-1.4.0/dagster/_core/definitions/logger_definition.py`

 * *Files 5% similar despite different names*

```diff
@@ -84,24 +84,30 @@
             )
 
             return logger_invocation_result(self, context)
 
     @public
     @property
     def logger_fn(self) -> "InitLoggerFunction":
+        """Callable[[InitLoggerContext], logging.Logger]: The function that will be invoked to
+        instantiate the logger.
+        """
         return self._logger_fn
 
     @public
     @property
     def config_schema(self) -> Any:
+        """Any: The schema for the logger's config. Configuration data available in `init_context.logger_config`.
+        """
         return self._config_schema
 
     @public
     @property
     def description(self) -> Optional[str]:
+        """Optional[str]: A human-readable description of the logger."""
         return self._description
 
     def copy_for_configured(
         self,
         description: Optional[str],
         config_schema: Any,
     ) -> "LoggerDefinition":
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/logger_invocation.py` & `dagster-1.4.0/dagster/_core/definitions/logger_invocation.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/materialize.py` & `dagster-1.4.0/dagster/_core/definitions/materialize.py`

 * *Files 1% similar despite different names*

```diff
@@ -32,15 +32,15 @@
 
     By default, will materialize assets to the local filesystem.
 
     Args:
         assets (Sequence[Union[AssetsDefinition, SourceAsset]]):
             The assets to materialize.
 
-            Unless you're using `non_argument_deps`, you must also include all assets that are
+            Unless you're using `deps` or `non_argument_deps`, you must also include all assets that are
             upstream of the assets that you want to materialize. This is because those upstream
             asset definitions have information that is needed to load their contents while
             materializing the downstream assets.
 
             You can use the `selection` argument to distinguish between assets that you want to
             materialize and assets that are just present for loading.
         resources (Optional[Mapping[str, object]]):
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/metadata/__init__.py` & `dagster-1.4.0/dagster/_core/definitions/metadata/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -82,15 +82,15 @@
     for k, v in metadata.items():
         try:
             normalized_value = normalize_metadata_value(v)
         except DagsterInvalidMetadata as e:
             if allow_invalid:
                 deprecation_warning(
                     "Support for arbitrary metadata values",
-                    "1.4.0",
+                    "2.0.0",
                     additional_warn_txt=(
                         "In the future, all user-supplied metadata values must be one of"
                         f" {RawMetadataValue}"
                     ),
                     stacklevel=4,  # to get the caller of `normalize_metadata`
                 )
                 normalized_value = TextMetadataValue(f"[{v.__class__.__name__}] (unserializable)")
@@ -136,15 +136,15 @@
 # ########################
 # ##### METADATA VALUE
 # ########################
 
 
 class MetadataValue(ABC, Generic[T_Packable]):
     """Utility class to wrap metadata values passed into Dagster events so that they can be
-    displayed in Dagit and other tooling.
+    displayed in the Dagster UI and other tooling.
 
     .. code-block:: python
 
         @op
         def emit_metadata(context, df):
             yield AssetMaterialization(
                 asset_key="my_dataset",
@@ -156,14 +156,15 @@
             )
     """
 
     @public
     @property
     @abstractmethod
     def value(self) -> T_Packable:
+        """The wrapped value."""
         raise NotImplementedError()
 
     @public
     @staticmethod
     def text(text: str) -> "TextMetadataValue":
         """Static constructor for a metadata value wrapping text as
         :py:class:`TextMetadataValue`. Can be used as the value type for the `metadata`
@@ -549,14 +550,15 @@
         return super(TextMetadataValue, cls).__new__(
             cls, check.opt_str_param(text, "text", default="")
         )
 
     @public
     @property
     def value(self) -> Optional[str]:
+        """Optional[str]: The wrapped text data."""
         return self.text
 
 
 @whitelist_for_serdes(storage_name="UrlMetadataEntryData")
 class UrlMetadataValue(
     NamedTuple(
         "_UrlMetadataValue",
@@ -576,14 +578,15 @@
         return super(UrlMetadataValue, cls).__new__(
             cls, check.opt_str_param(url, "url", default="")
         )
 
     @public
     @property
     def value(self) -> Optional[str]:
+        """Optional[str]: The wrapped URL."""
         return self.url
 
 
 @whitelist_for_serdes(storage_name="PathMetadataEntryData")
 class PathMetadataValue(
     NamedTuple("_PathMetadataValue", [("path", PublicAttr[Optional[str]])]), MetadataValue[str]
 ):
@@ -597,14 +600,15 @@
         return super(PathMetadataValue, cls).__new__(
             cls, check.opt_path_param(path, "path", default="")
         )
 
     @public
     @property
     def value(self) -> Optional[str]:
+        """Optional[str]: The wrapped path."""
         return self.path
 
 
 @whitelist_for_serdes(storage_name="NotebookMetadataEntryData")
 class NotebookMetadataValue(
     NamedTuple("_NotebookMetadataValue", [("path", PublicAttr[Optional[str]])]), MetadataValue[str]
 ):
@@ -618,14 +622,15 @@
         return super(NotebookMetadataValue, cls).__new__(
             cls, check.opt_path_param(path, "path", default="")
         )
 
     @public
     @property
     def value(self) -> Optional[str]:
+        """Optional[str]: The wrapped path to the notebook as a string."""
         return self.path
 
 
 @whitelist_for_serdes(storage_name="JsonMetadataEntryData")
 class JsonMetadataValue(
     NamedTuple(
         "_JsonMetadataValue",
@@ -649,14 +654,15 @@
         except TypeError:
             raise DagsterInvalidMetadata("Value is not JSON serializable.")
         return super(JsonMetadataValue, cls).__new__(cls, data)
 
     @public
     @property
     def value(self) -> Optional[Union[Sequence[Any], Mapping[str, Any]]]:
+        """Optional[Union[Sequence[Any], Dict[str, Any]]]: The wrapped JSON data."""
         return self.data
 
 
 @whitelist_for_serdes(storage_name="MarkdownMetadataEntryData")
 class MarkdownMetadataValue(
     NamedTuple(
         "_MarkdownMetadataValue",
@@ -673,16 +679,18 @@
     """
 
     def __new__(cls, md_str: Optional[str]):
         return super(MarkdownMetadataValue, cls).__new__(
             cls, check.opt_str_param(md_str, "md_str", default="")
         )
 
+    @public
     @property
     def value(self) -> Optional[str]:
+        """Optional[str]: The wrapped markdown as a string."""
         return self.md_str
 
 
 # This should be deprecated or fixed so that `value` does not return itself.
 @whitelist_for_serdes(storage_name="PythonArtifactMetadataEntryData")
 class PythonArtifactMetadataValue(
     NamedTuple(
@@ -705,14 +713,15 @@
         return super(PythonArtifactMetadataValue, cls).__new__(
             cls, check.str_param(module, "module"), check.str_param(name, "name")
         )
 
     @public
     @property
     def value(self) -> Self:
+        """PythonArtifactMetadataValue: Identity function."""
         return self
 
 
 @whitelist_for_serdes(storage_name="FloatMetadataEntryData")
 class FloatMetadataValue(
     NamedTuple(
         "_FloatMetadataValue",
@@ -782,16 +791,18 @@
     Args:
         run_id (str): The run id
     """
 
     def __new__(cls, run_id: str):
         return super(DagsterRunMetadataValue, cls).__new__(cls, check.str_param(run_id, "run_id"))
 
+    @public
     @property
     def value(self) -> str:
+        """str: The wrapped run id."""
         return self.run_id
 
 
 @whitelist_for_serdes(storage_name="DagsterAssetMetadataEntryData")
 class DagsterAssetMetadataValue(
     NamedTuple("_DagsterAssetMetadataValue", [("asset_key", PublicAttr["AssetKey"])]),
     MetadataValue["AssetKey"],
@@ -808,14 +819,15 @@
         return super(DagsterAssetMetadataValue, cls).__new__(
             cls, check.inst_param(asset_key, "asset_key", AssetKey)
         )
 
     @public
     @property
     def value(self) -> "AssetKey":
+        """AssetKey: The wrapped :py:class:`AssetKey`."""
         return self.asset_key
 
 
 # This should be deprecated or fixed so that `value` does not return itself.
 @experimental
 @whitelist_for_serdes(storage_name="TableMetadataEntryData")
 class TableMetadataValue(
@@ -871,14 +883,15 @@
             records,
             schema,
         )
 
     @public
     @property
     def value(self) -> Self:
+        """TableMetadataValue: Identity function."""
         return self
 
 
 @whitelist_for_serdes(storage_name="TableSchemaMetadataEntryData")
 class TableSchemaMetadataValue(
     NamedTuple("_TableSchemaMetadataValue", [("schema", PublicAttr[TableSchema])]),
     MetadataValue[TableSchema],
@@ -890,25 +903,29 @@
     """
 
     def __new__(cls, schema: TableSchema):
         return super(TableSchemaMetadataValue, cls).__new__(
             cls, check.inst_param(schema, "schema", TableSchema)
         )
 
+    @public
     @property
     def value(self) -> TableSchema:
+        """TableSchema: The wrapped :py:class:`TableSchema`."""
         return self.schema
 
 
 @whitelist_for_serdes(storage_name="NullMetadataEntryData")
 class NullMetadataValue(NamedTuple("_NullMetadataValue", []), MetadataValue[None]):
     """Representation of null."""
 
+    @public
     @property
     def value(self) -> None:
+        """None: The wrapped null value."""
         return None
 
 
 # ########################
 # ##### METADATA BACKCOMPAT
 # ########################
 
@@ -970,25 +987,25 @@
     Generic[T_MetadataValue],
 ):
     """A structure for describing metadata for Dagster events.
 
     .. note:: This class is no longer usable in any Dagster API, and will be completely removed in 2.0.
 
     Lists of objects of this type can be passed as arguments to Dagster events and will be displayed
-    in Dagit and other tooling.
+    in the Dagster UI and other tooling.
 
     Should be yielded from within an IO manager to append metadata for a given input/output event.
     For other event types, passing a dict with `MetadataValue` values to the `metadata` argument
     is preferred.
 
     Args:
         label (str): Short display label for this metadata entry.
         description (Optional[str]): A human-readable description of this metadata entry.
         value (MetadataValue): Typed metadata entry data. The different types allow
-            for customized display in tools like dagit.
+            for customized display in tools like the Dagster UI.
     """
 
     def __new__(
         cls,
         label: str,
         description: Optional[str] = None,
         entry_data: Optional["RawMetadataValue"] = None,
@@ -1004,15 +1021,15 @@
         value = cast(
             RawMetadataValue,
             canonicalize_backcompat_args(
                 new_val=value,
                 new_arg="value",
                 old_val=entry_data,
                 old_arg="entry_data",
-                breaking_version="1.0.0",
+                breaking_version="2.0.0",
             ),
         )
         value = normalize_metadata_value(value)
 
         return super(MetadataEntry, cls).__new__(
             cls,
             check.str_param(label, "label"),
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/metadata/table.py` & `dagster-1.4.0/dagster/_core/definitions/metadata/table.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/multi_asset_sensor_definition.py` & `dagster-1.4.0/dagster/_core/definitions/multi_asset_sensor_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -766,19 +766,24 @@
         self._cursor_advance_state_mutation.add_advanced_records(materializations_by_key)
         self._cursor_advance_state_mutation.advance_all_cursors_called = True
         self._cursor_updated = True
 
     @public
     @property
     def assets_defs_by_key(self) -> Mapping[AssetKey, Optional[AssetsDefinition]]:
+        """Mapping[AssetKey, Optional[AssetsDefinition]]: A mapping from AssetKey to the
+        AssetsDefinition object which produces it. If a given asset is monitored by this sensor, but
+        is not produced within the same code location as this sensor, then the value will be None.
+        """
         return self._assets_by_key
 
     @public
     @property
     def asset_keys(self) -> Sequence[AssetKey]:
+        """Sequence[AssetKey]: The asset keys which are monitored by this sensor."""
         return self._monitored_asset_keys
 
 
 class MultiAssetSensorCursorAdvances:
     _advanced_record_ids_by_key: Dict[AssetKey, Set[int]]
     _partition_key_by_record_id: Dict[int, Optional[str]]
     advance_all_cursors_called: bool
@@ -1076,15 +1081,15 @@
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]): The job
             object to target with this sensor.
         jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]]):
             (experimental) A list of jobs to be executed when the sensor fires.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         request_assets (Optional[AssetSelection]): (Experimental) an asset selection to launch a run
             for if the sensor condition is met. This can be provided instead of specifying a job.
     """
 
     def __init__(
         self,
         name: str,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/multi_dimensional_partitions.py` & `dagster-1.4.0/dagster/_core/definitions/multi_dimensional_partitions.py`

 * *Files 4% similar despite different names*

```diff
@@ -273,14 +273,28 @@
 
     @public
     def get_partition_keys(
         self,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> Sequence[MultiPartitionKey]:
+        """Returns a list of MultiPartitionKeys representing the partition keys of the
+        PartitionsDefinition.
+
+        Args:
+            current_time (Optional[datetime]): A datetime object representing the current time, only
+                applicable to time-based partition dimensions.
+            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore
+                object that is responsible for fetching dynamic partitions. Required when a
+                dimension is a DynamicPartitionsDefinition with a name defined. Users can pass the
+                DagsterInstance fetched via `context.instance` to this argument.
+
+        Returns:
+            Sequence[MultiPartitionKey]
+        """
         partition_key_sequences = [
             partition_dim.partitions_def.get_partition_keys(
                 current_time=current_time, dynamic_partitions_store=dynamic_partitions_store
             )
             for partition_dim in self._partitions_defs
         ]
 
@@ -335,16 +349,16 @@
         )
 
     def __str__(self) -> str:
         dimension_1 = self._partitions_defs[0]
         dimension_2 = self._partitions_defs[1]
         partition_str = (
             "Multi-partitioned, with dimensions: \n"
-            f"{dimension_1.name.capitalize()}: {str(dimension_1.partitions_def)} \n"
-            f"{dimension_2.name.capitalize()}: {str(dimension_2.partitions_def)}"
+            f"{dimension_1.name.capitalize()}: {dimension_1.partitions_def} \n"
+            f"{dimension_2.name.capitalize()}: {dimension_2.partitions_def}"
         )
         return partition_str
 
     def __repr__(self) -> str:
         return f"{type(self).__name__}(dimensions={[str(dim) for dim in self.partitions_defs]}"
 
     def get_partition_key_from_str(self, partition_key_str: str) -> MultiPartitionKey:
@@ -473,15 +487,15 @@
     def get_num_partitions(
         self,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> int:
         # Static partitions definitions can contain duplicate keys (will throw error in 1.3.0)
         # In the meantime, relying on get_num_partitions to handle duplicates to display
-        # correct counts in Dagit
+        # correct counts in the Dagster UI.
         dimension_counts = [
             dim.partitions_def.get_num_partitions(
                 current_time=current_time, dynamic_partitions_store=dynamic_partitions_store
             )
             for dim in self.partitions_defs
         ]
         return reduce(lambda x, y: x * y, dimension_counts, 1)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/node_container.py` & `dagster-1.4.0/dagster/_core/definitions/node_container.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/node_definition.py` & `dagster-1.4.0/dagster/_core/definitions/node_definition.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/observe.py` & `dagster-1.4.0/dagster/_core/definitions/observe.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/op_definition.py` & `dagster-1.4.0/dagster/_core/definitions/op_definition.py`

 * *Files 8% similar despite different names*

```diff
@@ -211,70 +211,87 @@
     @property
     def is_graph_job_op_node(self) -> bool:
         return True
 
     @public
     @property
     def name(self) -> str:
+        """str: The name of this op."""
         return super(OpDefinition, self).name
 
     @public
     @property
     def ins(self) -> Mapping[str, In]:
+        """Mapping[str, In]: A mapping from input name to the In object that represents that input.
+        """
         return {input_def.name: In.from_definition(input_def) for input_def in self.input_defs}
 
     @public
     @property
     def outs(self) -> Mapping[str, Out]:
+        """Mapping[str, Out]: A mapping from output name to the Out object that represents that output.
+        """
         return {output_def.name: Out.from_definition(output_def) for output_def in self.output_defs}
 
     @property
     def compute_fn(self) -> Union[Callable[..., Any], "DecoratedOpFunction"]:
         return self._compute_fn
 
     @public
     @property
     def config_schema(self) -> IDefinitionConfigSchema:
+        """IDefinitionConfigSchema: The config schema for this op."""
         return self._config_schema
 
     @public
     @property
     def required_resource_keys(self) -> AbstractSet[str]:
+        """AbstractSet[str]: A set of keys for resources that must be provided to this OpDefinition.
+        """
         return frozenset(self._required_resource_keys)
 
     @public
     @property
     def version(self) -> Optional[str]:
+        """[DEPRECATED] str: Version of the code encapsulated by the op. If set, this is used as a
+        default code version for all outputs.
+        """
         deprecation_warning("`version` property on OpDefinition", "2.0")
         return self._version
 
     @public
     @property
     def retry_policy(self) -> Optional[RetryPolicy]:
+        """Optional[RetryPolicy]: The RetryPolicy for this op."""
         return self._retry_policy
 
     @public
     @property
     def tags(self) -> Mapping[str, str]:
+        """Mapping[str, str]: The tags for this op."""
         return super(OpDefinition, self).tags
 
     @public
     def alias(self, name: str) -> "PendingNodeInvocation":
+        """Creates a copy of this op with the given name."""
         return super(OpDefinition, self).alias(name)
 
     @public
     def tag(self, tags: Optional[Mapping[str, str]]) -> "PendingNodeInvocation":
+        """Creates a copy of this op with the given tags."""
         return super(OpDefinition, self).tag(tags)
 
     @public
     def with_hooks(self, hook_defs: AbstractSet[HookDefinition]) -> "PendingNodeInvocation":
+        """Creates a copy of this op with the given hook definitions."""
         return super(OpDefinition, self).with_hooks(hook_defs)
 
     @public
     def with_retry_policy(self, retry_policy: RetryPolicy) -> "PendingNodeInvocation":
+        """Creates a copy of this op with the given retry policy."""
         return super(OpDefinition, self).with_retry_policy(retry_policy)
 
     def is_from_decorator(self) -> bool:
         from .decorators.op_decorator import DecoratedOpFunction
 
         return isinstance(self._compute_fn, DecoratedOpFunction)
 
@@ -310,15 +327,14 @@
     ) -> Sequence[InputDefinition]:
         handle = cast(NodeHandle, check.inst_param(handle, "handle", NodeHandle))
         unresolveable_input_defs = []
         for input_def in self.input_defs:
             if (
                 not input_def.dagster_type.loader
                 and not input_def.dagster_type.kind == DagsterTypeKind.NOTHING
-                and not input_def.root_manager_key
                 and not input_def.has_default_value
                 and not input_def.input_manager_key
             ):
                 input_asset_key = asset_layer.asset_key_for_input(handle, input_def.name)
                 # If input_asset_key is present, this input can be resolved
                 # by a source asset, so input does not need to be resolved
                 # at the top level.
@@ -388,22 +404,15 @@
             handle, asset_layer = outer_context
         node_description = f"{self.node_type_str} '{handle or self.name}'"
         for resource_key in sorted(list(self.required_resource_keys)):
             yield OpDefinitionResourceRequirement(
                 key=resource_key, node_description=node_description
             )
         for input_def in self.input_defs:
-            if input_def.root_manager_key:
-                yield InputManagerRequirement(
-                    key=input_def.root_manager_key,
-                    node_description=node_description,
-                    input_name=input_def.name,
-                    root_input=True,
-                )
-            elif input_def.input_manager_key:
+            if input_def.input_manager_key:
                 yield InputManagerRequirement(
                     key=input_def.input_manager_key,
                     node_description=node_description,
                     input_name=input_def.name,
                     root_input=False,
                 )
             elif asset_layer and handle:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/op_invocation.py` & `dagster-1.4.0/dagster/_core/definitions/op_invocation.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/op_selection.py` & `dagster-1.4.0/dagster/_core/definitions/op_selection.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/output.py` & `dagster-1.4.0/dagster/_core/definitions/output.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/partition.py` & `dagster-1.4.0/dagster/_core/definitions/partition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 import copy
 import hashlib
 import json
 from abc import ABC, abstractmethod
+from collections import defaultdict
 from datetime import (
     datetime,
     timedelta,
 )
 from enum import Enum
 from typing import (
     Any,
     Callable,
+    Dict,
     Generic,
     Iterable,
     Mapping,
     NamedTuple,
     Optional,
     Sequence,
     Set,
@@ -35,15 +37,14 @@
 from dagster._core.instance import DagsterInstance, DynamicPartitionsStore
 from dagster._core.storage.tags import PARTITION_NAME_TAG, PARTITION_SET_TAG
 from dagster._serdes import whitelist_for_serdes
 from dagster._utils import xor
 from dagster._utils.backcompat import (
     canonicalize_backcompat_args,
     deprecation_warning,
-    experimental_arg_warning,
 )
 from dagster._utils.cached_method import cached_method
 
 from ..errors import (
     DagsterInvalidDefinitionError,
     DagsterInvalidDeserializationVersionError,
     DagsterInvalidInvocationError,
@@ -59,17 +60,17 @@
 T_PartitionsDefinition = TypeVar(
     "T_PartitionsDefinition",
     bound="PartitionsDefinition",
     default="PartitionsDefinition",
     covariant=True,
 )
 
-# Dagit selects partition ranges following the format '2022-01-13...2022-01-14'
+# In the Dagster UI users can select partition ranges following the format '2022-01-13...2022-01-14'
 # "..." is an invalid substring in partition keys
-# The other escape characters are characters that may not display in Dagit
+# The other escape characters are characters that may not display in the Dagster UI.
 INVALID_PARTITION_SUBSTRINGS = ["...", "\a", "\b", "\f", "\n", "\r", "\t", "\v", "\0"]
 
 
 @deprecated
 class Partition(Generic[T_cov]):
     """A Partition represents a single slice of the entire set of a job's possible work. It consists
     of a value, which is an object that represents that partition, and an optional name, which is
@@ -145,14 +146,27 @@
     @abstractmethod
     @public
     def get_partition_keys(
         self,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> Sequence[T_str]:
+        """Returns a list of strings representing the partition keys of the PartitionsDefinition.
+
+        Args:
+            current_time (Optional[datetime]): A datetime object representing the current time, only
+                applicable to time-based partitions definitions.
+            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore
+                object that is responsible for fetching dynamic partitions. Required when the
+                partitions definition is a DynamicPartitionsDefinition with a name defined. Users
+                can pass the DagsterInstance fetched via `context.instance` to this argument.
+
+        Returns:
+            Sequence[str]
+        """
         ...
 
     def __str__(self) -> str:
         joined_keys = ", ".join([f"'{key}'" for key in self.get_partition_keys()])
         return joined_keys
 
     def get_last_partition_key(
@@ -172,27 +186,35 @@
         return partition_keys[0] if partition_keys else None
 
     def get_partition_keys_in_range(
         self,
         partition_key_range: PartitionKeyRange,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> Sequence[T_str]:
-        partition_keys = self.get_partition_keys(dynamic_partitions_store=dynamic_partitions_store)
-
         keys_exist = {
-            partition_key_range.start: partition_key_range.start in partition_keys,
-            partition_key_range.end: partition_key_range.end in partition_keys,
+            partition_key_range.start: self.has_partition_key(
+                partition_key_range.start, dynamic_partitions_store=dynamic_partitions_store
+            ),
+            partition_key_range.end: self.has_partition_key(
+                partition_key_range.end, dynamic_partitions_store=dynamic_partitions_store
+            ),
         }
         if not all(keys_exist.values()):
             raise DagsterInvalidInvocationError(
                 f"""Partition range {partition_key_range.start} to {partition_key_range.end} is
                 not a valid range. Nonexistent partition keys:
                 {list(key for key in keys_exist if keys_exist[key] is False)}"""
             )
 
+        # in the simple case, simply return the single key in the range
+        if partition_key_range.start == partition_key_range.end:
+            return [cast(T_str, partition_key_range.start)]
+
+        # defer this call as it is potentially expensive
+        partition_keys = self.get_partition_keys(dynamic_partitions_store=dynamic_partitions_store)
         return partition_keys[
             partition_keys.index(partition_key_range.start) : partition_keys.index(
                 partition_key_range.end
             )
             + 1
         ]
 
@@ -280,14 +302,26 @@
         ]
         if found_invalid_substrs:
             raise DagsterInvalidDefinitionError(
                 f"{found_invalid_substrs} are invalid substrings in a partition key"
             )
 
 
+def raise_error_on_duplicate_partition_keys(partition_keys: Sequence[str]) -> None:
+    counts: Dict[str, int] = defaultdict(lambda: 0)
+    for partition_key in partition_keys:
+        counts[partition_key] += 1
+        found_duplicates = [key for key in counts.keys() if counts[key] > 1]
+        if found_duplicates:
+            raise DagsterInvalidDefinitionError(
+                "Partition keys must be unique. Duplicate instances of partition keys:"
+                f" {found_duplicates}."
+            )
+
+
 class StaticPartitionsDefinition(PartitionsDefinition[str]):
     """A statically-defined set of partitions.
 
     Example:
         .. code-block:: python
 
             from dagster import StaticPartitionsDefinition, asset
@@ -300,25 +334,38 @@
             def ml_model_for_each_ocean():
                 ...
     """
 
     def __init__(self, partition_keys: Sequence[str]):
         check.sequence_param(partition_keys, "partition_keys", of_type=str)
 
-        # TODO 1.3.0 enforce that partition keys are unique
         raise_error_on_invalid_partition_key_substring(partition_keys)
+        raise_error_on_duplicate_partition_keys(partition_keys)
 
         self._partition_keys = partition_keys
 
     @public
     def get_partition_keys(
         self,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> Sequence[str]:
+        """Returns a list of strings representing the partition keys of the PartitionsDefinition.
+
+        Args:
+            current_time (Optional[datetime]): A datetime object representing the current time, only
+                applicable to time-based partitions definitions.
+            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore
+                object that is responsible for fetching dynamic partitions. Only applicable to
+                DynamicPartitionsDefinitions.
+
+        Returns:
+            Sequence[str]
+
+        """
         return self._partition_keys
 
     def __hash__(self):
         return hash(self.__repr__())
 
     def __eq__(self, other) -> bool:
         return isinstance(other, StaticPartitionsDefinition) and (
@@ -331,15 +378,15 @@
     def get_num_partitions(
         self,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> int:
         # We don't currently throw an error when a duplicate partition key is defined
         # in a static partitions definition, though we will at 1.3.0.
-        # This ensures that partition counts are correct in Dagit.
+        # This ensures that partition counts are correct in the Dagster UI.
         return len(set(self.get_partition_keys(current_time, dynamic_partitions_store)))
 
 
 class CachingDynamicPartitionsLoader(DynamicPartitionsStore):
     """A batch loader that caches the partition keys for a given dynamic partitions definition,
     to avoid repeated calls to the database for the same partitions definition.
     """
@@ -378,15 +425,15 @@
     This is useful for cases where the set of partitions is not known at definition time,
     but is instead determined at runtime.
 
     Partitions can be added and removed using `instance.add_dynamic_partitions` and
     `instance.delete_dynamic_partition` methods.
 
     Args:
-        name (Optional[str]): (Experimental) The name of the partitions definition.
+        name (Optional[str]): The name of the partitions definition.
         partition_fn (Optional[Callable[[Optional[datetime]], Union[Sequence[Partition], Sequence[str]]]]):
             A function that returns the current set of partitions. This argument is deprecated and
             will be removed in 2.0.0.
 
     Examples:
         .. code-block:: python
 
@@ -406,17 +453,14 @@
             Callable[[Optional[datetime]], Union[Sequence[Partition], Sequence[str]]]
         ] = None,
         name: Optional[str] = None,
     ):
         partition_fn = check.opt_callable_param(partition_fn, "partition_fn")
         name = check.opt_str_param(name, "name")
 
-        if name:
-            experimental_arg_warning("name", "DynamicPartitionsDefinition.__new__")
-
         if partition_fn:
             deprecation_warning(
                 "partition_fn", "2.0.0", "Provide partition definition name instead."
             )
 
         if partition_fn is None and name is None:
             raise DagsterInvalidDefinitionError(
@@ -459,30 +503,44 @@
 
     @public
     def get_partition_keys(
         self,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> Sequence[str]:
+        """Returns a list of strings representing the partition keys of the
+        PartitionsDefinition.
+
+        Args:
+            current_time (Optional[datetime]): A datetime object representing the current time, only
+                applicable to time-based partitions definitions.
+            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore
+                object that is responsible for fetching dynamic partitions. Required when the
+                partitions definition is a DynamicPartitionsDefinition with a name defined. Users
+                can pass the DagsterInstance fetched via `context.instance` to this argument.
+
+        Returns:
+            Sequence[str]
+        """
         if self.partition_fn:
             partitions = self.partition_fn(current_time)
             if all(isinstance(partition, Partition) for partition in partitions):
                 return [partition.name for partition in partitions]  # type: ignore  # (illegible conditional)
             else:
                 return partitions  # type: ignore  # (illegible conditional)
         else:
             check.opt_inst_param(
                 dynamic_partitions_store, "dynamic_partitions_store", DynamicPartitionsStore
             )
 
             if dynamic_partitions_store is None:
                 check.failed(
                     "The instance is not available to load partitions. You may be seeing this error"
-                    " when using dynamic partitions with a version of dagit or dagster-cloud that"
-                    " is older than 1.1.18."
+                    " when using dynamic partitions with a version of dagster-webserver or"
+                    " dagster-cloud that is older than 1.1.18."
                 )
 
             return dynamic_partitions_store.get_dynamic_partitions(
                 partitions_def_name=self._validated_name()
             )
 
     def has_partition_key(
@@ -493,16 +551,16 @@
     ) -> bool:
         if self.partition_fn:
             return partition_key in self.get_partition_keys(current_time)
         else:
             if dynamic_partitions_store is None:
                 check.failed(
                     "The instance is not available to load partitions. You may be seeing this error"
-                    " when using dynamic partitions with a version of dagit or dagster-cloud that"
-                    " is older than 1.1.18."
+                    " when using dynamic partitions with a version of dagster-webserver or"
+                    " dagster-cloud that is older than 1.1.18."
                 )
 
             return dynamic_partitions_store.has_dynamic_partition(
                 partitions_def_name=self._validated_name(), partition_key=partition_key
             )
 
     def build_add_request(self, partition_keys: Sequence[str]) -> AddDynamicPartitionsRequest:
@@ -570,46 +628,72 @@
         )
 
     @public
     @property
     def partitions_def(
         self,
     ) -> T_PartitionsDefinition:
+        """T_PartitionsDefinition: The partitions definition associated with this PartitionedConfig.
+        """
         return self._partitions
 
     @deprecated
     @public
     @property
     def run_config_for_partition_fn(
         self,
     ) -> Optional[Callable[[Partition], Mapping[str, Any]]]:
+        """Optional[Callable[[Partition], Mapping[str, Any]]]: A function that accepts a partition
+        and returns a dictionary representing the config to attach to runs for that partition.
+        Deprecated as of 1.3.3.
+        """
         return self._run_config_for_partition_fn
 
     @public
     @property
     def run_config_for_partition_key_fn(
         self,
     ) -> Optional[Callable[[str], Mapping[str, Any]]]:
-        return self._run_config_for_partition_key_fn
+        """Optional[Callable[[str], Mapping[str, Any]]]: A function that accepts a partition key
+        and returns a dictionary representing the config to attach to runs for that partition.
+        """
 
     @deprecated
     @public
     @property
     def tags_for_partition_fn(self) -> Optional[Callable[[Partition], Mapping[str, str]]]:
+        """Optional[Callable[[Partition], Mapping[str, str]]]: A function that
+        accepts a partition and returns a dictionary of tags to attach to runs for
+        that partition. Deprecated as of 1.3.3.
+        """
         return self._tags_for_partition_fn
 
     @public
     @property
     def tags_for_partition_key_fn(
         self,
     ) -> Optional[Callable[[str], Mapping[str, str]]]:
+        """Optional[Callable[[str], Mapping[str, str]]]: A function that
+        accepts a partition key and returns a dictionary of tags to attach to runs for
+        that partition.
+        """
         return self._tags_for_partition_key_fn
 
     @public
     def get_partition_keys(self, current_time: Optional[datetime] = None) -> Sequence[str]:
+        """Returns a list of partition keys, representing the full set of partitions that
+        config can be applied to.
+
+        Args:
+            current_time (Optional[datetime]): A datetime object representing the current time. Only
+                applicable to time-based partitions definitions.
+
+        Returns:
+            Sequence[str]
+        """
         return self.partitions_def.get_partition_keys(current_time)
 
     # Assumes partition key already validated
     def get_run_config_for_partition_key(
         self,
         partition_key: str,
     ) -> Mapping[str, Any]:
@@ -704,15 +788,15 @@
     """Creates a static partitioned config for a job.
 
     The provided partition_keys is a static list of strings identifying the set of partitions. The
     list of partitions is static, so while the run config returned by the decorated function may
     change over time, the list of valid partition keys does not.
 
     This has performance advantages over `dynamic_partitioned_config` in terms of loading different
-    partition views in Dagit.
+    partition views in the Dagster UI.
 
     The decorated function takes in a partition key and returns a valid run config for a particular
     target job.
 
     Args:
         partition_keys (Sequence[str]): A list of valid partition keys, which serve as the range of
             values that can be provided to the decorated run config function.
@@ -986,15 +1070,15 @@
 
         if cur_range_start is not None and cur_range_end is not None:
             result.append(PartitionKeyRange(cur_range_start, cur_range_end))
 
         return result
 
     def with_partition_keys(
-        self, partition_keys: Iterable[str]
+        self, partition_keys: Iterable[T_str]
     ) -> "DefaultPartitionsSubset[T_str]":
         return DefaultPartitionsSubset(
             self._partitions_def,
             self._subset | set(partition_keys),
         )
 
     def serialize(self) -> str:
@@ -1039,16 +1123,16 @@
     @property
     def partitions_def(self) -> PartitionsDefinition[T_str]:
         return self._partitions_def
 
     def __eq__(self, other: object) -> bool:
         return (
             isinstance(other, DefaultPartitionsSubset)
-            and self._partitions_def == other._partitions_def  # noqa: SLF001
-            and self._subset == other._subset  # noqa: SLF001
+            and self._partitions_def == other._partitions_def
+            and self._subset == other._subset
         )
 
     def __len__(self) -> int:
         return len(self._subset)
 
     def __contains__(self, value) -> bool:
         return value in self._subset
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/partition_mapping.py` & `dagster-1.4.0/dagster/_core/definitions/partition_mapping.py`

 * *Files 11% similar despite different names*

```diff
@@ -34,309 +34,256 @@
 from dagster._core.definitions.time_window_partitions import TimeWindowPartitionsDefinition
 from dagster._core.instance import DynamicPartitionsStore
 from dagster._serdes import whitelist_for_serdes
 from dagster._utils.backcompat import ExperimentalWarning
 from dagster._utils.cached_method import cached_method
 
 
+class UpstreamPartitionsResult(NamedTuple):
+    """Represents the result of mapping a PartitionsSubset to the corresponding
+    partitions in another PartitionsDefinition.
+
+    partitions_subset (PartitionsSubset): The resulting partitions subset that was
+        mapped to. Only contains partitions for existent partitions, filtering out nonexistent partitions.
+    required_but_nonexistent_partition_keys (Sequence[str]): A list containing invalid partition keys in to_partitions_def
+        that partitions in from_partitions_subset were mapped to.
+    """
+
+    partitions_subset: PartitionsSubset
+    required_but_nonexistent_partition_keys: Sequence[str]
+
+
 class PartitionMapping(ABC):
     """Defines a correspondence between the partitions in an asset and the partitions in an asset
     that it depends on.
 
     Overriding PartitionMapping outside of Dagster is not supported. The abstract methods of this
     class may change at any time.
     """
 
     @public
     @abstractmethod
-    def get_upstream_partitions_for_partition_range(
+    def get_downstream_partitions_for_partitions(
         self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        """Returns the range of partition keys in the upstream asset that include data necessary
-        to compute the contents of the given partition key range in the downstream asset.
+        upstream_partitions_subset: PartitionsSubset,
+        downstream_partitions_def: PartitionsDefinition,
+        current_time: Optional[datetime] = None,
+        dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
+    ) -> PartitionsSubset:
+        """Returns the subset of partition keys in the downstream asset that use the data in the given
+        partition key subset of the upstream asset.
 
         Args:
-            downstream_partition_key_range (PartitionKeyRange): The range of partition keys in the
-                downstream asset.
+            upstream_partitions_subset (Union[PartitionKeyRange, PartitionsSubset]): The
+                subset of partition keys in the upstream asset.
             downstream_partitions_def (PartitionsDefinition): The partitions definition for the
                 downstream asset.
-            upstream_partitions_def (PartitionsDefinition): The partitions definition for the
-                upstream asset.
         """
 
     @public
     @abstractmethod
-    def get_downstream_partitions_for_partition_range(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
+        downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        """Returns the range of partition keys in the downstream asset that use the data in the given
-        partition key range of the upstream asset.
-
-        Args:
-            upstream_partition_key_range (PartitionKeyRange): The range of partition keys in the
-                upstream asset.
-            downstream_partitions_def (PartitionsDefinition): The partitions definition for the
-                downstream asset.
-            upstream_partitions_def (PartitionsDefinition): The partitions definition for the
-                upstream asset.
+        current_time: Optional[datetime] = None,
+        dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
+    ) -> UpstreamPartitionsResult:
+        """Returns a UpstreamPartitionsResult object containing the partition keys the downstream
+        partitions subset was mapped to in the upstream partitions definition.
+
+        Valid upstream partitions will be included in UpstreamPartitionsResult.partitions_subset.
+        Invalid upstream partitions will be included in UpstreamPartitionsResult.required_but_nonexistent_partition_keys.
+
+        For example, if an upstream asset is time-partitioned and starts in June 2023, and the
+        downstream asset is time-partitioned and starts in May 2023, this function would return a
+        UpstreamPartitionsResult(PartitionsSubset("2023-06-01"), required_but_nonexistent_partition_keys=["2023-05-01"])
+        when downstream_partitions_subset contains 2023-05-01 and 2023-06-01.
         """
 
-    @public
-    def get_upstream_partitions_for_partitions(
+
+@whitelist_for_serdes
+class IdentityPartitionMapping(PartitionMapping, NamedTuple("_IdentityPartitionMapping", [])):
+    """Expects that the upstream and downstream assets are partitioned in the same way, and maps
+    partitions in the downstream asset to the same partition in the upstream asset.
+    """
+
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
-        """Returns the subset of partition keys in the upstream asset that include data necessary
-        to compute the contents of the given partition key subset in the downstream asset.
-
-        Args:
-            downstream_partitions_subset (Optional[PartitionsSubset]):
-                The subset of partition keys in the downstream asset.
-            upstream_partitions_def (PartitionsDefinition): The partitions definition for the
-                upstream asset.
-        """
-        upstream_key_ranges = []
+    ) -> UpstreamPartitionsResult:
         if downstream_partitions_subset is None:
-            upstream_key_ranges.append(
-                self.get_upstream_partitions_for_partition_range(
-                    None, None, upstream_partitions_def
-                )
-            )
-        else:
-            for key_range in downstream_partitions_subset.get_partition_key_ranges(
-                dynamic_partitions_store=dynamic_partitions_store
-            ):
-                upstream_key_ranges.append(
-                    self.get_upstream_partitions_for_partition_range(
-                        key_range,
-                        downstream_partitions_subset.partitions_def,
-                        upstream_partitions_def,
-                    )
-                )
+            check.failed("downstream asset is not partitioned")
+
+        if downstream_partitions_subset.partitions_def == upstream_partitions_def:
+            return UpstreamPartitionsResult(downstream_partitions_subset, [])
 
-        return upstream_partitions_def.empty_subset().with_partition_keys(
-            pk
-            for upstream_key_range in upstream_key_ranges
-            for pk in upstream_partitions_def.get_partition_keys_in_range(
-                upstream_key_range, dynamic_partitions_store=dynamic_partitions_store
+        upstream_partition_keys = set(
+            upstream_partitions_def.get_partition_keys(
+                dynamic_partitions_store=dynamic_partitions_store
             )
         )
+        downstream_partition_keys = set(downstream_partitions_subset.get_partition_keys())
+
+        return UpstreamPartitionsResult(
+            upstream_partitions_def.subset_with_partition_keys(
+                list(upstream_partition_keys & downstream_partition_keys)
+            ),
+            list(downstream_partition_keys - upstream_partition_keys),
+        )
 
-    @public
     def get_downstream_partitions_for_partitions(
         self,
         upstream_partitions_subset: PartitionsSubset,
         downstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> PartitionsSubset:
-        """Returns the subset of partition keys in the downstream asset that use the data in the given
-        partition key subset of the upstream asset.
+        if upstream_partitions_subset is None:
+            check.failed("upstream asset is not partitioned")
 
-        Args:
-            upstream_partitions_subset (Union[PartitionKeyRange, PartitionsSubset]): The
-                subset of partition keys in the upstream asset.
-            downstream_partitions_def (PartitionsDefinition): The partitions definition for the
-                downstream asset.
-        """
-        downstream_key_ranges = []
-        for key_range in upstream_partitions_subset.get_partition_key_ranges(
-            dynamic_partitions_store=dynamic_partitions_store
-        ):
-            downstream_key_ranges.append(
-                self.get_downstream_partitions_for_partition_range(
-                    key_range,
-                    downstream_partitions_def,
-                    upstream_partitions_subset.partitions_def,
-                )
-            )
+        if upstream_partitions_subset.partitions_def == downstream_partitions_def:
+            return upstream_partitions_subset
 
-        return downstream_partitions_def.empty_subset().with_partition_keys(
-            pk
-            for upstream_key_range in downstream_key_ranges
-            for pk in downstream_partitions_def.get_partition_keys_in_range(
-                upstream_key_range, dynamic_partitions_store=dynamic_partitions_store
+        upstream_partition_keys = set(upstream_partitions_subset.get_partition_keys())
+        downstream_partition_keys = set(
+            downstream_partitions_def.get_partition_keys(
+                dynamic_partitions_store=dynamic_partitions_store
             )
         )
 
-
-@whitelist_for_serdes
-class IdentityPartitionMapping(PartitionMapping, NamedTuple("_IdentityPartitionMapping", [])):
-    """Expects that the upstream and downstream assets are partitioned in the same way, and maps
-    partitions in the downstream asset to the same partition in the upstream asset.
-    """
-
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        if downstream_partitions_def is None or downstream_partition_key_range is None:
-            check.failed("downstream asset is not partitioned")
-
-        return downstream_partition_key_range
-
-    def get_downstream_partitions_for_partition_range(
-        self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        return upstream_partition_key_range
+        return downstream_partitions_def.empty_subset().with_partition_keys(
+            list(downstream_partition_keys & upstream_partition_keys)
+        )
 
 
 @whitelist_for_serdes
 class AllPartitionMapping(PartitionMapping, NamedTuple("_AllPartitionMapping", [])):
     """Maps every partition in the downstream asset to every partition in the upstream asset.
 
     Commonly used in the case when the downstream asset is not partitioned, in which the entire
     downstream asset depends on all partitions of the usptream asset.
     """
 
-    def get_upstream_partitions_for_partitions(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
+    ) -> UpstreamPartitionsResult:
         first = upstream_partitions_def.get_first_partition_key(
             current_time=None, dynamic_partitions_store=dynamic_partitions_store
         )
         last = upstream_partitions_def.get_last_partition_key(
             current_time=None, dynamic_partitions_store=dynamic_partitions_store
         )
 
-        empty_subset = upstream_partitions_def.empty_subset()
+        upstream_subset = upstream_partitions_def.empty_subset()
         if first is not None and last is not None:
-            return empty_subset.with_partition_key_range(
-                PartitionKeyRange(first, last), dynamic_partitions_store=dynamic_partitions_store
+            upstream_subset = upstream_subset.with_partition_key_range(
+                PartitionKeyRange(first, last),
+                dynamic_partitions_store=dynamic_partitions_store,
             )
-        else:
-            return empty_subset
 
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
+        return UpstreamPartitionsResult(upstream_subset, [])
 
-    def get_downstream_partitions_for_partition_range(
+    def get_downstream_partitions_for_partitions(
         self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
+        upstream_partitions_subset: PartitionsSubset,
+        downstream_partitions_def: PartitionsDefinition,
+        current_time: Optional[datetime] = None,
+        dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
+    ) -> PartitionsSubset:
         raise NotImplementedError()
 
 
 @whitelist_for_serdes
 class LastPartitionMapping(PartitionMapping, NamedTuple("_LastPartitionMapping", [])):
     """Maps all dependencies to the last partition in the upstream asset.
 
     Commonly used in the case when the downstream asset is not partitioned, in which the entire
     downstream asset depends on the last partition of the upstream asset.
     """
 
-    def get_upstream_partitions_for_partitions(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
+    ) -> UpstreamPartitionsResult:
         last = upstream_partitions_def.get_last_partition_key(
             current_time=None, dynamic_partitions_store=dynamic_partitions_store
         )
 
-        empty_subset = upstream_partitions_def.empty_subset()
+        upstream_subset = upstream_partitions_def.empty_subset()
         if last is not None:
-            return empty_subset.with_partition_keys([last])
-        else:
-            return empty_subset
+            upstream_subset = upstream_subset.with_partition_keys([last])
 
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
+        return UpstreamPartitionsResult(upstream_subset, [])
 
-    def get_downstream_partitions_for_partition_range(
+    def get_downstream_partitions_for_partitions(
         self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
+        upstream_partitions_subset: PartitionsSubset,
+        downstream_partitions_def: PartitionsDefinition,
+        current_time: Optional[datetime] = None,
+        dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
+    ) -> PartitionsSubset:
         raise NotImplementedError()
 
 
 @whitelist_for_serdes
 class SpecificPartitionsPartitionMapping(
     PartitionMapping,
     NamedTuple(
         "_SpecificPartitionsPartitionMapping", [("partition_keys", PublicAttr[Sequence[str]])]
     ),
 ):
     """Maps to a specific subset of partitions in the upstream asset.
 
     Example:
-    .. code-block:: python
-
-        from dagster import SpecificPartitionsPartitionMapping, StaticPartitionsDefinition, asset
+        .. code-block:: python
 
-        @asset(partitions_def=StaticPartitionsDefinition(["a", "b", "c"]))
-        def upstream():
-            ...
+            from dagster import SpecificPartitionsPartitionMapping, StaticPartitionsDefinition, asset
 
-        @asset(
-            ins={
-                "upstream": AssetIn(partition_mapping=SpecificPartitionsPartitionMapping(["a"]))
-            }
-        )
-        def a_downstream(upstream):
-            ...
+            @asset(partitions_def=StaticPartitionsDefinition(["a", "b", "c"]))
+            def upstream():
+                ...
+
+            @asset(
+                ins={
+                    "upstream": AssetIn(partition_mapping=SpecificPartitionsPartitionMapping(["a"]))
+                }
+            )
+            def a_downstream(upstream):
+                ...
     """
 
-    def get_upstream_partitions_for_partitions(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
-        return upstream_partitions_def.subset_with_partition_keys(self.partition_keys)
-
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
+    ) -> UpstreamPartitionsResult:
+        return UpstreamPartitionsResult(
+            upstream_partitions_def.subset_with_partition_keys(self.partition_keys), []
+        )
 
-    def get_downstream_partitions_for_partition_range(
+    def get_downstream_partitions_for_partitions(
         self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
+        upstream_partitions_subset: PartitionsSubset,
+        downstream_partitions_def: PartitionsDefinition,
+        current_time: Optional[datetime] = None,
+        dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
+    ) -> PartitionsSubset:
         raise NotImplementedError()
 
 
 @experimental
 @whitelist_for_serdes
 class MultiToSingleDimensionPartitionMapping(
     PartitionMapping,
@@ -434,42 +381,26 @@
                     "The single dimension partitions definition does not have the same partitions"
                     f" definition as dimension {matching_dimension_def.name}"
                 )
             partition_dimension_name = self.partition_dimension_name
 
         return (upstream_partitions_def, downstream_partitions_def, partition_dimension_name)
 
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
-
-    def get_downstream_partitions_for_partition_range(
-        self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
-
     def _get_matching_multipartition_keys_for_single_dim_subset(
         self,
         partitions_subset: PartitionsSubset,
         multipartitions_def: MultiPartitionsDefinition,
         partition_dimension_name: str,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> Sequence[str]:
         matching_keys = []
         for key in multipartitions_def.get_partition_keys(
             current_time=None, dynamic_partitions_store=dynamic_partitions_store
         ):
-            key = cast(MultiPartitionKey, key)  # noqa: PLW2901
+            key = cast(MultiPartitionKey, key)
             if (
                 key.keys_by_dimension[partition_dimension_name]
                 in partitions_subset.get_partition_keys()
             ):
                 matching_keys.append(key)
         return matching_keys
 
@@ -481,50 +412,56 @@
         upstream_partitions = set()
         for partition_key in partitions_subset.get_partition_keys():
             if not isinstance(partition_key, MultiPartitionKey):
                 check.failed("Partition keys in subset must be MultiPartitionKeys")
             upstream_partitions.add(partition_key.keys_by_dimension[partition_dimension_name])
         return upstream_partitions
 
-    def get_upstream_partitions_for_partitions(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
+    ) -> UpstreamPartitionsResult:
         if downstream_partitions_subset is None:
             check.failed("downstream asset is not partitioned")
 
         (
             upstream_partitions_def,
             _,
             partition_dimension_name,
         ) = self._check_partitions_defs_and_get_partition_dimension_name(
             upstream_partitions_def, downstream_partitions_subset.partitions_def
         )
 
         if isinstance(upstream_partitions_def, MultiPartitionsDefinition):
             # upstream partitions def is multipartitioned
             # downstream partitions def has single dimension
-            return upstream_partitions_def.empty_subset().with_partition_keys(
-                self._get_matching_multipartition_keys_for_single_dim_subset(
-                    downstream_partitions_subset,
-                    cast(MultiPartitionsDefinition, upstream_partitions_def),
-                    partition_dimension_name,
-                    dynamic_partitions_store,
-                )
+            return UpstreamPartitionsResult(
+                upstream_partitions_def.empty_subset().with_partition_keys(
+                    self._get_matching_multipartition_keys_for_single_dim_subset(
+                        downstream_partitions_subset,
+                        cast(MultiPartitionsDefinition, upstream_partitions_def),
+                        partition_dimension_name,
+                        dynamic_partitions_store,
+                    )
+                ),
+                [],
             )
         else:
             # upstream partitions_def has single dimension
             # downstream partitions def is multipartitioned
-            return upstream_partitions_def.empty_subset().with_partition_keys(
-                self._get_single_dim_keys_from_multipartitioned_subset(
-                    downstream_partitions_subset, partition_dimension_name
-                )
+            return UpstreamPartitionsResult(
+                upstream_partitions_def.empty_subset().with_partition_keys(
+                    self._get_single_dim_keys_from_multipartitioned_subset(
+                        downstream_partitions_subset, partition_dimension_name
+                    )
+                ),
+                [],
             )
 
     def get_downstream_partitions_for_partitions(
         self,
         upstream_partitions_subset: PartitionsSubset,
         downstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
@@ -608,14 +545,15 @@
 
     Accepts a mapping of upstream dimension name to downstream DimensionPartitionMapping, representing
     the explicit correspondence between the upstream and downstream MultiPartitions dimensions
     and the partition mapping used to calculate the downstream partitions.
 
     Examples:
         .. code-block:: python
+
             weekly_abc = MultiPartitionsDefinition(
                 {
                     "abc": StaticPartitionsDefinition(["a", "b", "c"]),
                     "weekly": WeeklyPartitionsDefinition("2023-01-01"),
                 }
             )
             daily_123 = MultiPartitionsDefinition(
@@ -640,14 +578,15 @@
 
     For upstream or downstream dimensions not explicitly defined in the mapping, Dagster will
     assume an `AllPartitionsMapping`, meaning that all upstream partitions in those dimensions
     will be mapped to all downstream partitions in those dimensions.
 
     Examples:
         .. code-block:: python
+
             weekly_abc = MultiPartitionsDefinition(
                 {
                     "abc": StaticPartitionsDefinition(["a", "b", "c"]),
                     "daily": DailyPartitionsDefinition("2023-01-01"),
                 }
             )
             daily_123 = MultiPartitionsDefinition(
@@ -688,30 +627,14 @@
                 downstream_mappings_by_upstream_dimension,
                 "downstream_mappings_by_upstream_dimension",
                 key_type=str,
                 value_type=DimensionPartitionMapping,
             ),
         )
 
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
-
-    def get_downstream_partitions_for_partition_range(
-        self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
-
     def _check_all_dimensions_accounted_for(
         self,
         upstream_partitions_def: PartitionsDefinition,
         downstream_partitions_def: PartitionsDefinition,
     ) -> None:
         if any(
             not isinstance(partitions_def, MultiPartitionsDefinition)
@@ -752,15 +675,15 @@
         self,
         a_partitions_def: MultiPartitionsDefinition,
         a_partition_keys: Sequence[MultiPartitionKey],
         b_partitions_def: MultiPartitionsDefinition,
         a_upstream_of_b: bool,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
         current_time: Optional[datetime] = None,
-    ) -> PartitionsSubset:
+    ) -> Union[UpstreamPartitionsResult, PartitionsSubset]:
         """Given two partitions definitions a_partitions_def and b_partitions_def that have a dependency
         relationship (a_upstream_of_b is True if a_partitions_def is upstream of b_partitions_def),
         and a_partition_keys, a list of partition keys in a_partitions_def, returns a list of
         partition keys in the partitions definition b_partitions_def that are
         dependencies of the partition keys in a_partition_keys.
         """
         a_partition_keys_by_dimension = defaultdict(set)
@@ -769,19 +692,16 @@
                 a_partition_keys_by_dimension[dimension_name].add(key)
 
         # Maps the dimension name and key of a partition in a_partitions_def to the list of
         # partition keys in b_partitions_def that are dependencies of that partition
         dep_b_keys_by_a_dim_and_key: Dict[str, Dict[str, List[str]]] = defaultdict(
             lambda: defaultdict(list)
         )
+        required_but_nonexistent_upstream_partitions = set()
 
-        a_dimension_partitions_defs_by_name = {
-            dimension.name: dimension.partitions_def
-            for dimension in a_partitions_def.partitions_defs
-        }
         b_dimension_partitions_def_by_name = {
             dimension.name: dimension.partitions_def
             for dimension in b_partitions_def.partitions_defs
         }
 
         if a_upstream_of_b:
             # a_partitions_def is upstream of b_partitions_def, so we need to map the
@@ -797,16 +717,20 @@
 
             for a_dim_name, keys in a_partition_keys_by_dimension.items():
                 if a_dim_name in a_dim_to_dependency_b_dim:
                     (
                         b_dim_name,
                         dimension_mapping,
                     ) = a_dim_to_dependency_b_dim[a_dim_name]
-                    a_dimension_partitions_def = a_dimension_partitions_defs_by_name[a_dim_name]
-                    b_dimension_partitions_def = b_dimension_partitions_def_by_name[b_dim_name]
+                    a_dimension_partitions_def = a_partitions_def.get_partitions_def_for_dimension(
+                        a_dim_name
+                    )
+                    b_dimension_partitions_def = b_partitions_def.get_partitions_def_for_dimension(
+                        b_dim_name
+                    )
                     for key in keys:
                         # if downstream dimension mapping exists, for a given key, get the list of
                         # downstream partition keys that are dependencies of that key
                         dep_b_keys_by_a_dim_and_key[a_dim_name][key] = list(
                             dimension_mapping.get_downstream_partitions_for_partitions(
                                 a_dimension_partitions_def.empty_subset().with_partition_keys(
                                     [key]
@@ -831,26 +755,36 @@
 
             for a_dim_name, keys in a_partition_keys_by_dimension.items():
                 if a_dim_name in a_dim_to_dependency_b_dim:
                     (
                         b_dim_name,
                         partition_mapping,
                     ) = a_dim_to_dependency_b_dim[a_dim_name]
-                    a_dimension_partitions_def = a_dimension_partitions_defs_by_name[a_dim_name]
-                    b_dimension_partitions_def = b_dimension_partitions_def_by_name[b_dim_name]
+                    a_dimension_partitions_def = a_partitions_def.get_partitions_def_for_dimension(
+                        a_dim_name
+                    )
+                    b_dimension_partitions_def = b_partitions_def.get_partitions_def_for_dimension(
+                        b_dim_name
+                    )
                     for key in keys:
-                        dep_b_keys_by_a_dim_and_key[a_dim_name][key] = list(
-                            partition_mapping.get_upstream_partitions_for_partitions(
+                        mapped_partitions_result = (
+                            partition_mapping.get_upstream_mapped_partitions_result_for_partitions(
                                 a_dimension_partitions_def.empty_subset().with_partition_keys(
                                     [key]
                                 ),
                                 b_dimension_partitions_def,
                                 current_time=current_time,
                                 dynamic_partitions_store=dynamic_partitions_store,
-                            ).get_partition_keys()
+                            )
+                        )
+                        dep_b_keys_by_a_dim_and_key[a_dim_name][key] = list(
+                            mapped_partitions_result.partitions_subset.get_partition_keys()
+                        )
+                        required_but_nonexistent_upstream_partitions.update(
+                            set(mapped_partitions_result.required_but_nonexistent_partition_keys)
                         )
 
         b_partition_keys = set()
 
         mapped_a_dim_names = a_dim_to_dependency_b_dim.keys()
         mapped_b_dim_names = [mapping[0] for mapping in a_dim_to_dependency_b_dim.values()]
         unmapped_b_dim_names = list(
@@ -875,41 +809,55 @@
                         {
                             (mapped_b_dim_names + unmapped_b_dim_names)[i]: key
                             for i, key in enumerate(b_key_values)
                         }
                     )
                 )
 
-        return b_partitions_def.empty_subset().with_partition_keys(b_partition_keys)
+        mapped_subset = b_partitions_def.empty_subset().with_partition_keys(b_partition_keys)
+        if a_upstream_of_b:
+            return mapped_subset
+        else:
+            return UpstreamPartitionsResult(
+                mapped_subset,
+                required_but_nonexistent_partition_keys=list(
+                    required_but_nonexistent_upstream_partitions
+                ),
+            )
 
-    def get_upstream_partitions_for_partitions(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
+    ) -> UpstreamPartitionsResult:
         if downstream_partitions_subset is None:
             check.failed("downstream asset is not partitioned")
 
         self._check_all_dimensions_accounted_for(
             upstream_partitions_def,
             downstream_partitions_subset.partitions_def,
         )
 
-        return self._get_dependency_partitions_subset(
+        result = self._get_dependency_partitions_subset(
             cast(MultiPartitionsDefinition, downstream_partitions_subset.partitions_def),
             list(
                 cast(Sequence[MultiPartitionKey], downstream_partitions_subset.get_partition_keys())
             ),
             cast(MultiPartitionsDefinition, upstream_partitions_def),
             a_upstream_of_b=False,
             dynamic_partitions_store=dynamic_partitions_store,
         )
 
+        if not isinstance(result, UpstreamPartitionsResult):
+            check.failed("Expected UpstreamPartitionsResult")
+
+        return result
+
     def get_downstream_partitions_for_partitions(
         self,
         upstream_partitions_subset: PartitionsSubset,
         downstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> PartitionsSubset:
@@ -917,24 +865,29 @@
             check.failed("upstream asset is not partitioned")
 
         self._check_all_dimensions_accounted_for(
             upstream_partitions_subset.partitions_def,
             downstream_partitions_def,
         )
 
-        return self._get_dependency_partitions_subset(
+        result = self._get_dependency_partitions_subset(
             cast(MultiPartitionsDefinition, upstream_partitions_subset.partitions_def),
             list(
                 cast(Sequence[MultiPartitionKey], upstream_partitions_subset.get_partition_keys())
             ),
             cast(MultiPartitionsDefinition, downstream_partitions_def),
             a_upstream_of_b=True,
             dynamic_partitions_store=dynamic_partitions_store,
         )
 
+        if isinstance(result, UpstreamPartitionsResult):
+            check.failed("Expected PartitionsSubset")
+
+        return result
+
 
 @whitelist_for_serdes
 class StaticPartitionMapping(
     PartitionMapping,
     NamedTuple(
         "_StaticPartitionMapping",
         [
@@ -1023,48 +976,32 @@
 
         downstream_subset = downstream_partitions_def.empty_subset()
         downstream_keys = set()
         for key in upstream_partitions_subset.get_partition_keys():
             downstream_keys.update(self._mapping[key])
         return downstream_subset.with_partition_keys(downstream_keys)
 
-    def get_upstream_partitions_for_partitions(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
+    ) -> UpstreamPartitionsResult:
         self._check_upstream(upstream_partitions_def=upstream_partitions_def)
 
         upstream_subset = upstream_partitions_def.empty_subset()
         if downstream_partitions_subset is None:
-            return upstream_subset
+            return UpstreamPartitionsResult(upstream_subset, [])
 
         upstream_keys = set()
         for key in downstream_partitions_subset.get_partition_keys():
             upstream_keys.update(self._inverse_mapping[key])
 
-        return upstream_subset.with_partition_keys(upstream_keys)
-
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
-
-    def get_downstream_partitions_for_partition_range(
-        self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
+        return UpstreamPartitionsResult(upstream_subset.with_partition_keys(upstream_keys), [])
 
 
 def _can_infer_single_to_multi_partition_mapping(
     upstream_partitions_def: PartitionsDefinition, downstream_partitions_def: PartitionsDefinition
 ) -> bool:
     multipartitions_defs = [
         partitions_def
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/partitioned_schedule.py` & `dagster-1.4.0/dagster/_core/definitions/partitioned_schedule.py`

 * *Files 1% similar despite different names*

```diff
@@ -42,15 +42,15 @@
         if partitions_def is None:
             check.failed(
                 f"Job '{resolved_job.name}' provided to build_schedule_from_partitioned_job must"
                 " contain partitioned assets or a partitions definition."
             )
 
         partitions_def = _check_valid_schedule_partitions_def(partitions_def)
-        time_partitions_def = get_time_partitions_def(partitions_def)
+        time_partitions_def = check.not_none(get_time_partitions_def(partitions_def))
 
         return ScheduleDefinition(
             job=resolved_job,
             name=self.name,
             execution_fn=_get_schedule_evaluation_fn(partitions_def, resolved_job, self.tags),
             execution_timezone=time_partitions_def.timezone,
             cron_schedule=time_partitions_def.get_cron_schedule(
@@ -141,15 +141,15 @@
         )
     else:
         partitions_def = job.partitions_def
         if partitions_def is None:
             check.failed("The provided job is not partitioned")
 
         partitions_def = _check_valid_schedule_partitions_def(partitions_def)
-        time_partitions_def = get_time_partitions_def(partitions_def)
+        time_partitions_def = check.not_none(get_time_partitions_def(partitions_def))
 
         return schedule(
             cron_schedule=time_partitions_def.get_cron_schedule(
                 minute_of_hour, hour_of_day, day_of_week, day_of_month
             ),
             job=job,
             default_status=default_status,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/policy.py` & `dagster-1.4.0/dagster/_core/definitions/policy.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/reconstruct.py` & `dagster-1.4.0/dagster/_core/definitions/reconstruct.py`

 * *Files 0% similar despite different names*

```diff
@@ -598,15 +598,15 @@
                 Definitions,
             ),
         )
         or _is_list_of_assets(definition)
     ):
         raise DagsterInvariantViolationError(
             "Loadable attributes must be either a JobDefinition, GraphDefinition, "
-            f"or RepositoryDefinition. Got {repr(definition)}."
+            f"or RepositoryDefinition. Got {definition!r}."
         )
     return definition
 
 
 def load_def_in_module(
     module_name: str, attribute: str, working_directory: Optional[str]
 ) -> LoadableDefinition:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/__init__.py` & `dagster-1.4.0/dagster/_core/definitions/repository_definition/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/caching_index.py` & `dagster-1.4.0/dagster/_core/definitions/repository_definition/caching_index.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/repository_data.py` & `dagster-1.4.0/dagster/_core/definitions/repository_definition/repository_data.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/repository_data_builder.py` & `dagster-1.4.0/dagster/_core/definitions/repository_definition/repository_data_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,15 +14,14 @@
 )
 
 import dagster._check as check
 from dagster._config.pythonic_config import (
     ConfigurableIOManagerFactoryResourceDefinition,
     ConfigurableResourceFactoryResourceDefinition,
     ResourceWithKeyMapping,
-    coerce_to_resource,
 )
 from dagster._core.definitions.asset_graph import AssetGraph
 from dagster._core.definitions.assets_job import (
     get_base_asset_jobs,
     is_base_asset_job_name,
 )
 from dagster._core.definitions.events import AssetKey
@@ -63,14 +62,16 @@
 
 
 def _env_vars_from_resource_defaults(resource_def: ResourceDefinition) -> Set[str]:
     """Given a resource definition, return a set of environment variables that are used in the
     resource's default config. This is used to extract environment variables from the top-level
     resources in a Definitions object.
     """
+    from dagster._core.execution.build_resources import wrap_resource_for_execution
+
     config_schema_default = cast(
         Mapping[str, Any],
         json.loads(resource_def.config_schema.default_value_as_json_str)
         if resource_def.config_schema.default_provided
         else {},
     )
 
@@ -82,15 +83,15 @@
             ConfigurableIOManagerFactoryResourceDefinition,
             ConfigurableResourceFactoryResourceDefinition,
         ),
     ):
         nested_resources = resource_def.inner_resource.nested_resources
         for nested_resource in nested_resources.values():
             env_vars = env_vars.union(
-                _env_vars_from_resource_defaults(coerce_to_resource(nested_resource))
+                _env_vars_from_resource_defaults(wrap_resource_for_execution(nested_resource))
             )
 
     return env_vars
 
 
 def build_caching_repository_data_from_list(
     repository_definitions: Sequence[RepositoryListDefinition],
@@ -182,15 +183,15 @@
             check.failed(f"Unexpected repository entry {definition}")
 
     if assets_defs or source_assets:
         for job_def in get_base_asset_jobs(
             assets=assets_defs,
             source_assets=source_assets,
             executor_def=default_executor_def,
-            resource_defs={},  # ????
+            resource_defs=top_level_resources,
         ):
             jobs[job_def.name] = job_def
 
         source_assets_by_key = {source_asset.key: source_asset for source_asset in source_assets}
         assets_defs_by_key = {key: asset for asset in assets_defs for key in asset.keys}
     else:
         source_assets_by_key = {}
@@ -226,15 +227,17 @@
                 unresolved_partitioned_asset_schedule.job,
             )
 
     # resolve all the UnresolvedAssetJobDefinitions using the full set of assets
     if unresolved_jobs:
         for name, unresolved_job_def in unresolved_jobs.items():
             resolved_job = unresolved_job_def.resolve(
-                asset_graph=asset_graph, default_executor_def=default_executor_def
+                asset_graph=asset_graph,
+                default_executor_def=default_executor_def,
+                resource_defs=top_level_resources,
             )
             jobs[name] = resolved_job
 
     # resolve all the UnresolvedPartitionedAssetScheduleDefinitions using
     # the resolved job containing the partitions def
     if unresolved_partitioned_asset_schedules:
         for (
@@ -313,16 +316,15 @@
 
     for key, raw_job_def in repository_definitions["jobs"].items():
         if isinstance(raw_job_def, GraphDefinition):
             repository_definitions["jobs"][key] = raw_job_def.coerce_to_job()
         elif isinstance(raw_job_def, UnresolvedAssetJobDefinition):
             repository_definitions["jobs"][key] = raw_job_def.resolve(
                 # TODO: https://github.com/dagster-io/dagster/issues/8263
-                assets=[],
-                source_assets=[],
+                asset_graph=AssetGraph.from_assets([]),
                 default_executor_def=None,
             )
         elif not isinstance(raw_job_def, JobDefinition) and not isfunction(raw_job_def):
             raise DagsterInvalidDefinitionError(
                 f"Object mapped to {key} is not an instance of JobDefinition or GraphDefinition."
             )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/repository_definition.py` & `dagster-1.4.0/dagster/_core/definitions/repository_definition/repository_definition.py`

 * *Files 4% similar despite different names*

```diff
@@ -112,19 +112,21 @@
     @property
     def repository_load_data(self) -> Optional[RepositoryLoadData]:
         return self._repository_load_data
 
     @public
     @property
     def name(self) -> str:
+        """str: The name of the repository."""
         return self._name
 
     @public
     @property
     def description(self) -> Optional[str]:
+        """Optional[str]: A human-readable description of the repository."""
         return self._description
 
     def load_all_definitions(self):
         # force load of all lazy constructed code artifacts
         self._repository_data.load_all_definitions()
 
     @public
@@ -182,35 +184,55 @@
             List[JobDefinition]: All jobs in the repository.
         """
         return self._repository_data.get_all_jobs()
 
     @public
     @property
     def schedule_defs(self) -> Sequence[ScheduleDefinition]:
+        """List[ScheduleDefinition]: All schedules in the repository."""
         return self._repository_data.get_all_schedules()
 
     @public
     def get_schedule_def(self, name: str) -> ScheduleDefinition:
+        """Get a schedule definition by name.
+
+        Args:
+            name (str): The name of the schedule.
+
+        Returns:
+            ScheduleDefinition: The schedule definition.
+        """
         return self._repository_data.get_schedule(name)
 
     @public
     def has_schedule_def(self, name: str) -> bool:
+        """bool: Check if a schedule with a given name is present in the repository."""
         return self._repository_data.has_schedule(name)
 
     @public
     @property
     def sensor_defs(self) -> Sequence[SensorDefinition]:
+        """Sequence[SensorDefinition]: All sensors in the repository."""
         return self._repository_data.get_all_sensors()
 
     @public
     def get_sensor_def(self, name: str) -> SensorDefinition:
+        """Get a sensor definition by name.
+
+        Args:
+            name (str): The name of the sensor.
+
+        Returns:
+            SensorDefinition: The sensor definition.
+        """
         return self._repository_data.get_sensor(name)
 
     @public
     def has_sensor_def(self, name: str) -> bool:
+        """bool: Check if a sensor with a given name is present in the repository."""
         return self._repository_data.has_sensor(name)
 
     @property
     def source_assets_by_key(self) -> Mapping[AssetKey, SourceAsset]:
         return self._repository_data.get_source_assets_by_key()
 
     @property
@@ -245,15 +267,19 @@
         self, asset_keys: Iterable[AssetKey]
     ) -> Optional[JobDefinition]:
         """Returns the asset base job that contains all the given assets, or None if there is no such
         job.
         """
         if self.has_job(ASSET_BASE_JOB_PREFIX):
             base_job = self.get_job(ASSET_BASE_JOB_PREFIX)
-            if all(key in base_job.asset_layer.assets_defs_by_key for key in asset_keys):
+            if all(
+                key in base_job.asset_layer.assets_defs_by_key
+                or base_job.asset_layer.is_observable_for_asset(key)
+                for key in asset_keys
+            ):
                 return base_job
         else:
             i = 0
             while self.has_job(f"{ASSET_BASE_JOB_PREFIX}_{i}"):
                 base_job = self.get_job(f"{ASSET_BASE_JOB_PREFIX}_{i}")
 
                 if all(
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/repository_definition/valid_definitions.py` & `dagster-1.4.0/dagster/_core/definitions/repository_definition/valid_definitions.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/resolved_asset_deps.py` & `dagster-1.4.0/dagster/_core/definitions/resolved_asset_deps.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/resource_annotation.py` & `dagster-1.4.0/dagster/_core/definitions/resource_annotation.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,38 +1,43 @@
 from inspect import Parameter
-from typing import Sequence, TypeVar
+from typing import Any, Optional, Sequence, Type, TypeVar
 
 from typing_extensions import Annotated
 
-from dagster._core.decorator_utils import get_function_params
+from dagster._core.decorator_utils import get_function_params, get_type_hints
 from dagster._core.definitions.resource_definition import ResourceDefinition
 
 
 def get_resource_args(fn) -> Sequence[Parameter]:
-    return [param for param in get_function_params(fn) if _is_resource_annotated(param)]
+    type_annotations = get_type_hints(fn)
+    return [
+        param
+        for param in get_function_params(fn)
+        if _is_resource_annotation(type_annotations.get(param.name))
+    ]
 
 
 RESOURCE_PARAM_METADATA = "resource_param"
 
 
-def _is_resource_annotated(param: Parameter) -> bool:
+def _is_resource_annotation(annotation: Optional[Type[Any]]) -> bool:
     from dagster._config.pythonic_config import ConfigurableResourceFactory
 
     extends_resource_definition = False
     try:
-        extends_resource_definition = isinstance(param.annotation, type) and issubclass(
-            param.annotation, (ResourceDefinition, ConfigurableResourceFactory)
+        extends_resource_definition = isinstance(annotation, type) and issubclass(
+            annotation, (ResourceDefinition, ConfigurableResourceFactory)
         )
     except TypeError:
         # Using builtin Python types in python 3.9+ will raise a TypeError when using issubclass
         # even though the isinstance check will succeed (as will inspect.isclass), for example
         # list[dict[str, str]] will raise a TypeError
         pass
 
     return (extends_resource_definition) or (
-        hasattr(param.annotation, "__metadata__")
-        and getattr(param.annotation, "__metadata__") == (RESOURCE_PARAM_METADATA,)
+        hasattr(annotation, "__metadata__")
+        and getattr(annotation, "__metadata__") == (RESOURCE_PARAM_METADATA,)
     )
 
 
 T = TypeVar("T")
 ResourceParam = Annotated[T, RESOURCE_PARAM_METADATA]
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/resource_definition.py` & `dagster-1.4.0/dagster/_core/definitions/resource_definition.py`

 * *Files 4% similar despite different names*

```diff
@@ -103,14 +103,17 @@
         self._required_resource_keys = check.opt_set_param(
             required_resource_keys, "required_resource_keys"
         )
         self._version = check.opt_str_param(version, "version")
         if version:
             experimental_arg_warning("version", "ResourceDefinition.__init__")
 
+        # this attribute will be updated by the @dagster_maintained_resource and @dagster_maintained_io_manager decorators
+        self._dagster_maintained = False
+
     @staticmethod
     def dagster_internal_init(
         *,
         resource_fn: ResourceFunction,
         config_schema: CoercableToConfigSchema,
         description: Optional[str],
         required_resource_keys: Optional[AbstractSet[str]],
@@ -131,26 +134,36 @@
     @property
     def config_schema(self) -> IDefinitionConfigSchema:
         return self._config_schema
 
     @public
     @property
     def description(self) -> Optional[str]:
+        """A human-readable description of the resource."""
         return self._description
 
     @public
     @property
     def version(self) -> Optional[str]:
+        """A string which can be used to identify a particular code version of a resource definition.
+        """
         return self._version
 
     @public
     @property
     def required_resource_keys(self) -> AbstractSet[str]:
+        """A set of the resource keys that this resource depends on. These keys will be made available
+        to the resource's init context during execution, and the resource will not be instantiated
+        until all required resources are available.
+        """
         return self._required_resource_keys
 
+    def _is_dagster_maintained(self) -> bool:
+        return self._dagster_maintained
+
     @public
     @staticmethod
     def none_resource(description: Optional[str] = None) -> "ResourceDefinition":
         """A helper function that returns a none resource.
 
         Args:
             description ([Optional[str]]): The description of the resource. Defaults to None.
@@ -191,33 +204,47 @@
         return ResourceDefinition(
             resource_fn=lambda _init_context: mock.MagicMock(), description=description
         )
 
     @public
     @staticmethod
     def string_resource(description: Optional[str] = None) -> "ResourceDefinition":
+        """Creates a ``ResourceDefinition`` which takes in a single string as configuration
+        and returns this configured string to any ops or assets which depend on it.
+
+        Args:
+            description ([Optional[str]]): The description of the string resource. Defaults to None.
+
+        Returns:
+            [ResourceDefinition]: A resource that takes in a single string as configuration and
+                returns that string.
+        """
         return ResourceDefinition(
             resource_fn=lambda init_context: init_context.resource_config,
             config_schema=str,
             description=description,
         )
 
     def copy_for_configured(
         self,
         description: Optional[str],
         config_schema: CoercableToConfigSchema,
     ) -> "ResourceDefinition":
-        return ResourceDefinition.dagster_internal_init(
+        resource_def = ResourceDefinition.dagster_internal_init(
             config_schema=config_schema,
             description=description or self.description,
             resource_fn=self.resource_fn,
             required_resource_keys=self.required_resource_keys,
             version=self.version,
         )
 
+        resource_def._dagster_maintained = self._is_dagster_maintained()  # noqa: SLF001
+
+        return resource_def
+
     def __call__(self, *args, **kwargs):
         from dagster._core.execution.context.init import UnboundInitResourceContext
 
         if has_at_least_one_parameter(self.resource_fn):
             if len(args) + len(kwargs) == 0:
                 raise DagsterInvalidInvocationError(
                     "Resource initialization function has context argument, but no context was"
@@ -261,14 +288,21 @@
         self, outer_context: Optional[object] = None
     ) -> Iterator[ResourceRequirement]:
         source_key = cast(str, outer_context)
         for resource_key in sorted(list(self.required_resource_keys)):
             yield ResourceDependencyRequirement(key=resource_key, source_key=source_key)
 
 
+def dagster_maintained_resource(
+    resource_def: ResourceDefinition,
+) -> ResourceDefinition:
+    resource_def._dagster_maintained = True  # noqa: SLF001
+    return resource_def
+
+
 class _ResourceDecoratorCallable:
     def __init__(
         self,
         config_schema: Optional[Mapping[str, Any]] = None,
         description: Optional[str] = None,
         required_resource_keys: Optional[AbstractSet[str]] = None,
         version: Optional[str] = None,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/resource_invocation.py` & `dagster-1.4.0/dagster/_core/definitions/resource_invocation.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/resource_requirement.py` & `dagster-1.4.0/dagster/_core/definitions/resource_requirement.py`

 * *Files 0% similar despite different names*

```diff
@@ -220,17 +220,17 @@
     ensure_resources_of_expected_type(resource_defs, requirements)
 
     # Error if resource defs don't provide the correct resource key
     for requirement in requirements:
         if not requirement.resources_contain_key(resource_defs):
             raise DagsterInvalidDefinitionError(
                 f"{requirement.describe_requirement()} was not provided. Please"
-                f" provide a {str(requirement.expected_type)} to key '{requirement.key}', or change"
+                f" provide a {requirement.expected_type} to key '{requirement.key}', or change"
                 " the required key to one of the following keys which points to an"
-                f" {str(requirement.expected_type)}:"
+                f" {requirement.expected_type}:"
                 f" {requirement.keys_of_expected_type(resource_defs)}"
             )
 
 
 def get_resource_key_conflicts(
     resource_defs: Mapping[str, "ResourceDefinition"],
     other_resource_defs: Mapping[str, "ResourceDefinition"],
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/run_config.py` & `dagster-1.4.0/dagster/_core/definitions/run_config.py`

 * *Files 4% similar despite different names*

```diff
@@ -30,16 +30,16 @@
     ExecutorDefinition,
     execute_in_process_executor,
     in_process_executor,
 )
 from dagster._core.definitions.input import InputDefinition
 from dagster._core.definitions.output import OutputDefinition
 from dagster._core.errors import DagsterInvalidDefinitionError
+from dagster._core.storage.input_manager import IInputManagerDefinition
 from dagster._core.storage.output_manager import IOutputManagerDefinition
-from dagster._core.storage.root_input_manager import IInputManagerDefinition
 from dagster._core.types.dagster_type import ALL_RUNTIME_BUILTINS, construct_dagster_type_dictionary
 from dagster._utils import check
 
 from .configurable import ConfigurableDefinition
 from .definition_config_schema import IDefinitionConfigSchema
 from .dependency import DependencyStructure, GraphNode, Node, NodeHandle, NodeInput, OpNode
 from .graph_definition import GraphDefinition
@@ -231,16 +231,14 @@
             and not has_upstream
         ):
             input_field = None
         elif name in direct_inputs and not has_upstream:
             input_field = None
         elif name in input_source_assets and not has_upstream:
             input_field = None
-        elif inp.root_manager_key and not has_upstream:
-            input_field = get_input_manager_input_field(node, inp, resource_defs)
         elif inp.dagster_type.loader and not has_upstream:
             input_field = get_type_loader_input_field(node, name, inp)
         else:
             input_field = None
 
         if input_field:
             inputs_field_fields[name] = input_field
@@ -270,35 +268,15 @@
 
 
 def get_input_manager_input_field(
     node: Node,
     input_def: InputDefinition,
     resource_defs: Mapping[str, ResourceDefinition],
 ) -> Optional[Field]:
-    if input_def.root_manager_key:
-        if input_def.root_manager_key not in resource_defs:
-            raise DagsterInvalidDefinitionError(
-                f"Input '{input_def.name}' for {node.describe_node()} requires root_manager_key"
-                f" '{input_def.root_manager_key}', but no resource has been provided. Please"
-                " include a resource definition for that key in the provided resource_defs."
-            )
-
-        root_manager = resource_defs[input_def.root_manager_key]
-        if not isinstance(root_manager, IInputManagerDefinition):
-            raise DagsterInvalidDefinitionError(
-                f"Input '{input_def.name}' for {node.describe_node()} requires root_manager_key "
-                f"'{input_def.root_manager_key}', but the resource definition provided is not an "
-                "IInputManagerDefinition"
-            )
-
-        input_config_schema = root_manager.input_config_schema
-        if input_config_schema:
-            return input_config_schema.as_field()
-        return None
-    elif input_def.input_manager_key:
+    if input_def.input_manager_key:
         if input_def.input_manager_key not in resource_defs:
             raise DagsterInvalidDefinitionError(
                 f"Input '{input_def.name}' for {node.describe_node()} requires input_manager_key"
                 f" '{input_def.input_manager_key}', but no resource has been provided. Please"
                 " include a resource definition for that key in the provided resource_defs."
             )
 
@@ -318,17 +296,15 @@
     return None
 
 
 def get_type_loader_input_field(node: Node, input_name: str, input_def: InputDefinition) -> Field:
     loader = check.not_none(input_def.dagster_type.loader)
     return Field(
         loader.schema_type,
-        is_required=(
-            not node.definition.input_has_default(input_name) and not input_def.root_manager_key
-        ),
+        is_required=(not node.definition.input_has_default(input_name)),
     )
 
 
 def get_outputs_field(
     node: Node,
     resource_defs: Mapping[str, ResourceDefinition],
 ) -> Optional[Field]:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/run_config_schema.py` & `dagster-1.4.0/dagster/_core/definitions/run_config_schema.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/run_request.py` & `dagster-1.4.0/dagster/_core/definitions/run_request.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from datetime import datetime
 from enum import Enum
 from typing import TYPE_CHECKING, Any, Mapping, NamedTuple, Optional, Sequence, Set, Union, cast
 
 import dagster._check as check
-from dagster._annotations import PublicAttr, experimental
+from dagster._annotations import PublicAttr
 from dagster._core.definitions.events import AssetKey
+from dagster._core.definitions.utils import validate_tags
 from dagster._core.instance import DynamicPartitionsStore
 from dagster._core.storage.dagster_run import DagsterRun, DagsterRunStatus
 from dagster._core.storage.tags import PARTITION_NAME_TAG
 from dagster._serdes.serdes import whitelist_for_serdes
 from dagster._utils.error import SerializableErrorInfo
 
 if TYPE_CHECKING:
@@ -28,15 +29,15 @@
 
 @whitelist_for_serdes
 class SkipReason(NamedTuple("_SkipReason", [("skip_message", PublicAttr[Optional[str]])])):
     """Represents a skipped evaluation, where no runs are requested. May contain a message to indicate
     why no runs were requested.
 
     Attributes:
-        skip_message (Optional[str]): A message displayed in dagit for why this evaluation resulted
+        skip_message (Optional[str]): A message displayed in the Dagster UI for why this evaluation resulted
             in no requested runs.
     """
 
     def __new__(cls, skip_message: Optional[str] = None):
         return super(SkipReason, cls).__new__(
             cls,
             skip_message=check.opt_str_param(skip_message, "skip_message"),
@@ -115,45 +116,45 @@
         run_key (Optional[str]): A string key to identify this launched run. For sensors, ensures that
             only one run is created per run key across all sensor evaluations.  For schedules,
             ensures that one run is created per tick, across failure recoveries. Passing in a `None`
             value means that a run will always be launched per evaluation.
         run_config (Optional[Mapping[str, Any]]: Configuration for the run. If the job has
             a :py:class:`PartitionedConfig`, this value will override replace the config
             provided by it.
-        tags (Optional[Dict[str, str]]): A dictionary of tags (string key-value pairs) to attach
+        tags (Optional[Dict[str, Any]]): A dictionary of tags (string key-value pairs) to attach
             to the launched run.
         job_name (Optional[str]): (Experimental) The name of the job this run request will launch.
             Required for sensors that target multiple jobs.
         asset_selection (Optional[Sequence[AssetKey]]): A sequence of AssetKeys that should be
             launched with this run.
-        stale_assets_only (Optional[Sequence[AssetKey]]): Set to true to further narrow the asset
+        stale_assets_only (bool): Set to true to further narrow the asset
             selection to stale assets. If passed without an asset selection, all stale assets in the
             job will be materialized. If the job does not materialize assets, this flag is ignored.
         partition_key (Optional[str]): The partition key for this run request.
     """
 
     def __new__(
         cls,
         run_key: Optional[str] = None,
         run_config: Optional[Union["RunConfig", Mapping[str, Any]]] = None,
-        tags: Optional[Mapping[str, str]] = None,
+        tags: Optional[Mapping[str, Any]] = None,
         job_name: Optional[str] = None,
         asset_selection: Optional[Sequence[AssetKey]] = None,
         stale_assets_only: bool = False,
         partition_key: Optional[str] = None,
     ):
         from dagster._core.definitions.run_config import convert_config_input
 
         return super(RunRequest, cls).__new__(
             cls,
             run_key=check.opt_str_param(run_key, "run_key"),
             run_config=check.opt_mapping_param(
                 convert_config_input(run_config), "run_config", key_type=str
             ),
-            tags=check.opt_mapping_param(tags, "tags", key_type=str, value_type=str),
+            tags=validate_tags(check.opt_mapping_param(tags, "tags", key_type=str)),
             job_name=check.opt_str_param(job_name, "job_name"),
             asset_selection=check.opt_nullable_sequence_param(
                 asset_selection, "asset_selection", of_type=AssetKey
             ),
             stale_assets_only=check.bool_param(stale_assets_only, "stale_assets_only"),
             partition_key=check.opt_str_param(partition_key, "partition_key"),
         )
@@ -336,15 +337,14 @@
             cls,
             dagster_run=check.opt_inst_param(dagster_run, "dagster_run", DagsterRun),
             error=check.opt_inst_param(error, "error", SerializableErrorInfo),
             run_status=check.opt_inst_param(run_status, "run_status", DagsterRunStatus),
         )
 
 
-@experimental
 class SensorResult(
     NamedTuple(
         "_SensorResult",
         [
             ("run_requests", Optional[Sequence[RunRequest]]),
             ("skip_reason", Optional[SkipReason]),
             ("cursor", Optional[str]),
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/run_status_sensor_definition.py` & `dagster-1.4.0/dagster/_core/definitions/run_status_sensor_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
+import functools
 import logging
 from contextlib import ExitStack
 from datetime import datetime
 from typing import (
     TYPE_CHECKING,
+    Any,
     Callable,
     Iterator,
     Mapping,
     NamedTuple,
     Optional,
     Sequence,
     Set,
@@ -121,37 +123,42 @@
         instance,
         context: Optional[
             SensorEvaluationContext
         ] = None,  # deprecated arg, but we need to keep it for backcompat
         resource_defs: Optional[Mapping[str, "ResourceDefinition"]] = None,
         logger: Optional[logging.Logger] = None,
         partition_key: Optional[str] = None,
+        _resources: Optional[Resources] = None,
+        _cm_scope_entered: bool = False,
     ) -> None:
         self._exit_stack = ExitStack()
         self._sensor_name = check.str_param(sensor_name, "sensor_name")
         self._dagster_run = check.inst_param(dagster_run, "dagster_run", DagsterRun)
         self._dagster_event = check.inst_param(dagster_event, "dagster_event", DagsterEvent)
         self._instance = check.inst_param(instance, "instance", DagsterInstance)
         self._logger: Optional[logging.Logger] = logger or (context.log if context else None)
         self._partition_key = check.opt_str_param(partition_key, "partition_key")
 
         # Wait to set resources unless they're accessed
         self._resource_defs = resource_defs
-        self._resources = None
-        self._cm_scope_entered = False
+        self._resources = _resources
+        self._cm_scope_entered = _cm_scope_entered
 
     def for_run_failure(self) -> "RunFailureSensorContext":
         """Converts RunStatusSensorContext to RunFailureSensorContext."""
         return RunFailureSensorContext(
             sensor_name=self._sensor_name,
             dagster_run=self._dagster_run,
             dagster_event=self._dagster_event,
             instance=self._instance,
             logger=self._logger,
             partition_key=self._partition_key,
+            resource_defs=self._resource_defs,
+            _resources=self._resources,
+            _cm_scope_entered=self._cm_scope_entered,
         )
 
     @property
     def resource_defs(self) -> Optional[Mapping[str, "ResourceDefinition"]]:
         return self._resource_defs
 
     @property
@@ -225,14 +232,15 @@
             self._logger = InstigationLogger()
 
         return self._logger
 
     @public
     @property
     def partition_key(self) -> Optional[str]:
+        """Optional[str]: The partition key of the relevant run."""
         return self._partition_key
 
     def __enter__(self) -> "RunStatusSensorContext":
         self._cm_scope_entered = True
         return self
 
     def __exit__(self, *exc) -> None:
@@ -426,15 +434,15 @@
             Dagster instance. If set to True, an error will be raised if you also specify
             monitored_jobs or job_selection. Defaults to False.
         job_selection (Optional[List[Union[JobDefinition, GraphDefinition, RepositorySelector, JobSelector, CodeLocationSelector]]]):
             (deprecated in favor of monitored_jobs) The jobs in the current repository that will be
             monitored by this failure sensor. Defaults to None, which means the alert will be sent
             when any job in the repository fails.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         request_job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJob]]): The job a RunRequest should
             execute if yielded from the sensor.
         request_jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition, UnresolvedAssetJob]]]): (experimental)
             A list of jobs to be executed if RunRequests are yielded from the sensor.
     """
 
     def inner(
@@ -457,16 +465,25 @@
             description=description,
             monitored_jobs=jobs,
             monitor_all_repositories=monitor_all_repositories,
             default_status=default_status,
             request_job=request_job,
             request_jobs=request_jobs,
         )
-        def _run_failure_sensor(context: RunStatusSensorContext):
-            return fn(context.for_run_failure())  # fmt: skip
+        @functools.wraps(fn)
+        def _run_failure_sensor(*args, **kwargs) -> Any:
+            args_modified = [
+                arg.for_run_failure() if isinstance(arg, RunStatusSensorContext) else arg
+                for arg in args
+            ]
+            kwargs_modified = {
+                k: v.for_run_failure() if isinstance(v, RunStatusSensorContext) else v
+                for k, v in kwargs.items()
+            }
+            return fn(*args_modified, **kwargs_modified)
 
         return _run_failure_sensor
 
     # This case is for when decorator is used bare, without arguments
     if callable(name):
         return inner(name)
 
@@ -489,15 +506,15 @@
         monitored_jobs (Optional[List[Union[JobDefinition, GraphDefinition, UnresolvedAssetJobDefinition, JobSelector, RepositorySelector, CodeLocationSelector]]]):
             The jobs in the current repository that will be monitored by this sensor. Defaults to
             None, which means the alert will be sent when any job in the repository fails.
         monitor_all_repositories (bool): If set to True, the sensor will monitor all runs in the
             Dagster instance. If set to True, an error will be raised if you also specify
             monitored_jobs or job_selection. Defaults to False.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         request_job (Optional[Union[GraphDefinition, JobDefinition]]): The job a RunRequest should
             execute if yielded from the sensor.
         request_jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition]]]): (experimental)
             A list of jobs to be executed if RunRequests are yielded from the sensor.
     """
 
     def __init__(
@@ -875,15 +892,15 @@
             If set to True, an error will be raised if you also specify monitored_jobs or job_selection.
             Defaults to False.
         job_selection (Optional[List[Union[JobDefinition, GraphDefinition, RepositorySelector, JobSelector, CodeLocationSelector]]]):
             (deprecated in favor of monitored_jobs) Jobs in the current repository that will be
             monitored by this sensor. Defaults to None, which means the alert will be sent when
             any job in the repository matches the requested run_status.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         request_job (Optional[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]): The job that should be
             executed if a RunRequest is yielded from the sensor.
         request_jobs (Optional[Sequence[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]]): (experimental)
             A list of jobs to be executed if RunRequests are yielded from the sensor.
     """
 
     def inner(
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/schedule_definition.py` & `dagster-1.4.0/dagster/_core/definitions/schedule_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -292,14 +292,15 @@
             },
             repository_def=self._repository_def,
         )
 
     @public
     @property
     def instance(self) -> "DagsterInstance":
+        """DagsterInstance: The current DagsterInstance."""
         # self._instance_ref should only ever be None when this ScheduleEvaluationContext was
         # constructed under test.
         if not self._instance_ref:
             raise DagsterInvariantViolationError(
                 "Attempted to initialize dagster instance, but no instance reference was provided."
             )
         if not self._instance:
@@ -502,15 +503,15 @@
         execution_timezone (Optional[str]): Timezone in which the schedule should run.
             Supported strings for timezones are the ones provided by the
             `IANA time zone database <https://www.iana.org/time-zones>` - e.g. "America/Los_Angeles".
         description (Optional[str]): A human-readable description of the schedule.
         job (Optional[Union[GraphDefinition, JobDefinition]]): The job that should execute when this
             schedule runs.
         default_status (DefaultScheduleStatus): Whether the schedule starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         required_resource_keys (Optional[Set[str]]): The set of resource keys required by the schedule.
     """
 
     def with_updated_job(self, new_job: ExecutableDefinition) -> "ScheduleDefinition":
         """Returns a copy of this schedule with the job replaced.
 
         Args:
@@ -776,50 +777,59 @@
             return copy.deepcopy(result)
         else:
             return result
 
     @public
     @property
     def name(self) -> str:
+        """str: The name of the schedule."""
         return self._name
 
     @public
     @property
     def job_name(self) -> str:
+        """str: The name of the job targeted by this schedule."""
         return self._target.job_name
 
     @public
     @property
     def description(self) -> Optional[str]:
+        """Optional[str]: A description for this schedule."""
         return self._description
 
     @public
     @property
     def cron_schedule(self) -> Union[str, Sequence[str]]:
+        """Union[str, Sequence[str]]: The cron schedule representing when this schedule will be evaluated.
+        """
         return self._cron_schedule  # type: ignore
 
-    @public
     @deprecated
     @property
     def environment_vars(self) -> Mapping[str, str]:
         return self._environment_vars
 
     @public
     @property
     def required_resource_keys(self) -> Set[str]:
+        """Set[str]: The set of keys for resources that must be provided to this schedule."""
         return self._required_resource_keys
 
     @public
     @property
     def execution_timezone(self) -> Optional[str]:
+        """Optional[str]: The timezone in which this schedule will be evaluated."""
         return self._execution_timezone
 
     @public
     @property
     def job(self) -> Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]:
+        """Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]: The job that is
+        targeted by this schedule.
+        """
         if isinstance(self._target, DirectTarget):
             return self._target.target
         raise DagsterInvalidDefinitionError("No job was provided to ScheduleDefinition.")
 
     def evaluate_tick(self, context: "ScheduleEvaluationContext") -> ScheduleExecutionData:
         """Evaluate schedule using the provided context.
 
@@ -924,8 +934,11 @@
             return self._target.load()
 
         check.failed("Target is not loadable")
 
     @public
     @property
     def default_status(self) -> DefaultScheduleStatus:
+        """DefaultScheduleStatus: The default status for this schedule when it is first loaded in
+        a code location.
+        """
         return self._default_status
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/scoped_resources_builder.py` & `dagster-1.4.0/dagster/_core/definitions/scoped_resources_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -105,25 +105,25 @@
                 Resources,
                 IContainsGenerator,
             ):
                 @property
                 def _original_resource_dict(self) -> Mapping[str, object]:
                     return resource_instance_dict
 
-            return _ScopedResourcesContainsGenerator(**resources_to_attach_to_context)  # type: ignore[call-arg]
+            return _ScopedResourcesContainsGenerator(**resources_to_attach_to_context)
 
         else:
 
             class _ScopedResources(
                 namedtuple("_ScopedResources", list(resources_to_attach_to_context.keys())),
                 Resources,
             ):
                 @property
                 def _original_resource_dict(self) -> Mapping[str, object]:
                     return resource_instance_dict
 
-            return _ScopedResources(**resources_to_attach_to_context)  # type: ignore[call-arg]
+            return _ScopedResources(**resources_to_attach_to_context)
 
     @classmethod
     def build_empty(cls) -> Resources:
         """Returns an empty Resources object, equivalent to ScopedResourcesBuilder().build(None)."""
         return cls().build(None)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/selector.py` & `dagster-1.4.0/dagster/_core/definitions/selector.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/sensor_definition.py` & `dagster-1.4.0/dagster/_core/definitions/sensor_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -218,14 +218,15 @@
                 **wrap_resources_for_execution(resources_dict),
             },
         )
 
     @public
     @property
     def resources(self) -> Resources:
+        """Resources: A mapping from resource key to instantiated resources for this sensor."""
         from dagster._core.definitions.scoped_resources_builder import (
             IContainsGenerator,
         )
         from dagster._core.execution.build_resources import build_resources
 
         if not self._resources:
             """
@@ -270,14 +271,15 @@
                 )
 
         return self._resources
 
     @public
     @property
     def instance(self) -> DagsterInstance:
+        """DagsterInstance: The current DagsterInstance."""
         # self._instance_ref should only ever be None when this SensorEvaluationContext was
         # constructed under test.
         if not self._instance:
             if not self._instance_ref:
                 raise DagsterInvariantViolationError(
                     "Attempted to initialize dagster instance, but no instance reference was"
                     " provided."
@@ -290,19 +292,23 @@
     @property
     def instance_ref(self) -> Optional[InstanceRef]:
         return self._instance_ref
 
     @public
     @property
     def last_completion_time(self) -> Optional[float]:
+        """Optional[float]: Timestamp representing the last time this sensor completed an evaluation.
+        """
         return self._last_completion_time
 
     @public
     @property
     def last_run_key(self) -> Optional[str]:
+        """Optional[str]: The run key supplied to the most recent RunRequest produced by this sensor.
+        """
         return self._last_run_key
 
     @public
     @property
     def cursor(self) -> Optional[str]:
         """The cursor value for this sensor, which was set in an earlier sensor evaluation."""
         return self._cursor
@@ -324,19 +330,21 @@
     @property
     def cursor_updated(self) -> bool:
         return self._cursor_updated
 
     @public
     @property
     def repository_name(self) -> Optional[str]:
+        """Optional[str]: The name of the repository that this sensor resides in."""
         return self._repository_name
 
     @public
     @property
     def repository_def(self) -> Optional["RepositoryDefinition"]:
+        """Optional[RepositoryDefinition]: The RepositoryDefinition that this sensor resides in."""
         return self._repository_def
 
     @property
     def log(self) -> logging.Logger:
         if self._logger:
             return self._logger
 
@@ -468,15 +476,15 @@
         name (Optional[str]): The name of the sensor to create. Defaults to name of evaluation_fn
         minimum_interval_seconds (Optional[int]): The minimum number of seconds that will elapse
             between sensor evaluations.
         description (Optional[str]): A human-readable description of the sensor.
         job (Optional[GraphDefinition, JobDefinition, UnresolvedAssetJob]): The job to execute when this sensor fires.
         jobs (Optional[Sequence[GraphDefinition, JobDefinition, UnresolvedAssetJob]]): (experimental) A list of jobs to execute when this sensor fires.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
         asset_selection (AssetSelection): (Experimental) an asset selection to launch a run for if
             the sensor condition is met. This can be provided instead of specifying a job.
     """
 
     def with_updated_jobs(self, new_jobs: Sequence[ExecutableDefinition]) -> "SensorDefinition":
         """Returns a copy of this sensor with the jobs replaced.
 
@@ -490,15 +498,15 @@
             minimum_interval_seconds=self.minimum_interval_seconds,
             description=self.description,
             job_name=None,  # if original init was passed job name, was resolved to a job
             jobs=new_jobs if len(new_jobs) > 1 else None,
             job=new_jobs[0] if len(new_jobs) == 1 else None,
             default_status=self.default_status,
             asset_selection=self.asset_selection,
-            required_resource_keys=self.required_resource_keys,
+            required_resource_keys=self._raw_required_resource_keys,
         )
 
     def with_updated_job(self, new_job: ExecutableDefinition) -> "SensorDefinition":
         """Returns a copy of this sensor with the job replaced.
 
         Args:
             job (ExecutableDefinition): The job that should execute when this
@@ -592,18 +600,18 @@
         check.param_invariant(
             len(required_resource_keys or []) == 0 or len(resource_arg_names) == 0,
             (
                 "Cannot specify resource requirements in both @sensor decorator and as arguments to"
                 " the decorated function"
             ),
         )
-        self._required_resource_keys = (
-            check.opt_set_param(required_resource_keys, "required_resource_keys", of_type=str)
-            or resource_arg_names
+        self._raw_required_resource_keys = check.opt_set_param(
+            required_resource_keys, "required_resource_keys", of_type=str
         )
+        self._required_resource_keys = self._raw_required_resource_keys or resource_arg_names
 
     @staticmethod
     def dagster_internal_init(
         *,
         name: Optional[str],
         evaluation_fn: Optional[RawSensorEvaluationFunction],
         job_name: Optional[str],
@@ -640,50 +648,61 @@
             context.resources, self.name, self._required_resource_keys
         )
         return self._raw_fn(**context_param, **resources)
 
     @public
     @property
     def required_resource_keys(self) -> Set[str]:
+        """Set[str]: The set of keys for resources that must be provided to this sensor."""
         return self._required_resource_keys
 
     @public
     @property
     def name(self) -> str:
+        """str: The name of this sensor."""
         return self._name
 
     @public
     @property
     def description(self) -> Optional[str]:
+        """Optional[str]: A description for this sensor."""
         return self._description
 
     @public
     @property
     def minimum_interval_seconds(self) -> Optional[int]:
+        """Optional[int]: The minimum number of seconds between sequential evaluations of this sensor.
+        """
         return self._min_interval
 
     @property
     def targets(self) -> Sequence[Union[DirectTarget, RepoRelativeTarget]]:
         return self._targets
 
     @public
     @property
     def job(self) -> Union[JobDefinition, GraphDefinition, UnresolvedAssetJobDefinition]:
+        """Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]: The job that is
+        targeted by this schedule.
+        """
         if self._targets:
             if len(self._targets) == 1 and isinstance(self._targets[0], DirectTarget):
                 return self._targets[0].target
             elif len(self._targets) > 1:
                 raise DagsterInvalidDefinitionError(
                     "Job property not available when SensorDefinition has multiple jobs."
                 )
         raise DagsterInvalidDefinitionError("No job was provided to SensorDefinition.")
 
     @public
     @property
     def jobs(self) -> List[Union[JobDefinition, GraphDefinition, UnresolvedAssetJobDefinition]]:
+        """List[Union[GraphDefinition, JobDefinition, UnresolvedAssetJobDefinition]]: A list of jobs
+        that are targeted by this schedule.
+        """
         if self._targets and all(isinstance(target, DirectTarget) for target in self._targets):
             return [target.target for target in self._targets]  # type: ignore  # (illegible conditional)
         raise DagsterInvalidDefinitionError("No job was provided to SensorDefinition.")
 
     @property
     def sensor_type(self) -> SensorType:
         return SensorType.STANDARD
@@ -882,24 +901,28 @@
     @property
     def _target(self) -> Optional[Union[DirectTarget, RepoRelativeTarget]]:
         return self._targets[0] if self._targets else None
 
     @public
     @property
     def job_name(self) -> Optional[str]:
+        """Optional[str]: The name of the job that is targeted by this sensor."""
         if len(self._targets) > 1:
             raise DagsterInvalidInvocationError(
                 f"Cannot use `job_name` property for sensor {self.name}, which targets multiple"
                 " jobs."
             )
         return self._targets[0].job_name
 
     @public
     @property
     def default_status(self) -> DefaultSensorStatus:
+        """DefaultSensorStatus: The default status for this sensor when it is first loaded in
+        a code location.
+        """
         return self._default_status
 
     @property
     def asset_selection(self) -> Optional[AssetSelection]:
         return self._asset_selection
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/source_asset.py` & `dagster-1.4.0/dagster/_core/definitions/source_asset.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,9 @@
-from __future__ import annotations
-
 import warnings
 from typing import (
-    TYPE_CHECKING,
     AbstractSet,
     Any,
     Callable,
     Dict,
     Iterator,
     Mapping,
     Optional,
@@ -50,20 +47,14 @@
     DagsterInvalidInvocationError,
     DagsterInvalidObservationError,
 )
 from dagster._core.storage.io_manager import IOManagerDefinition
 from dagster._utils.backcompat import ExperimentalWarning, experimental_arg_warning
 from dagster._utils.merger import merge_dicts
 
-if TYPE_CHECKING:
-    from dagster._core.execution.context.compute import (
-        OpExecutionContext,
-    )
-
-
 # Going with this catch-all for the time-being to permit pythonic resources
 SourceAssetObserveFunction: TypeAlias = Callable[..., Any]
 
 
 class SourceAsset(ResourceAddable):
     """A SourceAsset represents an asset that will be loaded by (but not updated by) Dagster.
 
@@ -88,26 +79,29 @@
     _io_manager_def: PublicAttr[Optional[IOManagerDefinition]]
     description: PublicAttr[Optional[str]]
     partitions_def: PublicAttr[Optional[PartitionsDefinition]]
     group_name: PublicAttr[str]
     resource_defs: PublicAttr[Dict[str, ResourceDefinition]]
     observe_fn: PublicAttr[Optional[SourceAssetObserveFunction]]
     _node_def: Optional[OpDefinition]  # computed lazily
+    auto_observe_interval_minutes: Optional[float]
 
     def __init__(
         self,
         key: CoercibleToAssetKey,
         metadata: Optional[ArbitraryMetadataMapping] = None,
         io_manager_key: Optional[str] = None,
         io_manager_def: Optional[object] = None,
         description: Optional[str] = None,
         partitions_def: Optional[PartitionsDefinition] = None,
         group_name: Optional[str] = None,
         resource_defs: Optional[Mapping[str, object]] = None,
         observe_fn: Optional[SourceAssetObserveFunction] = None,
+        *,
+        auto_observe_interval_minutes: Optional[float] = None,
         # This is currently private because it is necessary for source asset observation functions,
         # but we have not yet decided on a final API for associated one or more ops with a source
         # asset. If we were to make this public, then we would have a canonical public
         # `required_resource_keys` used for observation that might end up conflicting with a set of
         # required resource keys for a different operation.
         _required_resource_keys: Optional[AbstractSet[str]] = None,
         # Add additional fields to with_resources and with_group below
@@ -152,14 +146,17 @@
         self.group_name = validate_group_name(group_name)
         self.description = check.opt_str_param(description, "description")
         self.observe_fn = check.opt_callable_param(observe_fn, "observe_fn")
         self._required_resource_keys = check.opt_set_param(
             _required_resource_keys, "_required_resource_keys", of_type=str
         )
         self._node_def = None
+        self.auto_observe_interval_minutes = check.opt_numeric_param(
+            auto_observe_interval_minutes, "auto_observe_interval_minutes"
+        )
 
     def get_io_manager_key(self) -> str:
         return self.io_manager_key or DEFAULT_IO_MANAGER_KEY
 
     @property
     def io_manager_def(self) -> Optional[IOManagerDefinition]:
         io_manager_key = self.get_io_manager_key()
@@ -183,14 +180,17 @@
         return self.node_def is not None
 
     def _get_op_def_compute_fn(self, observe_fn: SourceAssetObserveFunction):
         from dagster._core.definitions.decorators.op_decorator import (
             DecoratedOpFunction,
             is_context_provided,
         )
+        from dagster._core.execution.context.compute import (
+            OpExecutionContext,
+        )
 
         observe_fn_has_context = is_context_provided(get_function_params(observe_fn))
 
         def fn(context: OpExecutionContext):
             resource_kwarg_keys = [param.name for param in get_resource_args(observe_fn)]
             resource_kwargs = {key: getattr(context.resources, key) for key in resource_kwarg_keys}
             observe_fn_return_value = (
@@ -306,14 +306,15 @@
                 io_manager_key=io_manager_key,
                 description=self.description,
                 partitions_def=self.partitions_def,
                 metadata=self.raw_metadata,
                 resource_defs=relevant_resource_defs,
                 group_name=self.group_name,
                 observe_fn=self.observe_fn,
+                auto_observe_interval_minutes=self.auto_observe_interval_minutes,
                 _required_resource_keys=self._required_resource_keys,
             )
 
     def with_attributes(
         self, group_name: Optional[str] = None, key: Optional[AssetKey] = None
     ) -> "SourceAsset":
         if group_name is not None and self.group_name != DEFAULT_GROUP_NAME:
@@ -331,14 +332,15 @@
                 io_manager_key=self.io_manager_key,
                 io_manager_def=self.io_manager_def,
                 description=self.description,
                 partitions_def=self.partitions_def,
                 group_name=group_name,
                 resource_defs=self.resource_defs,
                 observe_fn=self.observe_fn,
+                auto_observe_interval_minutes=self.auto_observe_interval_minutes,
                 _required_resource_keys=self._required_resource_keys,
             )
 
     def get_resource_requirements(self) -> Iterator[ResourceRequirement]:
         if self.node_def is not None:
             yield from self.node_def.get_resource_requirements()
         yield SourceAssetIOManagerRequirement(
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/step_launcher.py` & `dagster-1.4.0/dagster/_core/definitions/step_launcher.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/target.py` & `dagster-1.4.0/dagster/_core/definitions/target.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/time_window_partition_mapping.py` & `dagster-1.4.0/dagster/_core/definitions/time_window_partition_mapping.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,31 +1,37 @@
 from datetime import datetime
 from typing import NamedTuple, Optional, cast
 
 import dagster._check as check
 from dagster._annotations import PublicAttr
 from dagster._core.definitions.partition import PartitionsDefinition, PartitionsSubset
-from dagster._core.definitions.partition_key_range import PartitionKeyRange
-from dagster._core.definitions.partition_mapping import PartitionMapping
+from dagster._core.definitions.partition_mapping import PartitionMapping, UpstreamPartitionsResult
 from dagster._core.definitions.time_window_partitions import (
     TimeWindow,
     TimeWindowPartitionsDefinition,
     TimeWindowPartitionsSubset,
 )
-from dagster._core.errors import DagsterInvalidDefinitionError, DagsterInvalidInvocationError
+from dagster._core.errors import DagsterInvalidDefinitionError
 from dagster._core.instance import DynamicPartitionsStore
 from dagster._serdes import whitelist_for_serdes
+from dagster._utils.backcompat import (
+    experimental_arg_warning,
+)
 
 
 @whitelist_for_serdes
 class TimeWindowPartitionMapping(
     PartitionMapping,
     NamedTuple(
         "_TimeWindowPartitionMapping",
-        [("start_offset", PublicAttr[int]), ("end_offset", PublicAttr[int])],
+        [
+            ("start_offset", PublicAttr[int]),
+            ("end_offset", PublicAttr[int]),
+            ("allow_nonexistent_upstream_partitions", PublicAttr[bool]),
+        ],
     ),
 ):
     """The default mapping between two TimeWindowPartitionsDefinitions.
 
     A partition in the downstream partitions definition is mapped to all partitions in the upstream
     asset whose time windows overlap it.
 
@@ -50,14 +56,20 @@
             partitions "2022-07-03" and "2022-07-04". Only permitted to be non-zero when the
             upstream and downstream PartitionsDefinitions are the same. Defaults to 0.
         end_offset (int): If not 0, then the ends of the upstream windows are shifted by this
             offset relative to the ends of the downstream windows. For example, if start_offset=0
             and end_offset=1, then the downstream partition "2022-07-04" would map to the upstream
             partitions "2022-07-04" and "2022-07-05". Only permitted to be non-zero when the
             upstream and downstream PartitionsDefinitions are the same. Defaults to 0.
+        allow_nonexistent_upstream_partitions (bool): Defaults to false. If true, does not
+            raise an error when mapped upstream partitions fall outside the start-end time window of the
+            partitions def. For example, if the upstream partitions def starts on "2023-01-01" but
+            the downstream starts on "2022-01-01", setting this bool to true would return no
+            partition keys when get_upstream_partitions_for_partitions is called with "2022-06-01".
+            When set to false, would raise an error.
 
     Examples:
         .. code-block:: python
 
             from dagster import DailyPartitionsDefinition, TimeWindowPartitionMapping, AssetIn, asset
 
             partitions_def = DailyPartitionsDefinition(start_date="2020-01-01")
@@ -74,103 +86,101 @@
                     )
                 }
             )
             def asset2(asset1):
                 ...
     """
 
-    def __new__(cls, start_offset: int = 0, end_offset: int = 0):
+    def __new__(
+        cls,
+        start_offset: int = 0,
+        end_offset: int = 0,
+        allow_nonexistent_upstream_partitions: bool = False,
+    ):
+        if allow_nonexistent_upstream_partitions:
+            experimental_arg_warning(
+                "allow_nonexistent_upstream_partitions", "TimeWindowPartitionMapping.__init__"
+            )
+
         return super(TimeWindowPartitionMapping, cls).__new__(
             cls,
             start_offset=check.int_param(start_offset, "start_offset"),
             end_offset=check.int_param(end_offset, "end_offset"),
+            allow_nonexistent_upstream_partitions=check.bool_param(
+                allow_nonexistent_upstream_partitions,
+                "allow_nonexistent_upstream_partitions",
+            ),
         )
 
-    def get_upstream_partitions_for_partition_range(
-        self,
-        downstream_partition_key_range: Optional[PartitionKeyRange],
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
-
-    def get_upstream_partitions_for_partitions(
+    def get_upstream_mapped_partitions_result_for_partitions(
         self,
         downstream_partitions_subset: Optional[PartitionsSubset],
         upstream_partitions_def: PartitionsDefinition,
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
-    ) -> PartitionsSubset:
-        """Returns the partitions in the upstream asset that map to the given downstream partitions.
-
-        Raises an error if upstream partitions do not exist at the given current_time, fetching the
-        current time if not provided.
-        """
+    ) -> UpstreamPartitionsResult:
         if not isinstance(downstream_partitions_subset, TimeWindowPartitionsSubset):
             check.failed("downstream_partitions_subset must be a TimeWindowPartitionsSubset")
 
-        if not isinstance(upstream_partitions_def, TimeWindowPartitionsDefinition):
-            check.failed("upstream_partitions_def must be a TimeWindowPartitionsDefinition")
-
         return self._map_partitions(
-            cast(TimeWindowPartitionsDefinition, downstream_partitions_subset.partitions_def),
+            downstream_partitions_subset.partitions_def,
             upstream_partitions_def,
             downstream_partitions_subset,
-            self.start_offset,
-            self.end_offset,
-            raise_on_non_existent_partition=True,
+            start_offset=self.start_offset,
+            end_offset=self.end_offset,
             current_time=current_time,
         )
 
-    def get_downstream_partitions_for_partition_range(
-        self,
-        upstream_partition_key_range: PartitionKeyRange,
-        downstream_partitions_def: Optional[PartitionsDefinition],
-        upstream_partitions_def: PartitionsDefinition,
-    ) -> PartitionKeyRange:
-        raise NotImplementedError()
-
     def get_downstream_partitions_for_partitions(
         self,
         upstream_partitions_subset: PartitionsSubset,
         downstream_partitions_def: Optional[PartitionsDefinition],
         current_time: Optional[datetime] = None,
         dynamic_partitions_store: Optional[DynamicPartitionsStore] = None,
     ) -> PartitionsSubset:
         """Returns the partitions in the downstream asset that map to the given upstream partitions.
 
         Filters for partitions that exist at the given current_time, fetching the current time
         if not provided.
         """
-        if not isinstance(downstream_partitions_def, TimeWindowPartitionsDefinition):
-            check.failed("downstream_partitions_def must be a TimeWindowPartitionsDefinitions")
-
-        if not isinstance(upstream_partitions_subset, TimeWindowPartitionsSubset):
-            check.failed("upstream_partitions_subset must be a TimeWindowPartitionsSubset")
-
         return self._map_partitions(
-            cast(TimeWindowPartitionsDefinition, upstream_partitions_subset.partitions_def),
+            upstream_partitions_subset.partitions_def,
             downstream_partitions_def,
             upstream_partitions_subset,
-            -self.start_offset,
-            -self.end_offset,
-            raise_on_non_existent_partition=False,
+            end_offset=-self.start_offset,
+            start_offset=-self.end_offset,
             current_time=current_time,
-        )
+        ).partitions_subset
 
     def _map_partitions(
         self,
-        from_partitions_def: TimeWindowPartitionsDefinition,
-        to_partitions_def: TimeWindowPartitionsDefinition,
-        from_partitions_subset: TimeWindowPartitionsSubset,
+        from_partitions_def: PartitionsDefinition,
+        to_partitions_def: Optional[PartitionsDefinition],
+        from_partitions_subset: PartitionsSubset,
         start_offset: int,
         end_offset: int,
-        raise_on_non_existent_partition: bool,
         current_time: Optional[datetime] = None,
-    ) -> TimeWindowPartitionsSubset:
+    ) -> UpstreamPartitionsResult:
+        """Maps the partitions in from_partitions_subset to partitions in to_partitions_def.
+
+        If partitions in from_partitions_subset represent time windows that do not exist in
+        to_partitions_def, raises an error if raise_error_on_invalid_mapped_partition is True.
+        Otherwise, filters out the partitions that do not exist in to_partitions_def and returns
+        the filtered subset, also returning a bool indicating whether there were mapped time windows
+        that did not exist in to_partitions_def.
+        """
+        if not isinstance(from_partitions_subset, TimeWindowPartitionsSubset):
+            check.failed("from_partitions_subset must be a TimeWindowPartitionsSubset")
+
+        if not isinstance(from_partitions_def, TimeWindowPartitionsDefinition):
+            check.failed("from_partitions_def must be a TimeWindowPartitionsDefinition")
+
+        if not isinstance(to_partitions_def, TimeWindowPartitionsDefinition):
+            check.failed("to_partitions_def must be a TimeWindowPartitionsDefinition")
+
         if (start_offset != 0 or end_offset != 0) and (
             from_partitions_def.cron_schedule != to_partitions_def.cron_schedule
         ):
             raise DagsterInvalidDefinitionError(
                 "Can't use the start_offset or end_offset parameters of"
                 " TimeWindowPartitionMapping when the cron schedule of the upstream"
                 " PartitionsDefinition is different than the cron schedule of the downstream"
@@ -179,15 +189,15 @@
             )
 
         if to_partitions_def.timezone != from_partitions_def.timezone:
             raise DagsterInvalidDefinitionError("Timezones don't match")
 
         # skip fancy mapping logic in the simple case
         if from_partitions_def == to_partitions_def and start_offset == 0 and end_offset == 0:
-            return from_partitions_subset
+            return UpstreamPartitionsResult(from_partitions_subset, [])
 
         time_windows = []
         for from_partition_time_window in from_partitions_subset.included_time_windows:
             from_start_dt, from_end_dt = from_partition_time_window
             offsetted_start_dt = _offsetted_datetime(
                 from_partitions_def, from_start_dt, start_offset
             )
@@ -223,40 +233,65 @@
                 if window_start < window_end:
                     time_windows.append(TimeWindow(window_start, window_end))
 
         first_window = to_partitions_def.get_first_partition_window(current_time=current_time)
         last_window = to_partitions_def.get_last_partition_window(current_time=current_time)
 
         filtered_time_windows = []
+        required_but_nonexistent_partition_keys = set()
 
         for time_window in time_windows:
             if (
                 first_window
                 and last_window
                 and time_window.start <= last_window.start
                 and time_window.end >= first_window.end
             ):
-                window_end = min(time_window.end, last_window.end)
                 window_start = max(time_window.start, first_window.start)
+                window_end = min(time_window.end, last_window.end)
                 filtered_time_windows.append(TimeWindow(window_start, window_end))
 
-        if raise_on_non_existent_partition:
-            if filtered_time_windows != time_windows:
-                raise DagsterInvalidInvocationError(
-                    f"Provided time windows {time_windows} contain invalid time windows for"
-                    f" partitions definition {to_partitions_def}"
-                )
+            if self.allow_nonexistent_upstream_partitions:
+                # If allowed to have nonexistent upstream partitions, do not consider
+                # out of range partitions to be invalid
+                continue
+            else:
+                invalid_time_window = None
+                if not (first_window and last_window) or (
+                    time_window.start < first_window.start and time_window.end > last_window.end
+                ):
+                    invalid_time_window = time_window
+                elif time_window.start < first_window.start:
+                    invalid_time_window = TimeWindow(
+                        time_window.start, min(time_window.end, first_window.start)
+                    )
+                elif time_window.end > last_window.end:
+                    invalid_time_window = TimeWindow(
+                        max(time_window.start, last_window.end), time_window.end
+                    )
+
+                if invalid_time_window:
+                    required_but_nonexistent_partition_keys.update(
+                        set(
+                            to_partitions_def.get_partition_keys_in_time_window(
+                                time_window=invalid_time_window
+                            )
+                        )
+                    )
 
-        return TimeWindowPartitionsSubset(
-            to_partitions_def,
-            num_partitions=sum(
-                len(to_partitions_def.get_partition_keys_in_time_window(time_window))
-                for time_window in filtered_time_windows
+        return UpstreamPartitionsResult(
+            TimeWindowPartitionsSubset(
+                to_partitions_def,
+                num_partitions=sum(
+                    len(to_partitions_def.get_partition_keys_in_time_window(time_window))
+                    for time_window in filtered_time_windows
+                ),
+                included_time_windows=filtered_time_windows,
             ),
-            included_time_windows=filtered_time_windows,
+            sorted(list(required_but_nonexistent_partition_keys)),
         )
 
 
 def _offsetted_datetime(
     partitions_def: TimeWindowPartitionsDefinition, dt: datetime, offset: int
 ) -> Optional[datetime]:
     for _ in range(abs(offset)):
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/time_window_partitions.py` & `dagster-1.4.0/dagster/_core/definitions/time_window_partitions.py`

 * *Files 1% similar despite different names*

```diff
@@ -294,25 +294,14 @@
         if self.end_offset != 0:
             partition_def_str += (
                 " End offsetted by"
                 f" {self.end_offset} partition{'' if self.end_offset == 1 else 's'}."
             )
         return partition_def_str
 
-    def __eq__(self, other):
-        return (
-            isinstance(other, TimeWindowPartitionsDefinition)
-            and pendulum.instance(self.start, tz=self.timezone).timestamp()
-            == pendulum.instance(other.start, tz=other.timezone).timestamp()
-            and self.timezone == other.timezone
-            and self.fmt == other.fmt
-            and self.end_offset == other.end_offset
-            and self.cron_schedule == other.cron_schedule
-        )
-
     def __repr__(self):
         # Between python 3.8 and 3.9 the repr of a datetime object changed.
         # Replaces start time with timestamp as a workaround to make sure the repr is consistent across versions.
         return (
             f"TimeWindowPartitionsDefinition(start={self.start.timestamp()},"
             f" timezone='{self.timezone}', fmt='{self.fmt}', end_offset={self.end_offset},"
             f" cron_schedule='{self.cron_schedule}')"
@@ -555,44 +544,67 @@
         end_time = self.end_time_for_partition_key(partition_key_range.end)
 
         return self.get_partition_keys_in_time_window(TimeWindow(start_time, end_time))
 
     @public
     @property
     def schedule_type(self) -> Optional[ScheduleType]:
+        """Optional[ScheduleType]: An enum representing the partition cadence (hourly, daily,
+        weekly, or monthly).
+        """
         if re.fullmatch(r"\d+ \* \* \* \*", self.cron_schedule):
             return ScheduleType.HOURLY
         elif re.fullmatch(r"\d+ \d+ \* \* \*", self.cron_schedule):
             return ScheduleType.DAILY
         elif re.fullmatch(r"\d+ \d+ \* \* \d+", self.cron_schedule):
             return ScheduleType.WEEKLY
         elif re.fullmatch(r"\d+ \d+ \d+ \* \*", self.cron_schedule):
             return ScheduleType.MONTHLY
         else:
             return None
 
     @public
     @property
     def minute_offset(self) -> int:
+        """int: Number of minutes past the hour to "split" partitions. Defaults to 0.
+
+        For example, returns 15 if each partition starts at 15 minutes past the hour.
+        """
         match = re.fullmatch(r"(\d+) (\d+|\*) (\d+|\*) (\d+|\*) (\d+|\*)", self.cron_schedule)
         if match is None:
             check.failed(f"{self.cron_schedule} has no minute offset")
         return int(match.groups()[0])
 
     @public
     @property
     def hour_offset(self) -> int:
+        """int: Number of hours past 00:00 to "split" partitions. Defaults to 0.
+
+        For example, returns 1 if each partition starts at 01:00.
+        """
         match = re.fullmatch(r"(\d+|\*) (\d+) (\d+|\*) (\d+|\*) (\d+|\*)", self.cron_schedule)
         if match is None:
             check.failed(f"{self.cron_schedule} has no hour offset")
         return int(match.groups()[1])
 
     @public
     @property
     def day_offset(self) -> int:
+        """int: For a weekly or monthly partitions definition, returns the day to "split" partitions
+        by. Each partition will start on this day, and end before this day in the following
+        week/month. Returns 0 if the day_offset parameter is unset in the
+        WeeklyPartitionsDefinition, MonthlyPartitionsDefinition, or the provided cron schedule.
+
+        For weekly partitions, returns a value between 0 (representing Sunday) and 6 (representing
+        Saturday). Providing a value of 1 means that a partition will exist weekly from Monday to
+        the following Sunday.
+
+        For monthly partitions, returns a value between 0 (the first day of the month) and 31 (the
+        last possible day of the month).
+        """
         schedule_type = self.schedule_type
         if schedule_type == ScheduleType.WEEKLY:
             match = re.fullmatch(r"(\d+|\*) (\d+|\*) (\d+|\*) (\d+|\*) (\d+)", self.cron_schedule)
             if match is None:
                 check.failed(f"{self.cron_schedule} has no day offset")
             return int(match.groups()[4])
         elif schedule_type == ScheduleType.MONTHLY:
@@ -1623,21 +1635,20 @@
     @property
     def partitions_def(self) -> PartitionsDefinition:
         return self._partitions_def
 
     def __eq__(self, other):
         return (
             isinstance(other, TimeWindowPartitionsSubset)
-            and self._partitions_def == other._partitions_def  # noqa: SLF001
+            and self._partitions_def == other._partitions_def
             and (
                 # faster comparison, but will not catch all cases
                 (
-                    self._included_time_windows == other._included_time_windows  # noqa: SLF001
-                    and self._included_partition_keys
-                    == other._included_partition_keys  # noqa: SLF001
+                    self._included_time_windows == other._included_time_windows
+                    and self._included_partition_keys == other._included_partition_keys
                 )
                 # slower comparison, catches all cases
                 or self.included_time_windows == other.included_time_windows
             )
         )
 
     def __len__(self) -> int:
@@ -1809,29 +1820,32 @@
             return True
 
     return False
 
 
 def get_time_partitions_def(
     partitions_def: Optional[PartitionsDefinition],
-) -> TimeWindowPartitionsDefinition:
+) -> Optional[TimeWindowPartitionsDefinition]:
+    """For a given PartitionsDefinition, return the associated TimeWindowPartitionsDefinition if it
+    exists.
+    """
     from .multi_dimensional_partitions import MultiPartitionsDefinition
 
     if partitions_def is None:
-        check.failed("Cannot get time partitions def from None object")
+        return None
     elif isinstance(partitions_def, TimeWindowPartitionsDefinition):
         return partitions_def
-    elif isinstance(partitions_def, MultiPartitionsDefinition):
+    elif isinstance(
+        partitions_def, MultiPartitionsDefinition
+    ) and has_one_dimension_time_window_partitioning(partitions_def):
         return cast(
             TimeWindowPartitionsDefinition, partitions_def.time_window_dimension.partitions_def
         )
     else:
-        check.failed(
-            f"Cannot return time partitions def from non-time partitions def {partitions_def}"
-        )
+        return None
 
 
 def get_time_partition_key(
     partitions_def: Optional[PartitionsDefinition], partition_key: Optional[str]
 ) -> str:
     from .multi_dimensional_partitions import MultiPartitionsDefinition
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/unresolved_asset_job_definition.py` & `dagster-1.4.0/dagster/_core/definitions/unresolved_asset_job_definition.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,22 +10,21 @@
 
 from .asset_layer import build_asset_selection_job
 from .config import ConfigMapping
 from .metadata import RawMetadataValue
 
 if TYPE_CHECKING:
     from dagster._core.definitions import (
-        AssetsDefinition,
         AssetSelection,
         ExecutorDefinition,
         HookDefinition,
         JobDefinition,
         PartitionedConfig,
         PartitionsDefinition,
-        SourceAsset,
+        ResourceDefinition,
     )
     from dagster._core.definitions.asset_graph import InternalAssetGraph
     from dagster._core.definitions.asset_selection import CoercibleToAssetSelection
     from dagster._core.definitions.run_config import RunConfig
 
 
 class UnresolvedAssetJobDefinition(
@@ -161,46 +160,21 @@
             tags=run_request_tags,
             asset_selection=asset_selection,
             partition_key=partition_key,
         )
 
     def resolve(
         self,
-        assets: Optional[Sequence["AssetsDefinition"]] = None,
-        source_assets: Optional[Sequence["SourceAsset"]] = None,
+        asset_graph: "InternalAssetGraph",
         default_executor_def: Optional["ExecutorDefinition"] = None,
-        asset_graph: Optional["InternalAssetGraph"] = None,
+        resource_defs: Optional[Mapping[str, "ResourceDefinition"]] = None,
     ) -> "JobDefinition":
-        """Resolve this UnresolvedAssetJobDefinition into a JobDefinition.
-
-        The assets and source_assets arguments are deprecated. Although they were never technically
-        public, a lot of users use them, so going to wait until a minor release to get rid of them.
-        """
-        from dagster._core.definitions.asset_graph import AssetGraph
-
-        if asset_graph is not None:
-            if assets is not None or source_assets is not None:
-                check.failed(
-                    "If providing asset_graph, can't also provide assets and source_assets, and"
-                    " vice-versa."
-                )
-            assets = asset_graph.assets
-            source_assets = asset_graph.source_assets
-        else:
-            if assets is None or source_assets is None:
-                check.failed(
-                    "If asset_graph is not provided, must provide both assets and source_assets"
-                )
-            deprecation_warning(
-                "`assets` and `source_assets` arguments to `resolve`",
-                "1.3.0",
-                "Please use the `asset_graph` argument instead.",
-            )
-            asset_graph = AssetGraph.from_assets([*assets, *source_assets])
-
+        """Resolve this UnresolvedAssetJobDefinition into a JobDefinition."""
+        assets = asset_graph.assets
+        source_assets = asset_graph.source_assets
         selected_asset_keys = self.selection.resolve(asset_graph)
 
         asset_keys_by_partitions_def = defaultdict(set)
         for asset_key in selected_asset_keys:
             partitions_def = asset_graph.get_partitions_def(asset_key)
             if partitions_def is not None:
                 asset_keys_by_partitions_def[partitions_def].add(asset_key)
@@ -240,14 +214,15 @@
             description=self.description,
             tags=self.tags,
             metadata=self.metadata,
             asset_selection=selected_asset_keys,
             partitions_def=self.partitions_def if self.partitions_def else inferred_partitions_def,
             executor_def=self.executor_def or default_executor_def,
             hooks=self.hooks,
+            resource_defs=resource_defs,
         )
 
 
 def define_asset_job(
     name: str,
     selection: Optional["CoercibleToAssetSelection"] = None,
     config: Optional[
@@ -287,15 +262,15 @@
             Describes how the Job is parameterized at runtime.
 
             If no value is provided, then the schema for the job's run config is a standard
             format based on its ops and resources.
 
             If a dictionary is provided, then it must conform to the standard config schema, and
             it will be used as the job's run config for the job whenever the job is executed.
-            The values provided will be viewable and editable in the Dagit playground, so be
+            The values provided will be viewable and editable in the Dagster UI, so be
             careful with secrets.
 
             If a :py:class:`ConfigMapping` object is provided, then the schema for the job's run config is
             determined by the config mapping, and the ConfigMapping, which should return
             configuration in the standard format to configure the job.
         tags (Optional[Mapping[str, Any]]):
             Arbitrary information that will be attached to the execution of the Job.
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/utils.py` & `dagster-1.4.0/dagster/_core/definitions/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -78,15 +78,15 @@
 def is_valid_name(name: str) -> bool:
     check.str_param(name, "name")
 
     return name not in DISALLOWED_NAMES and has_valid_name_chars(name)
 
 
 def _kv_str(key: object, value: object) -> str:
-    return f'{key}="{repr(value)}"'
+    return f'{key}="{value!r}"'
 
 
 def struct_to_string(name: str, **kwargs: object) -> str:
     # Sort the kwargs to ensure consistent representations across Python versions
     props_str = ", ".join([_kv_str(key, value) for key, value in sorted(kwargs.items())])
     return f"{name}({props_str})"
```

### Comparing `dagster-1.3.9rc0/dagster/_core/definitions/version_strategy.py` & `dagster-1.4.0/dagster/_core/definitions/version_strategy.py`

 * *Files 17% similar despite different names*

```diff
@@ -48,18 +48,35 @@
     job will enable memoization on that job, such that only steps whose
     outputs do not have an up-to-date version will run.
     """
 
     @public
     @abstractmethod
     def get_op_version(self, context: OpVersionContext) -> str:
+        """Computes a version for an op.
+
+        Args:
+            context (OpVersionContext): The context for computing the version.
+
+        Returns:
+            str: The version for the op.
+        """
         raise NotImplementedError()
 
     @public
     def get_resource_version(self, context: ResourceVersionContext) -> Optional[str]:
+        """Computes a version for a resource.
+
+        Args:
+            context (ResourceVersionContext): The context for computing the version.
+
+        Returns:
+            Optional[str]: The version for the resource. If None, the resource will not be
+                memoized.
+        """
         return None
 
 
 class SourceHashVersionStrategy(VersionStrategy):
     """VersionStrategy that checks for changes to the source code of ops and resources.
 
     Only checks for changes within the immediate body of the op/resource's
@@ -69,16 +86,33 @@
 
     def _get_source_hash(self, fn):
         code_as_str = inspect.getsource(fn)
         return hashlib.sha1(code_as_str.encode("utf-8")).hexdigest()
 
     @public
     def get_op_version(self, context: OpVersionContext) -> str:
+        """Computes a version for an op by hashing its source code.
+
+        Args:
+            context (OpVersionContext): The context for computing the version.
+
+        Returns:
+            str: The version for the op.
+        """
         compute_fn = context.op_def.compute_fn
         if callable(compute_fn):
             return self._get_source_hash(compute_fn)
         else:
             return self._get_source_hash(compute_fn.decorated_fn)
 
     @public
     def get_resource_version(self, context: ResourceVersionContext) -> Optional[str]:
+        """Computes a version for a resource by hashing its source code.
+
+        Args:
+            context (ResourceVersionContext): The context for computing the version.
+
+        Returns:
+            Optional[str]: The version for the resource. If None, the resource will not be
+                memoized.
+        """
         return self._get_source_hash(context.resource_def.resource_fn)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/errors.py` & `dagster-1.4.0/dagster/_core/errors.py`

 * *Files 1% similar despite different names*

```diff
@@ -99,15 +99,15 @@
     return (
         """
 Error defining Dagster config class{config_class}{field_name}.
 Unable to resolve config type {invalid_type} to a supported Dagster config type.
 
 {PYTHONIC_CONFIG_ERROR_VERBIAGE}"""
     ).format(
-        config_class=f" {repr(config_class)}" if config_class else "",
+        config_class=f" {config_class!r}" if config_class else "",
         field_name=f" on field '{field_name}'" if field_name else "",
         invalid_type=repr(invalid_type),
         PYTHONIC_CONFIG_ERROR_VERBIAGE=pythonic_config_error_verbiage,
     )
 
 
 class DagsterInvalidPythonicConfigDefinitionError(DagsterError):
@@ -661,11 +661,17 @@
 
 
 class DagsterUndefinedDataVersionError(DagsterError):
     """The user attempted to retrieve the most recent logical version for an asset, but no logical version is defined.
     """
 
 
+class DagsterAssetBackfillDataLoadError(DagsterError):
+    """Indicates that an asset backfill is now unloadable. May happen when (1) a code location containing
+    targeted assets is unloadable or (2) and asset or an asset's partitions definition has been removed.
+    """
+
+
 class DagsterDefinitionChangedDeserializationError(DagsterError):
     """Indicates that a stored value can't be deserialized because the definition needed to interpret
     it has changed.
     """
```

### Comparing `dagster-1.3.9rc0/dagster/_core/event_api.py` & `dagster-1.4.0/dagster/_core/event_api.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from datetime import datetime
 from typing import Callable, Mapping, NamedTuple, Optional, Sequence, Union
 
 from typing_extensions import TypeAlias
 
 import dagster._check as check
 from dagster._annotations import PublicAttr
-from dagster._core.definitions.events import AssetKey, AssetMaterialization
+from dagster._core.definitions.events import AssetKey, AssetMaterialization, AssetObservation
 from dagster._core.errors import DagsterInvalidInvocationError
 from dagster._core.events import DagsterEventType
 from dagster._core.events.log import EventLogEntry
 from dagster._serdes import whitelist_for_serdes
 
 EventHandlerFn: TypeAlias = Callable[[EventLogEntry, str], None]
 
@@ -59,14 +59,18 @@
 
         return None
 
     @property
     def asset_materialization(self) -> Optional[AssetMaterialization]:
         return self.event_log_entry.asset_materialization
 
+    @property
+    def asset_observation(self) -> Optional[AssetObservation]:
+        return self.event_log_entry.asset_observation
+
 
 @whitelist_for_serdes
 class EventRecordsFilter(
     NamedTuple(
         "_EventRecordsFilter",
         [
             ("event_type", DagsterEventType),
```

### Comparing `dagster-1.3.9rc0/dagster/_core/events/__init__.py` & `dagster-1.4.0/dagster/_core/events/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -84,33 +84,33 @@
     STEP_INPUT = "STEP_INPUT"
     STEP_FAILURE = "STEP_FAILURE"
     STEP_START = "STEP_START"
     STEP_SUCCESS = "STEP_SUCCESS"
     STEP_SKIPPED = "STEP_SKIPPED"
 
     # The process carrying out step execution is starting/started. Shown as a
-    # marker start/end in Dagit
+    # marker start/end in the Dagster UI.
     STEP_WORKER_STARTING = "STEP_WORKER_STARTING"
     STEP_WORKER_STARTED = "STEP_WORKER_STARTED"
 
     # Resource initialization for execution has started/succeede/failed. Shown
-    # as a marker start/end in Dagit
+    # as a marker start/end in the Dagster UI.
     RESOURCE_INIT_STARTED = "RESOURCE_INIT_STARTED"
     RESOURCE_INIT_SUCCESS = "RESOURCE_INIT_SUCCESS"
     RESOURCE_INIT_FAILURE = "RESOURCE_INIT_FAILURE"
 
     STEP_UP_FOR_RETRY = "STEP_UP_FOR_RETRY"  # "failed" but want to retry
     STEP_RESTARTED = "STEP_RESTARTED"
 
     ASSET_MATERIALIZATION = "ASSET_MATERIALIZATION"
     ASSET_MATERIALIZATION_PLANNED = "ASSET_MATERIALIZATION_PLANNED"
     ASSET_OBSERVATION = "ASSET_OBSERVATION"
     STEP_EXPECTATION_RESULT = "STEP_EXPECTATION_RESULT"
 
-    # We want to display RUN_* events in dagit and in our LogManager output, but in order to
+    # We want to display RUN_* events in the Dagster UI and in our LogManager output, but in order to
     # support backcompat for our storage layer, we need to keep the persisted value to be strings
     # of the form "PIPELINE_*".  We may have user code that pass in the DagsterEventType
     # enum values into storage APIs (like get_event_records, which takes in an EventRecordsFilter).
     RUN_ENQUEUED = "PIPELINE_ENQUEUED"
     RUN_DEQUEUED = "PIPELINE_DEQUEUED"
     RUN_STARTING = "PIPELINE_STARTING"  # Launch is happening, execution hasn't started yet
     RUN_START = "PIPELINE_START"  # Execution has started
@@ -511,70 +511,79 @@
     def event_type(self) -> DagsterEventType:
         """DagsterEventType: The type of this event."""
         return DagsterEventType(self.event_type_value)
 
     @public
     @property
     def is_step_event(self) -> bool:
+        """bool: If this event relates to a specific step."""
         return self.event_type in STEP_EVENTS
 
     @public
     @property
     def is_hook_event(self) -> bool:
+        """bool: If this event relates to the execution of a hook."""
         return self.event_type in HOOK_EVENTS
 
-    @public
     @property
     def is_alert_event(self) -> bool:
         return self.event_type in ALERT_EVENTS
 
     @property
     def step_kind(self) -> "StepKind":
         from dagster._core.execution.plan.step import StepKind
 
         return StepKind(self.step_kind_value)
 
     @public
     @property
     def is_step_success(self) -> bool:
+        """bool: If this event is of type STEP_SUCCESS."""
         return self.event_type == DagsterEventType.STEP_SUCCESS
 
     @public
     @property
     def is_successful_output(self) -> bool:
+        """bool: If this event is of type STEP_OUTPUT."""
         return self.event_type == DagsterEventType.STEP_OUTPUT
 
     @public
     @property
     def is_step_start(self) -> bool:
+        """bool: If this event is of type STEP_START."""
         return self.event_type == DagsterEventType.STEP_START
 
     @public
     @property
     def is_step_failure(self) -> bool:
+        """bool: If this event is of type STEP_FAILURE."""
         return self.event_type == DagsterEventType.STEP_FAILURE
 
     @public
     @property
     def is_resource_init_failure(self) -> bool:
+        """bool: If this event is of type RESOURCE_INIT_FAILURE."""
         return self.event_type == DagsterEventType.RESOURCE_INIT_FAILURE
 
     @public
     @property
     def is_step_skipped(self) -> bool:
+        """bool: If this event is of type STEP_SKIPPED."""
         return self.event_type == DagsterEventType.STEP_SKIPPED
 
     @public
     @property
     def is_step_up_for_retry(self) -> bool:
+        """bool: If this event is of type STEP_UP_FOR_RETRY."""
         return self.event_type == DagsterEventType.STEP_UP_FOR_RETRY
 
     @public
     @property
     def is_step_restarted(self) -> bool:
+        """bool: If this event is of type STEP_RESTARTED."""
         return self.event_type == DagsterEventType.STEP_RESTARTED
 
     @property
     def is_job_success(self) -> bool:
         return self.event_type == DagsterEventType.RUN_SUCCESS
 
     @property
@@ -584,70 +593,86 @@
     @property
     def is_run_failure(self) -> bool:
         return self.event_type == DagsterEventType.RUN_FAILURE
 
     @public
     @property
     def is_failure(self) -> bool:
+        """bool: If this event represents the failure of a run or step."""
         return self.event_type in FAILURE_EVENTS
 
     @property
     def is_job_event(self) -> bool:
         return self.event_type in PIPELINE_EVENTS
 
     @public
     @property
     def is_engine_event(self) -> bool:
+        """bool: If this event is of type ENGINE_EVENT."""
         return self.event_type == DagsterEventType.ENGINE_EVENT
 
     @public
     @property
     def is_handled_output(self) -> bool:
+        """bool: If this event is of type HANDLED_OUTPUT."""
         return self.event_type == DagsterEventType.HANDLED_OUTPUT
 
     @public
     @property
     def is_loaded_input(self) -> bool:
+        """bool: If this event is of type LOADED_INPUT."""
         return self.event_type == DagsterEventType.LOADED_INPUT
 
     @public
     @property
     def is_step_materialization(self) -> bool:
+        """bool: If this event is of type ASSET_MATERIALIZATION."""
         return self.event_type == DagsterEventType.ASSET_MATERIALIZATION
 
     @public
     @property
     def is_expectation_result(self) -> bool:
+        """bool: If this event is of type STEP_EXPECTATION_RESULT."""
         return self.event_type == DagsterEventType.STEP_EXPECTATION_RESULT
 
     @public
     @property
     def is_asset_observation(self) -> bool:
+        """bool: If this event is of type ASSET_OBSERVATION."""
         return self.event_type == DagsterEventType.ASSET_OBSERVATION
 
     @public
     @property
     def is_asset_materialization_planned(self) -> bool:
+        """bool: If this event is of type ASSET_MATERIALIZATION_PLANNED."""
         return self.event_type == DagsterEventType.ASSET_MATERIALIZATION_PLANNED
 
     @public
     @property
     def asset_key(self) -> Optional[AssetKey]:
+        """Optional[AssetKey]: For events that correspond to a specific asset_key / partition
+        (ASSET_MATERIALIZTION, ASSET_OBSERVATION, ASSET_MATERIALIZATION_PLANNED), returns that
+        asset key. Otherwise, returns None.
+        """
         if self.event_type == DagsterEventType.ASSET_MATERIALIZATION:
             return self.step_materialization_data.materialization.asset_key
         elif self.event_type == DagsterEventType.ASSET_OBSERVATION:
             return self.asset_observation_data.asset_observation.asset_key
         elif self.event_type == DagsterEventType.ASSET_MATERIALIZATION_PLANNED:
             return self.asset_materialization_planned_data.asset_key
         else:
             return None
 
     @public
     @property
     def partition(self) -> Optional[str]:
+        """Optional[AssetKey]: For events that correspond to a specific asset_key / partition
+        (ASSET_MATERIALIZTION, ASSET_OBSERVATION, ASSET_MATERIALIZATION_PLANNED), returns that
+        partition. Otherwise, returns None.
+        """
         if self.event_type == DagsterEventType.ASSET_MATERIALIZATION:
             return self.step_materialization_data.materialization.partition
         elif self.event_type == DagsterEventType.ASSET_OBSERVATION:
             return self.asset_observation_data.asset_observation.partition
         elif self.event_type == DagsterEventType.ASSET_MATERIALIZATION_PLANNED:
             return self.asset_materialization_planned_data.partition
         else:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/events/log.py` & `dagster-1.4.0/dagster/_core/events/log.py`

 * *Files 6% similar despite different names*

```diff
@@ -88,18 +88,22 @@
             check.opt_str_param(job_name, "job_name"),
             check.opt_inst_param(dagster_event, "dagster_event", DagsterEvent),
         )
 
     @public
     @property
     def is_dagster_event(self) -> bool:
+        """bool: If this entry contains a DagsterEvent."""
         return bool(self.dagster_event)
 
     @public
     def get_dagster_event(self) -> DagsterEvent:
+        """DagsterEvent: Returns the DagsterEvent contained within this entry. If this entry does not
+        contain a DagsterEvent, an error will be raised.
+        """
         if not isinstance(self.dagster_event, DagsterEvent):
             check.failed(
                 "Not a dagster event, check is_dagster_event before calling get_dagster_event",
             )
 
         return self.dagster_event
 
@@ -108,15 +112,17 @@
 
     @staticmethod
     def from_json(json_str: str):
         return deserialize_value(json_str, EventLogEntry)
 
     @public
     @property
-    def dagster_event_type(self):
+    def dagster_event_type(self) -> Optional[DagsterEventType]:
+        """Optional[DagsterEventType]: The type of the DagsterEvent contained by this entry, if any.
+        """
         return self.dagster_event.event_type if self.dagster_event else None
 
     @public
     @property
     def message(self) -> str:
         """Return the message from the structured DagsterEvent if present, fallback to user_message.
         """
```

### Comparing `dagster-1.3.9rc0/dagster/_core/events/utils.py` & `dagster-1.4.0/dagster/_core/events/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/api.py` & `dagster-1.4.0/dagster/_core/execution/api.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/asset_backfill.py` & `dagster-1.4.0/dagster/_core/execution/asset_backfill.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import json
+import logging
 from datetime import datetime
 from enum import Enum
 from typing import (
     TYPE_CHECKING,
     AbstractSet,
     Dict,
     Iterable,
@@ -27,15 +28,21 @@
 from dagster._core.definitions.asset_selection import AssetSelection
 from dagster._core.definitions.assets_job import is_base_asset_job_name
 from dagster._core.definitions.events import AssetKey, AssetKeyPartitionKey
 from dagster._core.definitions.external_asset_graph import ExternalAssetGraph
 from dagster._core.definitions.partition import PartitionsSubset
 from dagster._core.definitions.run_request import RunRequest
 from dagster._core.definitions.selector import JobSubsetSelector
-from dagster._core.errors import DagsterBackfillFailedError
+from dagster._core.errors import (
+    DagsterAssetBackfillDataLoadError,
+    DagsterBackfillFailedError,
+    DagsterDefinitionChangedDeserializationError,
+    DagsterInvariantViolationError,
+)
+from dagster._core.event_api import EventRecordsFilter
 from dagster._core.events import DagsterEventType
 from dagster._core.host_representation import (
     ExternalExecutionPlan,
     ExternalJob,
 )
 from dagster._core.instance import DagsterInstance, DynamicPartitionsStore
 from dagster._core.storage.dagster_run import (
@@ -44,14 +51,15 @@
     RunsFilter,
 )
 from dagster._core.storage.tags import BACKFILL_ID_TAG, PARTITION_NAME_TAG
 from dagster._core.workspace.context import (
     BaseWorkspaceRequestContext,
     IWorkspaceProcessContext,
 )
+from dagster._core.workspace.workspace import IWorkspace
 from dagster._utils import hash_collection, utc_datetime_from_timestamp
 from dagster._utils.caching_instance_queryer import CachingInstanceQueryer
 
 if TYPE_CHECKING:
     from .backfill import PartitionBackfill
 
 
@@ -413,40 +421,70 @@
                 BACKFILL_ID_TAG: backfill_id,
             }
         )
     )
     return [run.run_id for run in backfill_runs if run.status in CANCELABLE_RUN_STATUSES]
 
 
+def _get_unloadable_location_names(context: IWorkspace, logger: logging.Logger) -> Sequence[str]:
+    location_entries_by_name = {
+        location_entry.origin.location_name: location_entry
+        for location_entry in context.get_workspace_snapshot().values()
+    }
+    unloadable_location_names = []
+
+    for location_name, location_entry in location_entries_by_name.items():
+        if location_entry.load_error:
+            logger.warning(
+                f"Failure loading location {location_name} due to error:"
+                f" {location_entry.load_error}"
+            )
+            unloadable_location_names.append(location_name)
+
+    return unloadable_location_names
+
+
 def execute_asset_backfill_iteration(
     backfill: "PartitionBackfill",
+    logger: logging.Logger,
     workspace_process_context: IWorkspaceProcessContext,
     instance: DagsterInstance,
 ) -> Iterable[None]:
     """Runs an iteration of the backfill, including submitting runs and updating the backfill object
     in the DB.
 
     This is a generator so that we can return control to the daemon and let it heartbeat during
     expensive operations.
     """
     from dagster._core.execution.backfill import BulkActionStatus, PartitionBackfill
 
-    asset_graph = ExternalAssetGraph.from_workspace(
-        workspace_process_context.create_request_context()
-    )
+    workspace_context = workspace_process_context.create_request_context()
+    unloadable_locations = _get_unloadable_location_names(workspace_context, logger)
+    asset_graph = ExternalAssetGraph.from_workspace(workspace_context)
+
     if backfill.serialized_asset_backfill_data is None:
         check.failed("Asset backfill missing serialized_asset_backfill_data")
 
-    asset_backfill_data = AssetBackfillData.from_serialized(
-        backfill.serialized_asset_backfill_data, asset_graph, backfill.backfill_timestamp
-    )
+    try:
+        asset_backfill_data = AssetBackfillData.from_serialized(
+            backfill.serialized_asset_backfill_data, asset_graph, backfill.backfill_timestamp
+        )
+    except DagsterDefinitionChangedDeserializationError as ex:
+        unloadable_locations_error = (
+            "This could be because it's inside a code location that's failing to load:"
+            f" {unloadable_locations}"
+            if unloadable_locations
+            else ""
+        )
+        raise DagsterAssetBackfillDataLoadError(f"{ex}. {unloadable_locations_error}")
+
     backfill_start_time = utc_datetime_from_timestamp(backfill.backfill_timestamp)
 
     instance_queryer = CachingInstanceQueryer(
-        instance=instance, evaluation_time=backfill_start_time
+        instance=instance, asset_graph=asset_graph, evaluation_time=backfill_start_time
     )
 
     if backfill.status == BulkActionStatus.REQUESTED:
         result = None
         for result in execute_asset_backfill_iteration_inner(
             backfill_id=backfill.backfill_id,
             asset_backfill_data=asset_backfill_data,
@@ -677,16 +715,20 @@
     """Returns the partitions that have been materialized by the backfill.
 
     This function is a generator so we can return control to the daemon and let it heartbeat
     during expensive operations.
     """
     recently_materialized_asset_partitions = AssetGraphSubset(asset_graph)
     for asset_key in asset_backfill_data.target_subset.asset_keys:
-        records = instance_queryer.get_materialization_records(
-            asset_key=asset_key, after_cursor=asset_backfill_data.latest_storage_id
+        records = instance_queryer.instance.get_event_records(
+            EventRecordsFilter(
+                event_type=DagsterEventType.ASSET_MATERIALIZATION,
+                asset_key=asset_key,
+                after_cursor=asset_backfill_data.latest_storage_id,
+            )
         )
         records_in_backfill = [
             record
             for record in records
             if instance_queryer.run_has_tag(
                 run_id=record.run_id, tag_key=BACKFILL_ID_TAG, tag_value=backfill_id
             )
@@ -743,17 +785,18 @@
     expensive operations.
     """
     initial_candidates: Set[AssetKeyPartitionKey] = set()
     request_roots = not asset_backfill_data.requested_runs_for_target_roots
     if request_roots:
         initial_candidates.update(asset_backfill_data.get_target_root_asset_partitions())
 
-        next_latest_storage_id = instance_queryer.get_latest_storage_id(
-            DagsterEventType.ASSET_MATERIALIZATION
+        next_latest_storage_id = instance_queryer.get_latest_storage_id_for_event_type(
+            event_type=DagsterEventType.ASSET_MATERIALIZATION
         )
+
         updated_materialized_subset = AssetGraphSubset(asset_graph)
         failed_and_downstream_subset = AssetGraphSubset(asset_graph)
     else:
         target_parent_asset_keys = {
             parent
             for target_asset_key in asset_backfill_data.target_subset.asset_keys
             for parent in asset_graph.get_parents(target_asset_key)
@@ -850,17 +893,26 @@
         if (
             candidate not in target_subset
             or candidate in failed_and_downstream_subset
             or candidate in materialized_subset
         ):
             return False
 
-        for parent in asset_graph.get_parents_partitions(
+        parent_partitions_result = asset_graph.get_parents_partitions(
             dynamic_partitions_store, current_time, *candidate
-        ):
+        )
+
+        if parent_partitions_result.required_but_nonexistent_parents_partitions:
+            raise DagsterInvariantViolationError(
+                f"Asset partition {candidate}"
+                " depends on invalid partition keys"
+                f" {parent_partitions_result.required_but_nonexistent_parents_partitions}"
+            )
+
+        for parent in parent_partitions_result.parent_partitions:
             can_run_with_parent = (
                 parent in asset_partitions_to_request
                 and asset_graph.have_same_partitioning(parent.asset_key, candidate.asset_key)
                 and parent.partition_key == candidate.partition_key
                 and asset_graph.get_repository_handle(candidate.asset_key)
                 is asset_graph.get_repository_handle(parent.asset_key)
             )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/backfill.py` & `dagster-1.4.0/dagster/_core/execution/backfill.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,17 +2,15 @@
 from typing import Mapping, NamedTuple, Optional, Sequence, Union
 
 from dagster import _check as check
 from dagster._core.definitions import AssetKey
 from dagster._core.definitions.asset_graph import AssetGraph
 from dagster._core.definitions.external_asset_graph import ExternalAssetGraph
 from dagster._core.definitions.partition import PartitionsSubset
-from dagster._core.errors import (
-    DagsterDefinitionChangedDeserializationError,
-)
+from dagster._core.errors import DagsterDefinitionChangedDeserializationError
 from dagster._core.execution.bulk_actions import BulkActionType
 from dagster._core.host_representation.origin import ExternalPartitionSetOrigin
 from dagster._core.instance import DynamicPartitionsStore
 from dagster._core.storage.tags import USER_TAG
 from dagster._core.workspace.workspace import IWorkspace
 from dagster._serdes import whitelist_for_serdes
 from dagster._utils import utc_datetime_from_timestamp
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/build_resources.py` & `dagster-1.4.0/dagster/_core/execution/build_resources.py`

 * *Files 6% similar despite different names*

```diff
@@ -111,26 +111,30 @@
         finally:
             list(resources_manager.generate_teardown_events())
 
 
 def wrap_resources_for_execution(
     resources: Optional[Mapping[str, Any]] = None
 ) -> Dict[str, ResourceDefinition]:
+    return (
+        {
+            resource_key: wrap_resource_for_execution(resource)
+            for resource_key, resource in resources.items()
+        }
+        if resources
+        else {}
+    )
+
+
+def wrap_resource_for_execution(resource: Any) -> ResourceDefinition:
     from dagster._config.pythonic_config import ConfigurableResourceFactory, PartialResource
 
-    resources = check.opt_mapping_param(resources, "resources", key_type=str)
-    resource_defs = {}
     # Wrap instantiated resource values in a resource definition.
     # If an instantiated IO manager is provided, wrap it in an IO manager definition.
-    for resource_key, resource in resources.items():
-        # Wrap instantiated resource values in a resource definition.
-        # If an instantiated IO manager is provided, wrap it in an IO manager definition.
-        if isinstance(resource, (ConfigurableResourceFactory, PartialResource)):
-            resource_defs[resource_key] = resource.get_resource_definition()
-        elif isinstance(resource, ResourceDefinition):
-            resource_defs[resource_key] = resource
-        elif isinstance(resource, IOManager):
-            resource_defs[resource_key] = IOManagerDefinition.hardcoded_io_manager(resource)
-        else:
-            resource_defs[resource_key] = ResourceDefinition.hardcoded_resource(resource)
-
-    return resource_defs
+    if isinstance(resource, (ConfigurableResourceFactory, PartialResource)):
+        return resource.get_resource_definition()
+    elif isinstance(resource, ResourceDefinition):
+        return resource
+    elif isinstance(resource, IOManager):
+        return IOManagerDefinition.hardcoded_io_manager(resource)
+    else:
+        return ResourceDefinition.hardcoded_resource(resource)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/compute_logs.py` & `dagster-1.4.0/dagster/_core/execution/compute_logs.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/compute.py` & `dagster-1.4.0/dagster/_core/execution/context/compute.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,25 @@
 from abc import ABC, abstractmethod
-from typing import AbstractSet, Any, Dict, Iterator, List, Mapping, Optional, Sequence, Set, cast
+from typing import (
+    AbstractSet,
+    Any,
+    Dict,
+    Iterator,
+    List,
+    Mapping,
+    Optional,
+    Sequence,
+    Set,
+    cast,
+)
 
 from typing_extensions import TypeAlias
 
 import dagster._check as check
-from dagster._annotations import experimental, public
+from dagster._annotations import deprecated, experimental, public
 from dagster._core.definitions.assets import AssetsDefinition
 from dagster._core.definitions.data_version import (
     DataProvenance,
     extract_data_provenance_from_entry,
 )
 from dagster._core.definitions.dependency import Node, NodeHandle
 from dagster._core.definitions.events import (
@@ -28,15 +39,14 @@
     DagsterInvalidPropertyError,
     DagsterInvariantViolationError,
 )
 from dagster._core.events import DagsterEvent
 from dagster._core.instance import DagsterInstance
 from dagster._core.log_manager import DagsterLogManager
 from dagster._core.storage.dagster_run import DagsterRun
-from dagster._utils.backcompat import deprecation_warning
 from dagster._utils.forked_pdb import ForkedPdb
 
 from .system import StepExecutionContext
 
 
 class AbstractComputeExecutionContext(ABC):
     """Base class for op context implemented by OpExecutionContext and DagstermillExecutionContext.
@@ -83,26 +93,25 @@
     @property
     @abstractmethod
     def op_config(self) -> Any:
         """The parsed config specific to this op."""
 
 
 class OpExecutionContext(AbstractComputeExecutionContext):
-    """The ``context`` object that can be made available as the first argument to an op's compute
-    function.
+    """The ``context`` object that can be made available as the first argument to the function
+    used for computing an op or asset.
+
+    This context object provides system information such as resources, config, and logging.
 
-    The context object provides system information such as resources, config,
-    and logging to an op's compute function. Users should not instantiate this
-    object directly. To construct an `OpExecutionContext` for testing
-    purposes, use :py:func:`dagster.build_op_context`.
+    To construct an execution context for testing purposes, use :py:func:`dagster.build_op_context`.
 
     Example:
         .. code-block:: python
 
-            from dagster import op
+            from dagster import op, OpExecutionContext
 
             @op
             def hello_world(context: OpExecutionContext):
                 context.log.info("Hello, world!")
     """
 
     __slots__ = ["_step_execution_context"]
@@ -116,14 +125,15 @@
         self._pdb: Optional[ForkedPdb] = None
         self._events: List[DagsterEvent] = []
         self._output_metadata: Dict[str, Any] = {}
 
     @public
     @property
     def op_config(self) -> Any:
+        """Any: The parsed config specific to this op."""
         return self._step_execution_context.op_config
 
     @property
     def dagster_run(self) -> DagsterRun:
         """PipelineRun: The current pipeline run."""
         return self._step_execution_context.dagster_run
 
@@ -185,14 +195,15 @@
 
     @public
     @property
     def run_config(self) -> Mapping[str, object]:
         """dict: The run config for the current execution."""
         return self._step_execution_context.run_config
 
+    @public
     @property
     def job_def(self) -> JobDefinition:
         """JobDefinition: The currently executing pipeline."""
         return self._step_execution_context.job_def
 
     @public
     @property
@@ -235,220 +246,59 @@
     @property
     def op_def(self) -> OpDefinition:
         """OpDefinition: The current op definition."""
         return cast(OpDefinition, self.op.definition)
 
     @public
     @property
-    def assets_def(self) -> AssetsDefinition:
-        assets_def = self.job_def.asset_layer.assets_def_for_node(self.node_handle)
-        if assets_def is None:
-            raise DagsterInvalidPropertyError(
-                f"Op '{self.op.name}' does not have an assets definition."
-            )
-        return assets_def
-
-    @public
-    @property
     def has_partition_key(self) -> bool:
         """Whether the current run is a partitioned run."""
         return self._step_execution_context.has_partition_key
 
     @public
     @property
     def partition_key(self) -> str:
         """The partition key for the current run.
 
         Raises an error if the current run is not a partitioned run.
         """
         return self._step_execution_context.partition_key
 
+    @deprecated
     @public
     @property
     def asset_partition_key_range(self) -> PartitionKeyRange:
-        """The asset partition key for the current run.
+        """The range of partition keys for the current run. (DEPRECATED: Use `partition_key_range` instead).
 
-        Raises an error if the current run is not a partitioned run.
+        If run is for a single partition key, return a `PartitionKeyRange` with the same start and
+        end. Raises an error if the current run is not a partitioned run.
+        """
+        return self.partition_key_range
+
+    @public
+    @property
+    def partition_key_range(self) -> PartitionKeyRange:
+        """The range of partition keys for the current run.
+
+        If run is for a single partition key, return a `PartitionKeyRange` with the same start and
+        end. Raises an error if the current run is not a partitioned run.
         """
         return self._step_execution_context.asset_partition_key_range
 
     @public
     @property
     def partition_time_window(self) -> TimeWindow:
         """The partition time window for the current run.
 
         Raises an error if the current run is not a partitioned run, or if the job's partition
         definition is not a TimeWindowPartitionsDefinition.
         """
         return self._step_execution_context.partition_time_window
 
     @public
-    @property
-    def selected_asset_keys(self) -> AbstractSet[AssetKey]:
-        assets_def = self.job_def.asset_layer.assets_def_for_node(self.node_handle)
-        if assets_def is None:
-            return set()
-        return assets_def.keys
-
-    @public
-    @property
-    def selected_output_names(self) -> AbstractSet[str]:
-        # map selected asset keys to the output names they correspond to
-        selected_asset_keys = self.selected_asset_keys
-        selected_outputs: Set[str] = set()
-        for output_name in self.op.output_dict.keys():
-            asset_info = self.job_def.asset_layer.asset_info_for_output(
-                self.node_handle, output_name
-            )
-            if any(  #  For graph-backed assets, check if a downstream asset is selected
-                [
-                    asset_key in selected_asset_keys
-                    for asset_key in self.job_def.asset_layer.downstream_dep_assets(
-                        self.node_handle, output_name
-                    )
-                ]
-            ) or (asset_info and asset_info.key in selected_asset_keys):
-                selected_outputs.add(output_name)
-
-        return selected_outputs
-
-    @public
-    def asset_key_for_output(self, output_name: str = "result") -> AssetKey:
-        asset_output_info = self.job_def.asset_layer.asset_info_for_output(
-            node_handle=self.op_handle, output_name=output_name
-        )
-        if asset_output_info is None:
-            check.failed(f"Output '{output_name}' has no asset")
-        else:
-            return asset_output_info.key
-
-    @public
-    def asset_key_for_input(self, input_name: str) -> AssetKey:
-        key = self.job_def.asset_layer.asset_key_for_input(
-            node_handle=self.op_handle, input_name=input_name
-        )
-        if key is None:
-            check.failed(f"Input '{input_name}' has no asset")
-        else:
-            return key
-
-    def output_asset_partition_key(self, output_name: str = "result") -> str:
-        deprecation_warning(
-            "OpExecutionContext.output_asset_partition_key",
-            "1.0.0",
-            additional_warn_txt="Use OpExecutionContext.asset_partition_key_for_output instead.",
-        )
-
-        return self.asset_partition_key_for_output(output_name)
-
-    @public
-    def asset_partition_key_for_output(self, output_name: str = "result") -> str:
-        """Returns the asset partition key for the given output. Defaults to "result", which is the
-        name of the default output.
-        """
-        return self._step_execution_context.asset_partition_key_for_output(output_name)
-
-    def output_asset_partitions_time_window(self, output_name: str = "result") -> TimeWindow:
-        deprecation_warning(
-            "OpExecutionContext.output_asset_partitions_time_window",
-            "1.0.0",
-            additional_warn_txt=(
-                "Use OpExecutionContext.asset_partitions_time_window_for_output instead."
-            ),
-        )
-
-        return self.asset_partitions_time_window_for_output(output_name)
-
-    @public
-    def asset_partitions_time_window_for_output(self, output_name: str = "result") -> TimeWindow:
-        """The time window for the partitions of the output asset.
-
-        Raises an error if either of the following are true:
-        - The output asset has no partitioning.
-        - The output asset is not partitioned with a TimeWindowPartitionsDefinition or a
-        MultiPartitionsDefinition with one time-partitioned dimension.
-        """
-        return self._step_execution_context.asset_partitions_time_window_for_output(output_name)
-
-    @public
-    def asset_partition_key_range_for_output(
-        self, output_name: str = "result"
-    ) -> PartitionKeyRange:
-        return self._step_execution_context.asset_partition_key_range_for_output(output_name)
-
-    @public
-    def asset_partition_key_range_for_input(self, input_name: str) -> PartitionKeyRange:
-        return self._step_execution_context.asset_partition_key_range_for_input(input_name)
-
-    @public
-    def asset_partition_key_for_input(self, input_name: str) -> str:
-        """Returns the partition key of the upstream asset corresponding to the given input."""
-        return self._step_execution_context.asset_partition_key_for_input(input_name)
-
-    @public
-    def asset_partitions_def_for_output(self, output_name: str = "result") -> PartitionsDefinition:
-        """The PartitionsDefinition on the upstream asset corresponding to this input."""
-        asset_key = self.asset_key_for_output(output_name)
-        result = self._step_execution_context.job_def.asset_layer.partitions_def_for_asset(
-            asset_key
-        )
-        if result is None:
-            raise DagsterInvariantViolationError(
-                f"Attempting to access partitions def for asset {asset_key}, but it is not"
-                " partitioned"
-            )
-
-        return result
-
-    @public
-    def asset_partitions_def_for_input(self, input_name: str) -> PartitionsDefinition:
-        """The PartitionsDefinition on the upstream asset corresponding to this input."""
-        asset_key = self.asset_key_for_input(input_name)
-        result = self._step_execution_context.job_def.asset_layer.partitions_def_for_asset(
-            asset_key
-        )
-        if result is None:
-            raise DagsterInvariantViolationError(
-                f"Attempting to access partitions def for asset {asset_key}, but it is not"
-                " partitioned"
-            )
-
-        return result
-
-    @public
-    def asset_partition_keys_for_output(self, output_name: str = "result") -> Sequence[str]:
-        """Returns a list of the partition keys for the given output."""
-        return self.asset_partitions_def_for_output(output_name).get_partition_keys_in_range(
-            self._step_execution_context.asset_partition_key_range_for_output(output_name),
-            dynamic_partitions_store=self.instance,
-        )
-
-    @public
-    def asset_partition_keys_for_input(self, input_name: str) -> Sequence[str]:
-        """Returns a list of the partition keys of the upstream asset corresponding to the
-        given input.
-        """
-        return list(
-            self._step_execution_context.asset_partitions_subset_for_input(
-                input_name
-            ).get_partition_keys()
-        )
-
-    @public
-    def asset_partitions_time_window_for_input(self, input_name: str = "result") -> TimeWindow:
-        """The time window for the partitions of the input asset.
-
-        Raises an error if either of the following are true:
-        - The input asset has no partitioning.
-        - The input asset is not partitioned with a TimeWindowPartitionsDefinition or a
-        MultiPartitionsDefinition with one time-partitioned dimension.
-        """
-        return self._step_execution_context.asset_partitions_time_window_for_input(input_name)
-
-    @public
     def has_tag(self, key: str) -> bool:
         """Check if a logging tag is set.
 
         Args:
             key (str): The tag to check.
 
         Returns:
@@ -584,14 +434,180 @@
 
     @public
     def get_mapping_key(self) -> Optional[str]:
         """Which mapping_key this execution is for if downstream of a DynamicOutput, otherwise None.
         """
         return self._step_execution_context.step.get_mapping_key()
 
+    #############################################################################################
+    # asset related methods
+    #############################################################################################
+
+    @public
+    @property
+    def assets_def(self) -> AssetsDefinition:
+        """The backing AssetsDefinition for what is currently executing, errors if not available."""
+        assets_def = self.job_def.asset_layer.assets_def_for_node(self.node_handle)
+        if assets_def is None:
+            raise DagsterInvalidPropertyError(
+                f"Op '{self.op.name}' does not have an assets definition."
+            )
+        return assets_def
+
+    @public
+    @property
+    def selected_asset_keys(self) -> AbstractSet[AssetKey]:
+        """Get the set of AssetKeys this execution is expected to materialize."""
+        assets_def = self.job_def.asset_layer.assets_def_for_node(self.node_handle)
+        if assets_def is None:
+            return set()
+        return assets_def.keys
+
+    @public
+    @property
+    def selected_output_names(self) -> AbstractSet[str]:
+        """Get the output names that correspond to the current selection of assets this execution is expected to materialize.
+        """
+        # map selected asset keys to the output names they correspond to
+        selected_asset_keys = self.selected_asset_keys
+        selected_outputs: Set[str] = set()
+        for output_name in self.op.output_dict.keys():
+            asset_info = self.job_def.asset_layer.asset_info_for_output(
+                self.node_handle, output_name
+            )
+            if any(  #  For graph-backed assets, check if a downstream asset is selected
+                [
+                    asset_key in selected_asset_keys
+                    for asset_key in self.job_def.asset_layer.downstream_dep_assets(
+                        self.node_handle, output_name
+                    )
+                ]
+            ) or (asset_info and asset_info.key in selected_asset_keys):
+                selected_outputs.add(output_name)
+
+        return selected_outputs
+
+    @public
+    def asset_key_for_output(self, output_name: str = "result") -> AssetKey:
+        """Return the AssetKey for the corresponding output."""
+        asset_output_info = self.job_def.asset_layer.asset_info_for_output(
+            node_handle=self.op_handle, output_name=output_name
+        )
+        if asset_output_info is None:
+            check.failed(f"Output '{output_name}' has no asset")
+        else:
+            return asset_output_info.key
+
+    @public
+    def asset_key_for_input(self, input_name: str) -> AssetKey:
+        """Return the AssetKey for the corresponding input."""
+        key = self.job_def.asset_layer.asset_key_for_input(
+            node_handle=self.op_handle, input_name=input_name
+        )
+        if key is None:
+            check.failed(f"Input '{input_name}' has no asset")
+        else:
+            return key
+
+    @public
+    def asset_partition_key_for_output(self, output_name: str = "result") -> str:
+        """Returns the asset partition key for the given output. Defaults to "result", which is the
+        name of the default output.
+        """
+        return self._step_execution_context.asset_partition_key_for_output(output_name)
+
+    @public
+    def asset_partitions_time_window_for_output(self, output_name: str = "result") -> TimeWindow:
+        """The time window for the partitions of the output asset.
+
+        Raises an error if either of the following are true:
+        - The output asset has no partitioning.
+        - The output asset is not partitioned with a TimeWindowPartitionsDefinition or a
+        MultiPartitionsDefinition with one time-partitioned dimension.
+        """
+        return self._step_execution_context.asset_partitions_time_window_for_output(output_name)
+
+    @public
+    def asset_partition_key_range_for_output(
+        self, output_name: str = "result"
+    ) -> PartitionKeyRange:
+        """Return the PartitionKeyRange for the corresponding output. Errors if not present."""
+        return self._step_execution_context.asset_partition_key_range_for_output(output_name)
+
+    @public
+    def asset_partition_key_range_for_input(self, input_name: str) -> PartitionKeyRange:
+        """Return the PartitionKeyRange for the corresponding input. Errors if there is more or less than one.
+        """
+        return self._step_execution_context.asset_partition_key_range_for_input(input_name)
+
+    @public
+    def asset_partition_key_for_input(self, input_name: str) -> str:
+        """Returns the partition key of the upstream asset corresponding to the given input."""
+        return self._step_execution_context.asset_partition_key_for_input(input_name)
+
+    @public
+    def asset_partitions_def_for_output(self, output_name: str = "result") -> PartitionsDefinition:
+        """The PartitionsDefinition on the upstream asset corresponding to this input."""
+        asset_key = self.asset_key_for_output(output_name)
+        result = self._step_execution_context.job_def.asset_layer.partitions_def_for_asset(
+            asset_key
+        )
+        if result is None:
+            raise DagsterInvariantViolationError(
+                f"Attempting to access partitions def for asset {asset_key}, but it is not"
+                " partitioned"
+            )
+
+        return result
+
+    @public
+    def asset_partitions_def_for_input(self, input_name: str) -> PartitionsDefinition:
+        """The PartitionsDefinition on the upstream asset corresponding to this input."""
+        asset_key = self.asset_key_for_input(input_name)
+        result = self._step_execution_context.job_def.asset_layer.partitions_def_for_asset(
+            asset_key
+        )
+        if result is None:
+            raise DagsterInvariantViolationError(
+                f"Attempting to access partitions def for asset {asset_key}, but it is not"
+                " partitioned"
+            )
+
+        return result
+
+    @public
+    def asset_partition_keys_for_output(self, output_name: str = "result") -> Sequence[str]:
+        """Returns a list of the partition keys for the given output."""
+        return self.asset_partitions_def_for_output(output_name).get_partition_keys_in_range(
+            self._step_execution_context.asset_partition_key_range_for_output(output_name),
+            dynamic_partitions_store=self.instance,
+        )
+
+    @public
+    def asset_partition_keys_for_input(self, input_name: str) -> Sequence[str]:
+        """Returns a list of the partition keys of the upstream asset corresponding to the
+        given input.
+        """
+        return list(
+            self._step_execution_context.asset_partitions_subset_for_input(
+                input_name
+            ).get_partition_keys()
+        )
+
+    @public
+    def asset_partitions_time_window_for_input(self, input_name: str = "result") -> TimeWindow:
+        """The time window for the partitions of the input asset.
+
+        Raises an error if either of the following are true:
+        - The input asset has no partitioning.
+        - The input asset is not partitioned with a TimeWindowPartitionsDefinition or a
+        MultiPartitionsDefinition with one time-partitioned dimension.
+        """
+        return self._step_execution_context.asset_partitions_time_window_for_input(input_name)
+
     @public
     @experimental
     def get_asset_provenance(self, asset_key: AssetKey) -> Optional[DataProvenance]:
         """Return the provenance information for the most recent materialization of an asset.
 
         Args:
             asset_key (AssetKey): Key of the asset for which to retrieve provenance.
@@ -604,8 +620,12 @@
         record = self.instance.get_latest_data_version_record(asset_key)
 
         return (
             None if record is None else extract_data_provenance_from_entry(record.event_log_entry)
         )
 
 
-SourceAssetObserveContext: TypeAlias = OpExecutionContext
+# actually forking the object type for assets is tricky for users in the cases of:
+#  * manually constructing ops to make AssetsDefinitions
+#  * having ops in a graph that form a graph backed asset
+# so we have a single type that users can call by their preferred name where appropriate
+AssetExecutionContext: TypeAlias = OpExecutionContext
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/hook.py` & `dagster-1.4.0/dagster/_core/execution/context/hook.py`

 * *Files 0% similar despite different names*

```diff
@@ -77,14 +77,15 @@
     def hook_def(self) -> HookDefinition:
         """The hook that the context object belongs to."""
         return self._hook_def
 
     @public
     @property
     def instance(self) -> "DagsterInstance":
+        """The instance configured to run the current job."""
         return self._step_execution_context.instance
 
     @property
     def op(self) -> Node:
         """The op instance associated with the hook."""
         return self._step_execution_context.op
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/init.py` & `dagster-1.4.0/dagster/_core/execution/context/init.py`

 * *Files 4% similar despite different names*

```diff
@@ -58,30 +58,33 @@
     def resource_def(self) -> Optional[ResourceDefinition]:
         """The definition of the resource currently being constructed."""
         return self._resource_def
 
     @public
     @property
     def resources(self) -> Resources:
+        """The resources that are available to the resource that we are initalizing."""
         return self._resources
 
     @public
     @property
     def instance(self) -> Optional[DagsterInstance]:
+        """The Dagster instance configured for the current execution context."""
         return self._instance
 
     @property
     def dagster_run(self) -> Optional[DagsterRun]:
         """The dagster run to use. When initializing resources outside of execution context, this will be None.
         """
         return self._dagster_run
 
     @public
     @property
     def log(self) -> Optional[DagsterLogManager]:
+        """The Dagster log manager configured for the current execution context."""
         return self._log_manager
 
     # backcompat: keep around this property from when InitResourceContext used to be a NamedTuple
     @public
     @property
     def log_manager(self) -> Optional[DagsterLogManager]:
         """The log manager for this run of the job."""
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/input.py` & `dagster-1.4.0/dagster/_core/execution/context/input.py`

 * *Files 1% similar despite different names*

```diff
@@ -33,15 +33,15 @@
     from dagster._core.log_manager import DagsterLogManager
     from dagster._core.types.dagster_type import DagsterType
 
     from .output import OutputContext
 
 
 class InputContext:
-    """The ``context`` object available to the load_input method of :py:class:`RootInputManager`.
+    """The ``context`` object available to the load_input method of :py:class:`InputManager`.
 
     Users should not instantiate this object directly. In order to construct
     an `InputContext` for testing an IO Manager's `load_input` method, use
     :py:func:`dagster.build_input_context`.
 
     Example:
     .. code-block:: python
@@ -225,22 +225,22 @@
             )
 
         return self._log
 
     @public
     @property
     def resource_config(self) -> Optional[Mapping[str, Any]]:
-        """The config associated with the resource that initializes the RootInputManager."""
+        """The config associated with the resource that initializes the InputManager."""
         return self._resource_config
 
     @public
     @property
     def resources(self) -> Any:
         """The resources required by the resource that initializes the
-        input manager. If using the :py:func:`@root_input_manager` decorator, these resources
+        input manager. If using the :py:func:`@input_manager` decorator, these resources
         correspond to those requested with the `required_resource_keys` parameter.
         """
         if self._resources is None:
             raise DagsterInvariantViolationError(
                 "Attempting to access resources, "
                 "but it was not provided when constructing the InputContext"
             )
@@ -252,19 +252,23 @@
                 "open a context manager: `with build_input_context(...) as context:`"
             )
         return self._resources
 
     @public
     @property
     def has_asset_key(self) -> bool:
+        """Returns True if an asset is being loaded as input, otherwise returns False. A return value of False
+        indicates that an output from an op is being loaded as the input.
+        """
         return self._asset_key is not None
 
     @public
     @property
     def asset_key(self) -> AssetKey:
+        """The ``AssetKey`` of the asset that is being loaded as an input."""
         if self._asset_key is None:
             raise DagsterInvariantViolationError(
                 "Attempting to access asset_key, but no asset is associated with this input"
             )
 
         return self._asset_key
 
@@ -315,14 +319,15 @@
             )
 
         return self._partition_key
 
     @public
     @property
     def has_asset_partitions(self) -> bool:
+        """Returns True if the asset being loaded as input is partitioned."""
         return self._asset_partitions_subset is not None
 
     @public
     @property
     def asset_partition_key(self) -> str:
         """The partition key for input asset.
 
@@ -444,14 +449,19 @@
                 "InputContext.upstream_output not defined. Cannot compute an identifier"
             )
 
         return self.upstream_output.get_identifier()
 
     @public
     def get_asset_identifier(self) -> Sequence[str]:
+        """The sequence of strings making up the AssetKey for the asset being loaded as an input.
+        If the asset is partitioned, the identifier contains the partition key as the final element in the
+        sequence. For example, for the asset key ``AssetKey(["foo", "bar", "baz"])``, materialized with
+        partition key "2023-06-01", ``get_asset_identifier`` will return ``["foo", "bar", "baz", "2023-06-01"]``.
+        """
         if self.asset_key is not None:
             if self.has_asset_partitions:
                 return [*self.asset_key.path, self.asset_partition_key]
             else:
                 return self.asset_key.path
         else:
             check.failed("Can't get asset identifier for an input with no asset key")
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/invocation.py` & `dagster-1.4.0/dagster/_core/execution/context/invocation.py`

 * *Files 21% similar despite different names*

```diff
@@ -23,14 +23,15 @@
     ExpectationResult,
     UserEvent,
 )
 from dagster._core.definitions.hook_definition import HookDefinition
 from dagster._core.definitions.job_definition import JobDefinition
 from dagster._core.definitions.multi_dimensional_partitions import MultiPartitionsDefinition
 from dagster._core.definitions.op_definition import OpDefinition
+from dagster._core.definitions.partition_key_range import PartitionKeyRange
 from dagster._core.definitions.resource_definition import (
     IContainsGenerator,
     ResourceDefinition,
     Resources,
     ScopedResourcesBuilder,
 )
 from dagster._core.definitions.resource_requirement import ensure_requirements_satisfied
@@ -71,14 +72,15 @@
     def __init__(
         self,
         op_config: Any,
         resources_dict: Mapping[str, Any],
         resources_config: Mapping[str, Any],
         instance: Optional[DagsterInstance],
         partition_key: Optional[str],
+        partition_key_range: Optional[PartitionKeyRange],
         mapping_key: Optional[str],
         assets_def: Optional[AssetsDefinition],
     ):
         from dagster._core.execution.api import ephemeral_instance_if_missing
         from dagster._core.execution.context_creation_job import initialize_console_manager
 
         self._op_config = op_config
@@ -104,15 +106,20 @@
         )
         self._resources = self._resources_cm.__enter__()
         self._resources_contain_cm = isinstance(self._resources, IContainsGenerator)
 
         self._log = initialize_console_manager(None)
         self._pdb: Optional[ForkedPdb] = None
         self._cm_scope_entered = False
+        check.invariant(
+            not (partition_key and partition_key_range),
+            "Must supply at most one of partition_key or partition_key_range",
+        )
         self._partition_key = partition_key
+        self._partition_key_range = partition_key_range
         self._user_events: List[UserEvent] = []
         self._output_metadata: Dict[str, Any] = {}
 
         self._assets_def = check.opt_inst_param(assets_def, "assets_def", AssetsDefinition)
 
     def __enter__(self):
         self._cm_scope_entered = True
@@ -219,14 +226,28 @@
 
     @property
     def partition_key(self) -> str:
         if self._partition_key:
             return self._partition_key
         check.failed("Tried to access partition_key for a non-partitioned run")
 
+    @property
+    def partition_key_range(self) -> PartitionKeyRange:
+        """The range of partition keys for the current run.
+
+        If run is for a single partition key, return a `PartitionKeyRange` with the same start and
+        end. Raises an error if the current run is not a partitioned run.
+        """
+        if self._partition_key_range:
+            return self._partition_key_range
+        elif self._partition_key:
+            return PartitionKeyRange(self._partition_key, self._partition_key)
+        else:
+            check.failed("Tried to access partition_key range for a non-partitioned run")
+
     def asset_partition_key_for_output(self, output_name: str = "result") -> str:
         return self.partition_key
 
     def has_tag(self, key: str) -> bool:
         raise DagsterInvalidPropertyError(_property_msg("has_tag", "method"))
 
     def get_tag(self, key: str) -> str:
@@ -268,14 +289,15 @@
             alias=op_def_or_invocation.given_alias
             if isinstance(op_def_or_invocation, PendingNodeInvocation)
             else None,
             user_events=self._user_events,
             output_metadata=self._output_metadata,
             mapping_key=self._mapping_key,
             partition_key=self._partition_key,
+            partition_key_range=self._partition_key_range,
             assets_def=self._assets_def,
         )
 
     def get_events(self) -> Sequence[UserEvent]:
         """Retrieve the list of user-generated events that were logged via the context.
 
         **Examples:**
@@ -330,25 +352,27 @@
         """
         return UnboundOpExecutionContext(
             op_config=self._op_config,
             resources_dict=resources_dict,
             resources_config=self._resources_config,
             instance=self._instance,
             partition_key=self._partition_key,
+            partition_key_range=self._partition_key_range,
             mapping_key=self._mapping_key,
             assets_def=self._assets_def,
         )
 
     def replace_config(self, config: Mapping[str, Any]) -> "UnboundOpExecutionContext":
         return UnboundOpExecutionContext(
             op_config=config,
             resources_dict=self._resource_defs,
             resources_config=self._resources_config,
             instance=self._instance,
             partition_key=self._partition_key,
+            partition_key_range=self._partition_key_range,
             mapping_key=self._mapping_key,
             assets_def=self._assets_def,
         )
 
 
 def _validate_resource_requirements(
     resource_defs: Mapping[str, ResourceDefinition], op_def: OpDefinition
@@ -378,14 +402,15 @@
     _hook_defs: Optional[AbstractSet[HookDefinition]]
     _alias: str
     _user_events: List[UserEvent]
     _seen_outputs: Dict[str, Union[str, Set[str]]]
     _output_metadata: Dict[str, Any]
     _mapping_key: Optional[str]
     _partition_key: Optional[str]
+    _partition_key_range: Optional[PartitionKeyRange]
     _assets_def: Optional[AssetsDefinition]
 
     def __init__(
         self,
         op_def: OpDefinition,
         op_config: Any,
         resources: "Resources",
@@ -396,14 +421,15 @@
         tags: Optional[Mapping[str, str]],
         hook_defs: Optional[AbstractSet[HookDefinition]],
         alias: Optional[str],
         user_events: List[UserEvent],
         output_metadata: Dict[str, Any],
         mapping_key: Optional[str],
         partition_key: Optional[str],
+        partition_key_range: Optional[PartitionKeyRange],
         assets_def: Optional[AssetsDefinition],
     ):
         self._op_def = op_def
         self._op_config = op_config
         self._resources = resources
         self._instance = instance
         self._log = log_manager
@@ -413,14 +439,15 @@
         self._alias = alias if alias else self._op_def.name
         self._resources_config = resources_config
         self._user_events = user_events
         self._seen_outputs = {}
         self._output_metadata = output_metadata
         self._mapping_key = mapping_key
         self._partition_key = partition_key
+        self._partition_key_range = partition_key_range
         self._assets_def = assets_def
 
     @property
     def op_config(self) -> Any:
         return self._op_config
 
     @property
@@ -558,14 +585,28 @@
 
     @property
     def partition_key(self) -> str:
         if self._partition_key is not None:
             return self._partition_key
         check.failed("Tried to access partition_key for a non-partitioned asset")
 
+    @property
+    def partition_key_range(self) -> PartitionKeyRange:
+        """The range of partition keys for the current run.
+
+        If run is for a single partition key, return a `PartitionKeyRange` with the same start and
+        end. Raises an error if the current run is not a partitioned run.
+        """
+        if self._partition_key_range:
+            return self._partition_key_range
+        elif self._partition_key:
+            return PartitionKeyRange(self._partition_key, self._partition_key)
+        else:
+            check.failed("Tried to access partition_key range for a non-partitioned run")
+
     def asset_partition_key_for_output(self, output_name: str = "result") -> str:
         return self.partition_key
 
     def asset_partitions_time_window_for_output(self, output_name: str = "result") -> TimeWindow:
         partitions_def = self.assets_def.partitions_def
         if partitions_def is None:
             check.failed("Tried to access partition_key for a non-partitioned asset")
@@ -667,33 +708,36 @@
 def build_op_context(
     resources: Optional[Mapping[str, Any]] = None,
     op_config: Any = None,
     resources_config: Optional[Mapping[str, Any]] = None,
     instance: Optional[DagsterInstance] = None,
     config: Any = None,
     partition_key: Optional[str] = None,
+    partition_key_range: Optional[PartitionKeyRange] = None,
     mapping_key: Optional[str] = None,
     _assets_def: Optional[AssetsDefinition] = None,
 ) -> UnboundOpExecutionContext:
     """Builds op execution context from provided parameters.
 
     ``build_op_context`` can be used as either a function or context manager. If there is a
     provided resource that is a context manager, then ``build_op_context`` must be used as a
     context manager. This function can be used to provide the context argument when directly
     invoking a op.
 
     Args:
         resources (Optional[Dict[str, Any]]): The resources to provide to the context. These can be
             either values or resource definitions.
-        config (Optional[Any]): The op config to provide to the context.
+        op_config (Optional[Mapping[str, Any]]): The config to provide to the op.
+        resources_config (Optional[Mapping[str, Any]]): The config to provide to the resources.
         instance (Optional[DagsterInstance]): The dagster instance configured for the context.
             Defaults to DagsterInstance.ephemeral().
         mapping_key (Optional[str]): A key representing the mapping key from an upstream dynamic
             output. Can be accessed using ``context.get_mapping_key()``.
         partition_key (Optional[str]): String value representing partition key to execute with.
+        partition_key_range (Optional[PartitionKeyRange]): Partition key range to execute with.
         _assets_def (Optional[AssetsDefinition]): Internal argument that populates the op's assets
             definition, not meant to be populated by users.
 
     Examples:
         .. code-block:: python
 
             context = build_op_context()
@@ -713,10 +757,57 @@
         resources_dict=check.opt_mapping_param(resources, "resources", key_type=str),
         resources_config=check.opt_mapping_param(
             resources_config, "resources_config", key_type=str
         ),
         op_config=op_config,
         instance=check.opt_inst_param(instance, "instance", DagsterInstance),
         partition_key=check.opt_str_param(partition_key, "partition_key"),
+        partition_key_range=check.opt_inst_param(
+            partition_key_range, "partition_key_range", PartitionKeyRange
+        ),
         mapping_key=check.opt_str_param(mapping_key, "mapping_key"),
         assets_def=check.opt_inst_param(_assets_def, "_assets_def", AssetsDefinition),
     )
+
+
+def build_asset_context(
+    resources: Optional[Mapping[str, Any]] = None,
+    resources_config: Optional[Mapping[str, Any]] = None,
+    asset_config: Optional[Mapping[str, Any]] = None,
+    instance: Optional[DagsterInstance] = None,
+    partition_key: Optional[str] = None,
+    partition_key_range: Optional[PartitionKeyRange] = None,
+):
+    """Builds asset execution context from provided parameters.
+
+    ``build_asset_context`` can be used as either a function or context manager. If there is a
+    provided resource that is a context manager, then ``build_asset_context`` must be used as a
+    context manager. This function can be used to provide the context argument when directly
+    invoking an asset.
+
+    Args:
+        resources (Optional[Dict[str, Any]]): The resources to provide to the context. These can be
+            either values or resource definitions.
+        resources_config (Optional[Mapping[str, Any]]): The config to provide to the resources.
+        asset_config (Optional[Mapping[str, Any]]): The config to provide to the asset.
+        instance (Optional[DagsterInstance]): The dagster instance configured for the context.
+            Defaults to DagsterInstance.ephemeral().
+        partition_key (Optional[str]): String value representing partition key to execute with.
+        partition_key_range (Optional[PartitionKeyRange]): Partition key range to execute with.
+
+    Examples:
+        .. code-block:: python
+
+            context = build_asset_context()
+            asset_to_invoke(context)
+
+            with build_asset_context(resources={"foo": context_manager_resource}) as context:
+                asset_to_invoke(context)
+    """
+    return build_op_context(
+        op_config=asset_config,
+        resources=resources,
+        resources_config=resources_config,
+        partition_key=partition_key,
+        partition_key_range=partition_key_range,
+        instance=instance,
+    )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/logger.py` & `dagster-1.4.0/dagster/_core/execution/context/logger.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/output.py` & `dagster-1.4.0/dagster/_core/execution/context/output.py`

 * *Files 1% similar despite different names*

```diff
@@ -272,15 +272,15 @@
     def version(self) -> Optional[str]:
         """(Experimental) The version of the output."""
         return self._version
 
     @public
     @property
     def resource_config(self) -> Optional[Mapping[str, object]]:
-        """The config associated with the resource that initializes the RootInputManager."""
+        """The config associated with the resource that initializes the InputManager."""
         return self._resource_config
 
     @public
     @property
     def resources(self) -> Any:
         """The resources required by the output manager, specified by the `required_resource_keys`
         parameter.
@@ -303,19 +303,23 @@
     def asset_info(self) -> Optional[AssetOutputInfo]:
         """(Experimental) Asset info corresponding to the output."""
         return self._asset_info
 
     @public
     @property
     def has_asset_key(self) -> bool:
+        """Returns True if an asset is being stored, otherwise returns False. A return value of False
+        indicates that an output from an op is being stored.
+        """
         return self._asset_info is not None
 
     @public
     @property
     def asset_key(self) -> AssetKey:
+        """The ``AssetKey`` of the asset that is being stored as an output."""
         if self._asset_info is None:
             raise DagsterInvariantViolationError(
                 "Attempting to access asset_key, "
                 "but it was not provided when constructing the OutputContext"
             )
 
         return self._asset_info.key
@@ -387,14 +391,15 @@
             )
 
         return self._partition_key
 
     @public
     @property
     def has_asset_partitions(self) -> bool:
+        """Returns True if the asset being stored is partitioned."""
         if self._warn_on_step_context_use:
             warnings.warn(
                 "You are using InputContext.upstream_output.has_asset_partitions"
                 "This use on upstream_output is deprecated and will fail in the future"
                 "Try to obtain what you need directly from InputContext"
                 "For more details: https://github.com/dagster-io/dagster/issues/7900"
             )
@@ -567,14 +572,19 @@
             "`OutputContext.get_identifier` instead."
         )
 
         return self.get_identifier()
 
     @public
     def get_asset_identifier(self) -> Sequence[str]:
+        """The sequence of strings making up the AssetKey for the asset being stored as an output.
+        If the asset is partitioned, the identifier contains the partition key as the final element in the
+        sequence. For example, for the asset key ``AssetKey(["foo", "bar", "baz"])`` materialized with
+        partition key "2023-06-01", ``get_asset_identifier`` will return ``["foo", "bar", "baz", "2023-06-01"]``.
+        """
         if self.asset_key is not None:
             if self.has_asset_partitions:
                 return [*self.asset_key.path, self.asset_partition_key]
             else:
                 return self.asset_key.path
         else:
             check.failed("Can't get asset output identifier for an output with no asset key")
@@ -739,14 +749,18 @@
     io_manager_key = output_def.io_manager_key
     resource_config = resolved_run_config.resources[io_manager_key].config
 
     node_handle = execution_plan.get_step_by_key(step.key).node_handle
     asset_info = job_def.asset_layer.asset_info_for_output(
         node_handle=node_handle, output_name=step_output.name
     )
+    if asset_info is not None:
+        metadata = job_def.asset_layer.metadata_for_asset(asset_info.key) or output_def.metadata
+    else:
+        metadata = output_def.metadata
 
     if step_context:
         check.invariant(
             not resources,
             (
                 "Expected either resources or step context to be set, but "
                 "received both. If step context is provided, resources for IO manager will be "
@@ -756,15 +770,15 @@
         resources = build_resources_for_manager(io_manager_key, step_context)
 
     return OutputContext(
         step_key=step_output_handle.step_key,
         name=step_output_handle.output_name,
         job_name=job_def.name,
         run_id=run_id,
-        metadata=output_def.metadata,
+        metadata=metadata,
         mapping_key=step_output_handle.mapping_key,
         config=output_config,
         op_def=job_def.get_node(step.node_handle).definition,  # type: ignore  # (should be OpDefinition not NodeDefinition)
         dagster_type=output_def.dagster_type,
         log_manager=log_manager,
         version=version,
         step_context=step_context,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context/system.py` & `dagster-1.4.0/dagster/_core/execution/context/system.py`

 * *Files 1% similar despite different names*

```diff
@@ -559,15 +559,15 @@
         return self.job_def.get_op(self._step.node_handle)
 
     @property
     def op_retry_policy(self) -> Optional[RetryPolicy]:
         return self.job_def.get_retry_policy_for_handle(self.node_handle)
 
     def describe_op(self) -> str:
-        return f'op "{str(self.node_handle)}"'
+        return f'op "{self.node_handle}"'
 
     def get_io_manager(self, step_output_handle: StepOutputHandle) -> IOManager:
         step_output = self.execution_plan.get_step_output(step_output_handle)
         io_manager_key = (
             self.job_def.get_node(step_output.node_handle)
             .output_def_named(step_output.name)
             .io_manager_key
@@ -829,23 +829,26 @@
 
     @property
     def op_config(self) -> Any:
         op_config = self.resolved_run_config.ops.get(str(self.node_handle))
         return op_config.config if op_config else None
 
     @property
-    def step_materializes_assets(self) -> bool:
-        step_outputs = self.step.step_outputs
-        if len(step_outputs) == 0:
-            return False
-        else:
+    def is_sda_step(self) -> bool:
+        """Whether this step corresponds to a software define asset, inferred by presence of asset info on outputs.
+
+        note: ops can materialize assets as well.
+        """
+        for output in self.step.step_outputs:
             asset_info = self.job_def.asset_layer.asset_info_for_output(
-                self.node_handle, step_outputs[0].name
+                self.node_handle, output.name
             )
-            return asset_info is not None
+            if asset_info is not None:
+                return True
+        return False
 
     def set_data_version(self, asset_key: AssetKey, data_version: "DataVersion") -> None:
         self._data_version_cache[asset_key] = data_version
 
     def has_data_version(self, asset_key: AssetKey) -> bool:
         return asset_key in self._data_version_cache
 
@@ -960,20 +963,32 @@
                 partition_mapping = infer_partition_mapping(
                     asset_layer.partition_mapping_for_node_input(
                         self.node_handle, upstream_asset_key
                     ),
                     partitions_def,
                     upstream_asset_partitions_def,
                 )
-                return partition_mapping.get_upstream_partitions_for_partitions(
-                    partitions_subset,
-                    upstream_asset_partitions_def,
-                    dynamic_partitions_store=self.instance,
+                mapped_partitions_result = (
+                    partition_mapping.get_upstream_mapped_partitions_result_for_partitions(
+                        partitions_subset,
+                        upstream_asset_partitions_def,
+                        dynamic_partitions_store=self.instance,
+                    )
                 )
 
+                if mapped_partitions_result.required_but_nonexistent_partition_keys:
+                    raise DagsterInvariantViolationError(
+                        f"Partition key range {self.asset_partition_key_range} in"
+                        f" {self.node_handle.name} depends on invalid partition keys"
+                        f" {mapped_partitions_result.required_but_nonexistent_partition_keys} in"
+                        f" upstream asset {upstream_asset_key}"
+                    )
+
+                return mapped_partitions_result.partitions_subset
+
         check.failed("The input has no asset partitions")
 
     def asset_partition_key_for_input(self, input_name: str) -> str:
         start, end = self.asset_partition_key_range_for_input(input_name)
         if start == end:
             return start
         else:
@@ -987,14 +1002,17 @@
             node_handle=self.node_handle, output_name=output_name
         )
         if asset_info:
             return asset_info.partitions_def
         else:
             return None
 
+    def partitions_def_for_output(self, output_name: str) -> Optional[PartitionsDefinition]:
+        return self._partitions_def_for_output(output_name)
+
     def has_asset_partitions_for_output(self, output_name: str) -> bool:
         return self._partitions_def_for_output(output_name) is not None
 
     def asset_partition_key_range_for_output(self, output_name: str) -> PartitionKeyRange:
         if self._partitions_def_for_output(output_name) is not None:
             return self.asset_partition_key_range
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/context_creation_job.py` & `dagster-1.4.0/dagster/_core/execution/context_creation_job.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/execute_in_process.py` & `dagster-1.4.0/dagster/_core/execution/execute_in_process.py`

 * *Files 2% similar despite different names*

```diff
@@ -121,17 +121,16 @@
     else:
         cur_node_handle = NodeHandle(node.name, parent_handle)
         op_def = cast(OpDefinition, node.definition)
         input_def = op_def.input_def_named(input_name)
         if (
             not input_def.dagster_type.loader
             and not input_def.dagster_type.kind == DagsterTypeKind.NOTHING
-            and not input_def.root_manager_key
             and not input_def.has_default_value
             and not top_level_input_provided
         ):
             raise DagsterInvalidInvocationError(
                 f"Attempted to invoke execute_in_process for '{job_name}' without specifying an"
                 f" input_value for input '{top_level_input_name}', but downstream input"
-                f" {input_def.name} of op '{str(cur_node_handle)}' has no other way of being"
+                f" {input_def.name} of op '{cur_node_handle}' has no other way of being"
                 " loaded."
             )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/execute_in_process_result.py` & `dagster-1.4.0/dagster/_core/execution/execute_in_process_result.py`

 * *Files 9% similar despite different names*

```diff
@@ -46,30 +46,33 @@
         self._output_capture = check.opt_mapping_param(
             output_capture, "output_capture", key_type=StepOutputHandle
         )
 
     @public
     @property
     def job_def(self) -> JobDefinition:
+        """JobDefinition: The job definition that was executed."""
         return self._job_def
 
     @public
     @property
     def dagster_run(self) -> DagsterRun:
+        """DagsterRun: The Dagster run that was executed."""
         return self._dagster_run
 
     @public
     @property
     def all_events(self) -> Sequence[DagsterEvent]:
         """List[DagsterEvent]: All dagster events emitted during execution."""
         return self._event_list
 
     @public
     @property
     def run_id(self) -> str:
+        """str: The run ID of the executed :py:class:`DagsterRun`."""
         return self.dagster_run.run_id
 
     def _get_output_for_handle(self, handle: NodeHandle, output_name: str) -> Any:
         mapped_outputs = {}
         step_key = str(handle)
         output_found = False
         for step_output_handle, value in self._output_capture.items():
@@ -116,14 +119,22 @@
         """
         return super(ExecuteInProcessResult, self).output_for_node(
             node_str, output_name=output_name
         )
 
     @public
     def asset_value(self, asset_key: CoercibleToAssetKey) -> Any:
+        """Retrieves the value of an asset that was materialized during the execution of the job.
+
+        Args:
+            asset_key (CoercibleToAssetKey): The key of the asset to retrieve.
+
+        Returns:
+            Any: The value of the retrieved asset.
+        """
         node_output_handle = self._job_def.asset_layer.node_output_handle_for_asset(
             AssetKey.from_coercible(asset_key)
         )
         return self.output_for_node(
             node_str=str(node_output_handle.node_handle), output_name=node_output_handle.output_name
         )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/execution_result.py` & `dagster-1.4.0/dagster/_core/execution/execution_result.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/host_mode.py` & `dagster-1.4.0/dagster/_core/execution/host_mode.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/job_backfill.py` & `dagster-1.4.0/dagster/_core/execution/job_backfill.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/job_execution_result.py` & `dagster-1.4.0/dagster/_core/execution/job_execution_result.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/memoization.py` & `dagster-1.4.0/dagster/_core/execution/memoization.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/active.py` & `dagster-1.4.0/dagster/_core/execution/plan/active.py`

 * *Files 0% similar despite different names*

```diff
@@ -378,15 +378,15 @@
         steps = []
         steps_to_skip = list(self._pending_skip)
         for key in steps_to_skip:
             step = self.get_step_by_key(key)
             steps.append(step)
             self._in_flight.add(key)
             self._pending_skip.remove(key)
-            self._gathering_dynamic_outputs
+            self._gathering_dynamic_outputs  # noqa: B018
             self._skip_for_dynamic_outputs(step)
 
         return sorted(steps, key=self._sort_key_fn)
 
     def get_steps_to_abandon(self) -> Sequence[ExecutionStep]:
         self._update()
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/compute.py` & `dagster-1.4.0/dagster/_core/execution/plan/compute.py`

 * *Files 1% similar despite different names*

```diff
@@ -194,10 +194,10 @@
         if isinstance(step_output, (DynamicOutput, Output)):
             emitted_result_names.add(step_output.output_name)
 
     op_output_names = {output.name for output in step.step_outputs}
     omitted_outputs = op_output_names.difference(emitted_result_names)
     if omitted_outputs:
         step_context.log.info(
-            f"{step_context.op_def.node_type_str} '{str(step.node_handle)}' did not fire "
-            f"outputs {repr(omitted_outputs)}"
+            f"{step_context.op_def.node_type_str} '{step.node_handle}' did not fire "
+            f"outputs {omitted_outputs!r}"
         )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/compute_generator.py` & `dagster-1.4.0/dagster/_core/execution/plan/compute_generator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/execute_plan.py` & `dagster-1.4.0/dagster/_core/execution/plan/execute_plan.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/execute_step.py` & `dagster-1.4.0/dagster/_core/execution/plan/execute_step.py`

 * *Files 0% similar despite different names*

```diff
@@ -318,15 +318,15 @@
     if step_context.previous_attempt_count > 0:
         yield DagsterEvent.step_restarted_event(step_context, step_context.previous_attempt_count)
     else:
         yield DagsterEvent.step_start_event(step_context)
 
     inputs = {}
 
-    if step_context.step_materializes_assets:
+    if step_context.is_sda_step:
         step_context.fetch_external_input_asset_records()
 
     for step_input in step_context.step.step_inputs:
         input_def = step_context.op_def.input_def_named(step_input.name)
         dagster_type = input_def.dagster_type
 
         if dagster_type.is_nothing:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/external_step.py` & `dagster-1.4.0/dagster/_core/execution/plan/external_step.py`

 * *Files 15% similar despite different names*

```diff
@@ -4,16 +4,17 @@
 import subprocess
 import sys
 from typing import TYPE_CHECKING, Callable, Iterator, Optional, Sequence, cast
 
 import dagster._check as check
 from dagster._config import Field, StringSource
 from dagster._core.code_pointer import FileCodePointer, ModuleCodePointer
+from dagster._core.definitions.partition import DynamicPartitionsDefinition
 from dagster._core.definitions.reconstruct import ReconstructableJob, ReconstructableRepository
-from dagster._core.definitions.resource_definition import resource
+from dagster._core.definitions.resource_definition import dagster_maintained_resource, resource
 from dagster._core.definitions.step_launcher import StepLauncher, StepRunRef
 from dagster._core.errors import raise_execution_interrupts
 from dagster._core.events import DagsterEvent
 from dagster._core.events.log import EventLogEntry
 from dagster._core.execution.api import create_execution_plan
 from dagster._core.execution.context.system import StepExecutionContext
 from dagster._core.execution.context_creation_job import PlanExecutionContextManager
@@ -26,14 +27,15 @@
 PICKLED_EVENTS_FILE_NAME = "events.pkl"
 PICKLED_STEP_RUN_REF_FILE_NAME = "step_run_ref.pkl"
 
 if TYPE_CHECKING:
     from dagster._core.execution.plan.step import ExecutionStep
 
 
+@dagster_maintained_resource
 @resource(
     config_schema={
         "scratch_dir": Field(
             StringSource,
             description="Directory used to pass files between the plan process and step process.",
         ),
     },
@@ -231,10 +233,29 @@
 
 def run_step_from_ref(
     step_run_ref: StepRunRef, instance: DagsterInstance
 ) -> Iterator[DagsterEvent]:
     check.inst_param(instance, "instance", DagsterInstance)
     step_context = step_run_ref_to_step_context(step_run_ref, instance)
 
+    # Note: This is a patch that enables using DynamicPartitionsDefinitions with step launchers in the specific case where:
+    # 1. The external step operates on a single dynamic partition.
+    # 2. No dynamic partitions are added in this external step.
+    # A more complete solution would require including all dynamic partitions on the StepRunRef object.
+    if step_context.has_partition_key:
+        partitions_def = next(
+            step_context.partitions_def_for_output(output_name=output_name)
+            for output_name in step_context.op_def.output_dict.keys()
+        )
+
+        # If we deal with DynamicPartitions, add the relevant partition to the remote instance
+        if (
+            isinstance(partitions_def, DynamicPartitionsDefinition)
+            and partitions_def.name is not None
+        ):
+            step_context.instance.add_dynamic_partitions(
+                partitions_def_name=partitions_def.name, partition_keys=[step_context.partition_key]
+            )
+
     # The step should be forced to run locally with respect to the remote process that this step
     # context is being deserialized in
     return dagster_event_sequence_for_step(step_context, force_local_execution=True)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/handle.py` & `dagster-1.4.0/dagster/_core/execution/plan/handle.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/inputs.py` & `dagster-1.4.0/dagster/_core/execution/plan/inputs.py`

 * *Files 2% similar despite different names*

```diff
@@ -256,53 +256,51 @@
             check.failed(
                 f"Must have an io_manager associated with asset {input_asset_key} to load it using"
                 " FromSourceAsset"
             )
         return {input_manager_key}
 
 
-@whitelist_for_serdes(storage_field_names={"node_handle": "solid_handle"})
-class FromRootInputManager(
+@whitelist_for_serdes(
+    storage_name="FromRootInputManager", storage_field_names={"node_handle": "solid_handle"}
+)
+class FromInputManager(
     NamedTuple(
-        "_FromRootInputManager",
+        "_FromInputManager",
         [
             ("node_handle", NodeHandle),
             ("input_name", str),
         ],
     ),
     StepInputSource,
 ):
-    """Load input value via a RootInputManager."""
+    """Load input value via a InputManager."""
 
     def load_input_object(
         self,
         step_context: "StepExecutionContext",
         input_def: InputDefinition,
     ) -> Iterator[object]:
         from dagster._core.events import DagsterEvent
 
         check.invariant(
             step_context.node_handle == self.node_handle and input_def.name == self.input_name,
             (
-                "RootInputManager source must be op input and not one along composition mapping. "
+                "InputManager source must be op input and not one along composition mapping. "
                 f"Loading for op {step_context.node_handle}.{input_def.name} "
                 f"but source is {self.node_handle}.{self.input_name}."
             ),
         )
 
         input_def = step_context.op_def.input_def_named(input_def.name)
 
         op_config = step_context.resolved_run_config.ops.get(str(self.node_handle))
         config_data = op_config.inputs.get(self.input_name) if op_config else None
 
-        input_manager_key = check.not_none(
-            input_def.root_manager_key
-            if input_def.root_manager_key
-            else input_def.input_manager_key
-        )
+        input_manager_key = check.not_none(input_def.input_manager_key)
 
         loader = getattr(step_context.resources, input_manager_key)
 
         load_input_context = step_context.for_input_manager(
             input_def.name,
             config_data,
             metadata=input_def.metadata,
@@ -328,17 +326,15 @@
         job_def: JobDefinition,
         resolved_run_config: ResolvedRunConfig,
     ) -> Optional[str]:
         from ..resolve_versions import check_valid_version, resolve_config_version
 
         node = job_def.get_node(self.node_handle)
         input_manager_key: str = check.not_none(
-            node.input_def_named(self.input_name).root_manager_key
-            if node.input_def_named(self.input_name).root_manager_key
-            else node.input_def_named(self.input_name).input_manager_key
+            node.input_def_named(self.input_name).input_manager_key
         )
         input_manager_def = job_def.resource_defs[input_manager_key]
 
         op_config = resolved_run_config.ops[node.name]
         input_config = op_config.inputs.get(self.input_name)
         resource_config = check.not_none(
             resolved_run_config.resources.get(input_manager_key)
@@ -346,42 +342,38 @@
 
         version_context = ResourceVersionContext(
             resource_def=input_manager_def,
             resource_config=resource_config,
         )
 
         if job_def.version_strategy is not None:
-            root_manager_def_version = job_def.version_strategy.get_resource_version(
+            input_manager_def_version = job_def.version_strategy.get_resource_version(
                 version_context
             )
         else:
-            root_manager_def_version = input_manager_def.version
+            input_manager_def_version = input_manager_def.version
 
-        if root_manager_def_version is None:
+        if input_manager_def_version is None:
             raise DagsterInvariantViolationError(
                 f"While using memoization, version for input manager '{input_manager_key}' was "
                 "None. Please either provide a versioning strategy for your job, or provide a "
-                "version using the root_input_manager or input_manager decorator."
+                "version using the input_manager decorator."
             )
 
-        check_valid_version(root_manager_def_version)
+        check_valid_version(input_manager_def_version)
         return join_and_hash(
             resolve_config_version(input_config),
             resolve_config_version(resource_config),
-            root_manager_def_version,
+            input_manager_def_version,
         )
 
     def required_resource_keys(self, job_def: JobDefinition) -> Set[str]:
         input_def = job_def.get_node(self.node_handle).input_def_named(self.input_name)
 
-        input_manager_key: str = check.not_none(
-            input_def.root_manager_key
-            if input_def.root_manager_key
-            else input_def.input_manager_key
-        )
+        input_manager_key: str = check.not_none(input_def.input_manager_key)
 
         return {input_manager_key}
 
 
 @whitelist_for_serdes(storage_field_names={"node_handle": "solid_handle"})
 class FromStepOutput(
     NamedTuple(
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/instance_concurrency_context.py` & `dagster-1.4.0/dagster/_core/execution/plan/instance_concurrency_context.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/local_external_step_main.py` & `dagster-1.4.0/dagster/_core/execution/plan/local_external_step_main.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/objects.py` & `dagster-1.4.0/dagster/_core/execution/plan/objects.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/outputs.py` & `dagster-1.4.0/dagster/_core/execution/plan/outputs.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/plan.py` & `dagster-1.4.0/dagster/_core/execution/plan/plan.py`

 * *Files 0% similar despite different names*

```diff
@@ -51,17 +51,17 @@
 from ..resolve_versions import resolve_step_output_versions
 from .compute import create_step_outputs
 from .inputs import (
     FromConfig,
     FromDefaultValue,
     FromDirectInputValue,
     FromDynamicCollect,
+    FromInputManager,
     FromMultipleSources,
     FromPendingDynamicStepOutput,
-    FromRootInputManager,
     FromSourceAsset,
     FromStepOutput,
     FromUnresolvedStepOutput,
     StepInput,
     StepInputSource,
     StepInputSourceUnion,
     StepInputUnion,
@@ -438,16 +438,16 @@
         and
         #  make sure input is unconnected in the outer dependency structure too
         not node.container_maps_input(input_handle.input_name)
     ):
         # can only load from source asset if assets defs are available
         if asset_layer.asset_key_for_input(handle, input_handle.input_name):
             return FromSourceAsset(node_handle=handle, input_name=input_name)
-        elif input_def.root_manager_key or input_def.input_manager_key:
-            return FromRootInputManager(node_handle=handle, input_name=input_name)
+        elif input_def.input_manager_key:
+            return FromInputManager(node_handle=handle, input_name=input_name)
 
     if dependency_structure.has_direct_dep(input_handle):
         node_output_handle = dependency_structure.get_direct_dep(input_handle)
         step_output_handle = step_output_map[node_output_handle]
         if isinstance(step_output_handle, UnresolvedStepOutputHandle):
             return FromUnresolvedStepOutput(
                 unresolved_step_output_handle=step_output_handle,
@@ -1073,15 +1073,15 @@
                     check.inst(cast(StepHandle, step_snap.step_handle), ttype=StepHandle),
                     job_name,
                     step_inputs,  # type: ignore  # (StepInput or UnresolvedCollectStepInput only)
                     step_outputs,
                     step_snap.tags,
                 )
             else:
-                raise Exception(f"Unexpected step kind {str(step_snap.kind)}")
+                raise Exception(f"Unexpected step kind {step_snap.kind}")
 
             step_dict[step.handle] = step
             step_dict_by_key[step.key] = step
 
         step_handles_to_execute = [
             StepHandle.parse_from_key(key) for key in execution_plan_snapshot.step_keys_to_execute
         ]
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/state.py` & `dagster-1.4.0/dagster/_core/execution/plan/state.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/step.py` & `dagster-1.4.0/dagster/_core/execution/plan/step.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/plan/utils.py` & `dagster-1.4.0/dagster/_core/execution/plan/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/poll_compute_logs.py` & `dagster-1.4.0/dagster/_core/execution/poll_compute_logs.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/resolve_versions.py` & `dagster-1.4.0/dagster/_core/execution/resolve_versions.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/resources_init.py` & `dagster-1.4.0/dagster/_core/execution/resources_init.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/retries.py` & `dagster-1.4.0/dagster/_core/execution/retries.py`

 * *Files 0% similar despite different names*

```diff
@@ -24,15 +24,15 @@
     DISABLED = "disabled"
     # Designed for use of inner plan execution within "orchestrator" engine such as multiprocess,
     # up_for_retry steps are not directly re-enqueued, deferring that to the engine.
     DEFERRED = "deferred"
 
     @staticmethod
     def from_config(config_value: Mapping[str, Mapping]) -> Optional["RetryMode"]:
-        for selector, _ in config_value.items():
+        for selector in config_value.keys():
             return RetryMode(selector)
         return None
 
     @property
     def enabled(self) -> bool:
         return self == RetryMode.ENABLED
```

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/run_cancellation_thread.py` & `dagster-1.4.0/dagster/_core/execution/run_cancellation_thread.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/stats.py` & `dagster-1.4.0/dagster/_core/execution/stats.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/tags.py` & `dagster-1.4.0/dagster/_core/execution/tags.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/validate_run_config.py` & `dagster-1.4.0/dagster/_core/execution/validate_run_config.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/watch_orphans.py` & `dagster-1.4.0/dagster/_core/execution/watch_orphans.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/execution/with_resources.py` & `dagster-1.4.0/dagster/_core/execution/with_resources.py`

 * *Files 0% similar despite different names*

```diff
@@ -83,15 +83,15 @@
     for key, resource_def in resource_defs.items():
         if key in resource_config_by_key:
             resource_config = resource_config_by_key[key]
             if not isinstance(resource_config, dict) or "config" not in resource_config:
                 raise DagsterInvalidInvocationError(
                     f"Error with config for resource key '{key}': Expected a "
                     "dictionary of the form {'config': ...}, but received "
-                    f"{str(resource_config)}"
+                    f"{resource_config}"
                 )
 
             outer_config_shape = Shape({"config": resource_def.get_config_field()})
             config_evr = validate_config(outer_config_shape, resource_config)
             if not config_evr.success:
                 raise DagsterInvalidConfigError(
                     f"Error when applying config for resource with key '{key}' ",
```

### Comparing `dagster-1.3.9rc0/dagster/_core/executor/base.py` & `dagster-1.4.0/dagster/_core/executor/base.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/executor/child_process_executor.py` & `dagster-1.4.0/dagster/_core/executor/child_process_executor.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/executor/in_process.py` & `dagster-1.4.0/dagster/_core/executor/in_process.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/executor/init.py` & `dagster-1.4.0/dagster/_core/executor/init.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/executor/multiprocess.py` & `dagster-1.4.0/dagster/_core/executor/multiprocess.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/executor/step_delegating/step_delegating_executor.py` & `dagster-1.4.0/dagster/_core/executor/step_delegating/step_delegating_executor.py`

 * *Files 0% similar despite different names*

```diff
@@ -199,15 +199,15 @@
                         active_execution.mark_interrupted()
                         if not plan_context.instance.run_will_resume(plan_context.run_id):
                             DagsterEvent.engine_event(
                                 plan_context,
                                 "Executor received termination signal, forwarding to steps",
                                 EngineEventData.interrupted(list(running_steps.keys())),
                             )
-                            for _, step in running_steps.items():
+                            for step in running_steps.values():
                                 list(
                                     self._step_handler.terminate_step(
                                         self._get_step_handler_context(
                                             plan_context, [step], active_execution
                                         )
                                     )
                                 )
@@ -260,15 +260,15 @@
                     list(active_execution.plan_events_iterator(plan_context))
 
                     curr_time = pendulum.now("UTC")
                     if (
                         curr_time - last_check_step_health_time
                     ).total_seconds() >= self._check_step_health_interval_seconds:
                         last_check_step_health_time = curr_time
-                        for _, step in running_steps.items():
+                        for step in running_steps.values():
                             step_context = plan_context.for_step(step)
 
                             try:
                                 health_check_result = self._step_handler.check_step_health(
                                     self._get_step_handler_context(
                                         plan_context, [step], active_execution
                                     )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/executor/step_delegating/step_handler/base.py` & `dagster-1.4.0/dagster/_core/executor/step_delegating/step_handler/base.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/__init__.py` & `dagster-1.4.0/dagster/_core/host_representation/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""This subpackage contains all classes that host processes (e.g. dagit)
+"""This subpackage contains all classes that host processes (e.g. dagster-webserver)
 use to manipulate and represent definitions that are resident
 in user processes and containers.  e.g. ExternalPipeline.
 
 It also contains classes that represent historical representations
 that have been persisted. e.g. HistoricalPipeline
 """
 
@@ -48,15 +48,15 @@
     ExternalJobOrigin as ExternalJobOrigin,
     ExternalRepositoryOrigin as ExternalRepositoryOrigin,
     GrpcServerCodeLocationOrigin as GrpcServerCodeLocationOrigin,
     InProcessCodeLocationOrigin as InProcessCodeLocationOrigin,
     ManagedGrpcPythonEnvCodeLocationOrigin as ManagedGrpcPythonEnvCodeLocationOrigin,
 )
 
-# isort: split
+# ruff: isort: split
 from .code_location import (
     CodeLocation as CodeLocation,
     GrpcServerCodeLocation as GrpcServerCodeLocation,
     InProcessCodeLocation as InProcessCodeLocation,
 )
 from .job_index import JobIndex as JobIndex
 from .represented import RepresentedJob as RepresentedJob
```

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/code_location.py` & `dagster-1.4.0/dagster/_core/host_representation/code_location.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/external.py` & `dagster-1.4.0/dagster/_core/host_representation/external.py`

 * *Files 1% similar despite different names*

```diff
@@ -75,15 +75,15 @@
 
 if TYPE_CHECKING:
     from dagster._core.scheduler.instigation import InstigatorState
 
 
 class ExternalRepository:
     """ExternalRepository is a object that represents a loaded repository definition that
-    is resident in another process or container. Host processes such as dagit use
+    is resident in another process or container. Host processes such as dagster-webserver use
     objects such as these to interact with user-defined artifacts.
     """
 
     def __init__(
         self,
         external_repository_data: ExternalRepositoryData,
         repository_handle: RepositoryHandle,
@@ -284,15 +284,15 @@
 
     def get_display_metadata(self) -> Mapping[str, str]:
         return self.handle.display_metadata
 
 
 class ExternalJob(RepresentedJob):
     """ExternalJob is a object that represents a loaded job definition that
-    is resident in another process or container. Host processes such as dagit use
+    is resident in another process or container. Host processes such as dagster-webserver use
     objects such as these to interact with user-defined artifacts.
     """
 
     def __init__(
         self,
         external_job_data: Optional[ExternalJobData],
         repository_handle: RepositoryHandle,
@@ -581,14 +581,18 @@
     def asset_keys_using(self) -> List[AssetKey]:
         return self._external_resource_data.asset_keys_using
 
     @property
     def job_ops_using(self) -> List[ResourceJobUsageEntry]:
         return self._external_resource_data.job_ops_using
 
+    @property
+    def is_dagster_maintained(self) -> bool:
+        return self._external_resource_data.dagster_maintained
+
 
 class ExternalSchedule:
     def __init__(self, external_schedule_data: ExternalScheduleData, handle: RepositoryHandle):
         self._external_schedule_data = check.inst_param(
             external_schedule_data, "external_schedule_data", ExternalScheduleData
         )
         self._handle = InstigatorHandle(
```

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/external_data.py` & `dagster-1.4.0/dagster/_core/host_representation/external_data.py`

 * *Files 0% similar despite different names*

```diff
@@ -32,19 +32,15 @@
     _check as check,
 )
 from dagster._config.pythonic_config import (
     ConfigurableIOManagerFactoryResourceDefinition,
     ConfigurableResourceFactoryResourceDefinition,
     ResourceWithKeyMapping,
 )
-from dagster._config.snap import (
-    ConfigFieldSnap,
-    ConfigSchemaSnapshot,
-    snap_from_config_type,
-)
+from dagster._config.snap import ConfigFieldSnap, ConfigSchemaSnapshot, snap_from_config_type
 from dagster._core.definitions import (
     JobDefinition,
     PartitionsDefinition,
     RepositoryDefinition,
     ScheduleDefinition,
     SourceAsset,
 )
@@ -66,18 +62,15 @@
     MetadataFieldSerializer,
     MetadataUserInput,
     MetadataValue,
     normalize_metadata,
 )
 from dagster._core.definitions.multi_dimensional_partitions import MultiPartitionsDefinition
 from dagster._core.definitions.op_definition import OpDefinition
-from dagster._core.definitions.partition import (
-    DynamicPartitionsDefinition,
-    ScheduleType,
-)
+from dagster._core.definitions.partition import DynamicPartitionsDefinition, ScheduleType
 from dagster._core.definitions.partition_mapping import (
     PartitionMapping,
     get_builtin_partition_mapping_types,
 )
 from dagster._core.definitions.resource_definition import ResourceDefinition
 from dagster._core.definitions.schedule_definition import DefaultScheduleStatus
 from dagster._core.definitions.sensor_definition import (
@@ -470,15 +463,15 @@
         )
 
 
 @whitelist_for_serdes
 class ExternalSensorMetadata(
     NamedTuple("_ExternalSensorMetadata", [("asset_keys", Optional[Sequence[AssetKey]])])
 ):
-    """Stores additional sensor metadata which is available on the Dagit frontend."""
+    """Stores additional sensor metadata which is available in the Dagster UI."""
 
     def __new__(cls, asset_keys: Optional[Sequence[AssetKey]] = None):
         return super(ExternalSensorMetadata, cls).__new__(
             cls,
             asset_keys=check.opt_nullable_sequence_param(
                 asset_keys, "asset_keys", of_type=AssetKey
             ),
@@ -970,14 +963,15 @@
             ("config_schema_snap", ConfigSchemaSnapshot),
             ("nested_resources", Dict[str, NestedResource]),
             ("parent_resources", Dict[str, str]),
             ("resource_type", str),
             ("is_top_level", bool),
             ("asset_keys_using", List[AssetKey]),
             ("job_ops_using", List[ResourceJobUsageEntry]),
+            ("dagster_maintained", bool),
         ],
     )
 ):
     """Serializable data associated with a top-level resource in a Repository, e.g. one bound using the Definitions API.
 
     Includes information about the resource definition and config schema, user-passed values, etc.
     """
@@ -991,14 +985,15 @@
         config_schema_snap: ConfigSchemaSnapshot,
         nested_resources: Optional[Mapping[str, NestedResource]] = None,
         parent_resources: Optional[Mapping[str, str]] = None,
         resource_type: str = UNKNOWN_RESOURCE_TYPE,
         is_top_level: bool = True,
         asset_keys_using: Optional[Sequence[AssetKey]] = None,
         job_ops_using: Optional[Sequence[ResourceJobUsageEntry]] = None,
+        dagster_maintained: bool = False,
     ):
         return super(ExternalResourceData, cls).__new__(
             cls,
             name=check.str_param(name, "name"),
             resource_snapshot=check.inst_param(
                 resource_snapshot, "resource_snapshot", ResourceDefSnap
             ),
@@ -1036,14 +1031,15 @@
             or [],
             job_ops_using=list(
                 check.opt_sequence_param(
                     job_ops_using, "job_ops_using", of_type=ResourceJobUsageEntry
                 )
             )
             or [],
+            dagster_maintained=dagster_maintained,
         )
 
 
 @whitelist_for_serdes(
     storage_field_names={"metadata": "metadata_entries"},
     field_serializers={"metadata": MetadataFieldSerializer},
 )
@@ -1074,14 +1070,15 @@
             ("is_observable", bool),
             # If a set of assets can't be materialized independently from each other, they will all
             # have the same atomic_execution_unit_id. This ID should be stable across reloads and
             # unique deployment-wide.
             ("atomic_execution_unit_id", Optional[str]),
             ("required_top_level_resources", Optional[Sequence[str]]),
             ("auto_materialize_policy", Optional[AutoMaterializePolicy]),
+            ("auto_observe_interval_minutes", Optional[float]),
         ],
     )
 ):
     """A definition of a node in the logical asset graph.
 
     A function for computing the asset and an identifier for that asset.
     """
@@ -1106,14 +1103,15 @@
         group_name: Optional[str] = None,
         freshness_policy: Optional[FreshnessPolicy] = None,
         is_source: Optional[bool] = None,
         is_observable: bool = False,
         atomic_execution_unit_id: Optional[str] = None,
         required_top_level_resources: Optional[Sequence[str]] = None,
         auto_materialize_policy: Optional[AutoMaterializePolicy] = None,
+        auto_observe_interval_minutes: Optional[float] = None,
     ):
         # backcompat logic to handle ExternalAssetNodes serialized without op_names/graph_name
         if not op_names:
             op_names = list(filter(None, [op_name]))
 
         # backcompat logic to handle ExternalAssetNodes serialzied without is_source
         if is_source is None:
@@ -1161,14 +1159,17 @@
                 required_top_level_resources, "required_top_level_resources", of_type=str
             ),
             auto_materialize_policy=check.opt_inst_param(
                 auto_materialize_policy,
                 "auto_materialize_policy",
                 AutoMaterializePolicy,
             ),
+            auto_observe_interval_minutes=check.opt_numeric_param(
+                auto_observe_interval_minutes, "auto_observe_interval_minutes"
+            ),
         )
 
 
 ResourceJobUsageMap = Dict[str, List[ResourceJobUsageEntry]]
 
 
 class NodeHandleResourceUse(NamedTuple):
@@ -1422,14 +1423,15 @@
                     depended_by=list(dep_by[source_asset.key].values()),
                     job_names=job_names,
                     op_description=source_asset.description,
                     metadata=source_asset.metadata,
                     group_name=source_asset.group_name,
                     is_source=True,
                     is_observable=source_asset.is_observable,
+                    auto_observe_interval_minutes=source_asset.auto_observe_interval_minutes,
                     partitions_def_data=external_partitions_definition_from_def(
                         source_asset.partitions_def
                     )
                     if source_asset.partitions_def
                     else None,
                 )
             )
@@ -1632,26 +1634,39 @@
             ConfigurableIOManagerFactoryResourceDefinition,
         ),
     ):
         resource_type = _get_class_name(resource_type_def.configurable_resource_cls)
     else:
         resource_type = _get_class_name(type(resource_type_def))
 
+    dagster_maintained = (
+        resource_type_def._is_dagster_maintained()  # noqa: SLF001
+        if type(resource_type_def)
+        in (
+            ResourceDefinition,
+            IOManagerDefinition,
+            ConfigurableResourceFactoryResourceDefinition,
+            ConfigurableIOManagerFactoryResourceDefinition,
+        )
+        else False
+    )
+
     return ExternalResourceData(
         name=name,
         resource_snapshot=build_resource_def_snap(name, resource_def),
         configured_values=configured_values,
         config_field_snaps=unconfigured_config_type_snap.fields or [],
         config_schema_snap=config_type.get_schema_snapshot(),
         nested_resources=nested_resources,
         parent_resources=parent_resources,
         is_top_level=True,
         asset_keys_using=resource_asset_usage_map.get(name, []),
         job_ops_using=resource_job_usage_map.get(name, []),
         resource_type=resource_type,
+        dagster_maintained=dagster_maintained,
     )
 
 
 def external_schedule_data_from_def(schedule_def: ScheduleDefinition) -> ExternalScheduleData:
     check.inst_param(schedule_def, "schedule_def", ScheduleDefinition)
     return ExternalScheduleData(
         name=schedule_def.name,
@@ -1730,15 +1745,15 @@
 
 def external_dynamic_partitions_definition_from_def(
     partitions_def: DynamicPartitionsDefinition,
 ) -> ExternalDynamicPartitionsDefinitionData:
     check.inst_param(partitions_def, "partitions_def", DynamicPartitionsDefinition)
     if partitions_def.name is None:
         raise DagsterInvalidDefinitionError(
-            "Dagit does not support dynamic partitions definitions without a name parameter."
+            "Dagster does not support dynamic partitions definitions without a name parameter."
         )
     return ExternalDynamicPartitionsDefinitionData(name=partitions_def.name)
 
 
 def external_partition_set_data_from_def(
     job_def: JobDefinition,
 ) -> Optional[ExternalPartitionSetData]:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/grpc_server_registry.py` & `dagster-1.4.0/dagster/_core/host_representation/grpc_server_registry.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/grpc_server_state_subscriber.py` & `dagster-1.4.0/dagster/_core/host_representation/grpc_server_state_subscriber.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/handle.py` & `dagster-1.4.0/dagster/_core/host_representation/handle.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/historical.py` & `dagster-1.4.0/dagster/_core/host_representation/historical.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/job_index.py` & `dagster-1.4.0/dagster/_core/host_representation/job_index.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/origin.py` & `dagster-1.4.0/dagster/_core/host_representation/origin.py`

 * *Files 1% similar despite different names*

```diff
@@ -158,15 +158,15 @@
             ("container_context", Optional[Mapping[str, Any]]),
             ("location_name", str),
         ],
     ),
     CodeLocationOrigin,
 ):
     """Identifies a repository location constructed in the same process. Primarily
-    used in tests, since Dagster system processes like Dagit and the daemon do not
+    used in tests, since Dagster system processes like the webserver and daemon do not
     load user code in the same process.
     """
 
     def __new__(
         cls,
         loadable_target_origin: LoadableTargetOrigin,
         container_image: Optional[str] = None,
@@ -259,23 +259,23 @@
         )
 
     @contextmanager
     def create_single_location(
         self,
         instance: "DagsterInstance",
     ) -> Iterator["GrpcServerCodeLocation"]:
-        from dagster._core.workspace.context import DAGIT_GRPC_SERVER_HEARTBEAT_TTL
+        from dagster._core.workspace.context import WEBSERVER_GRPC_SERVER_HEARTBEAT_TTL
 
         from .code_location import GrpcServerCodeLocation
         from .grpc_server_registry import GrpcServerRegistry
 
         with GrpcServerRegistry(
             instance_ref=instance.get_ref(),
             reload_interval=0,
-            heartbeat_ttl=DAGIT_GRPC_SERVER_HEARTBEAT_TTL,
+            heartbeat_ttl=WEBSERVER_GRPC_SERVER_HEARTBEAT_TTL,
             startup_timeout=(
                 instance.code_server_process_startup_timeout
                 if instance
                 else DEFAULT_LOCAL_CODE_SERVER_STARTUP_TIMEOUT
             ),
             wait_for_processes_on_shutdown=instance.wait_for_local_code_server_processes_on_shutdown,
         ) as grpc_server_registry:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/host_representation/represented.py` & `dagster-1.4.0/dagster/_core/host_representation/represented.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/instance/__init__.py` & `dagster-1.4.0/dagster/_core/instance/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 from typing import (
     TYPE_CHECKING,
     AbstractSet,
     Any,
     Callable,
     Dict,
     Generic,
+    Iterable,
     List,
     Mapping,
     Optional,
     Sequence,
     Set,
     Tuple,
     Type,
@@ -30,14 +31,15 @@
 )
 
 import yaml
 from typing_extensions import Protocol, Self, TypeAlias, TypeVar, runtime_checkable
 
 import dagster._check as check
 from dagster._annotations import public
+from dagster._core.definitions.data_version import extract_data_provenance_from_entry
 from dagster._core.definitions.events import AssetKey
 from dagster._core.errors import (
     DagsterHomeNotSetError,
     DagsterInvalidInvocationError,
     DagsterInvariantViolationError,
     DagsterRunAlreadyExists,
     DagsterRunConflict,
@@ -87,14 +89,15 @@
 IS_AIRFLOW_INGEST_PIPELINE_STR = "is_airflow_ingest_pipeline"
 
 if TYPE_CHECKING:
     from dagster._core.debug import DebugRunPayload
     from dagster._core.definitions.job_definition import (
         JobDefinition,
     )
+    from dagster._core.definitions.partition import PartitionsDefinition
     from dagster._core.definitions.repository_definition.repository_definition import (
         RepositoryLoadData,
     )
     from dagster._core.definitions.run_request import InstigatorType
     from dagster._core.event_api import EventHandlerFn
     from dagster._core.events import DagsterEvent, DagsterEventType, EngineEventData
     from dagster._core.events.log import EventLogEntry
@@ -127,15 +130,18 @@
     from dagster._core.storage.event_log import EventLogStorage
     from dagster._core.storage.event_log.base import (
         AssetRecord,
         EventLogConnection,
         EventLogRecord,
         EventRecordsFilter,
     )
-    from dagster._core.storage.partition_status_cache import AssetStatusCacheValue
+    from dagster._core.storage.partition_status_cache import (
+        AssetPartitionStatus,
+        AssetStatusCacheValue,
+    )
     from dagster._core.storage.root import LocalArtifactStorage
     from dagster._core.storage.runs import RunStorage
     from dagster._core.storage.runs.base import RunGroupInfo
     from dagster._core.storage.schedules import ScheduleStorage
     from dagster._core.storage.sql import AlembicVersion
     from dagster._core.workspace.workspace import IWorkspace
     from dagster._daemon.types import DaemonHeartbeat, DaemonStatus
@@ -194,15 +200,15 @@
                 record=record,
             )
         )
 
         try:
             self._instance.handle_new_event(event)
         except Exception as e:
-            sys.stderr.write(f"Exception while writing logger call to event log: {str(e)}\n")
+            sys.stderr.write(f"Exception while writing logger call to event log: {e}\n")
             if event.dagster_event:
                 # Swallow user-generated log failures so that the entire step/run doesn't fail, but
                 # raise failures writing system-generated log events since they are the source of
                 # truth for the state of the run
                 raise
             elif event.run_id:
                 self._instance.report_engine_event(
@@ -448,14 +454,27 @@
     @public
     @staticmethod
     def ephemeral(
         tempdir: Optional[str] = None,
         preload: Optional[Sequence["DebugRunPayload"]] = None,
         settings: Optional[Dict] = None,
     ) -> "DagsterInstance":
+        """Create a `DagsterInstance` suitable for ephemeral execution, useful in test contexts. An
+        ephemeral instance uses mostly in-memory components. Use `local_temp` to create a test
+        instance that is fully persistent.
+
+        Args:
+            tempdir (Optional[str]): The path of a directory to be used for local artifact storage.
+            preload (Optional[Sequence[DebugRunPayload]]): A sequence of payloads to load into the
+                instance's run storage. Useful for debugging.
+            settings (Optional[Dict]): Settings for the instance.
+
+        Returns:
+            DagsterInstance: An ephemeral DagsterInstance.
+        """
         from dagster._core.launcher.sync_in_memory_run_launcher import SyncInMemoryRunLauncher
         from dagster._core.run_coordinator import DefaultRunCoordinator
         from dagster._core.storage.event_log import InMemoryEventLogStorage
         from dagster._core.storage.noop_compute_log_manager import NoOpComputeLogManager
         from dagster._core.storage.root import LocalArtifactStorage, TemporaryLocalArtifactStorage
         from dagster._core.storage.runs import InMemoryRunStorage
 
@@ -474,14 +493,19 @@
             run_launcher=SyncInMemoryRunLauncher(),
             settings=settings,
         )
 
     @public
     @staticmethod
     def get() -> "DagsterInstance":
+        """Get the current `DagsterInstance` as specified by the ``DAGSTER_HOME`` environment variable.
+
+        Returns:
+            DagsterInstance: The current DagsterInstance.
+        """
         dagster_home_path = os.getenv("DAGSTER_HOME")
 
         if not dagster_home_path:
             raise DagsterHomeNotSetError(
                 "The environment variable $DAGSTER_HOME is not set. \nDagster requires this"
                 " environment variable to be set to an existing directory in your filesystem. This"
                 " directory is used to store metadata across sessions, or load the dagster.yaml"
@@ -516,14 +540,25 @@
 
     @public
     @staticmethod
     def local_temp(
         tempdir: Optional[str] = None,
         overrides: Optional[DagsterInstanceOverrides] = None,
     ) -> "DagsterInstance":
+        """Create a DagsterInstance that uses a temporary directory for local storage. This is a
+        regular, fully persistent instance. Use `ephemeral` to get an ephemeral instance with
+        in-memory components.
+
+        Args:
+            tempdir (Optional[str]): The path of a directory to be used for local artifact storage.
+            overrides (Optional[DagsterInstanceOverrides]): Override settings for the instance.
+
+        Returns:
+            DagsterInstance
+        """
         if tempdir is None:
             created_dir = TemporaryDirectory()
             i = DagsterInstance.from_ref(
                 InstanceRef.from_dir(created_dir.name, overrides=overrides)
             )
             DagsterInstance._TEMP_DIRS[i] = created_dir
             return i
@@ -692,59 +727,53 @@
     def scheduler_class(self) -> Optional[str]:
         return self.scheduler.__class__.__name__ if self.scheduler else None
 
     # run coordinator
 
     @property
     def run_coordinator(self) -> "RunCoordinator":
-        from dagster._core.run_coordinator import RunCoordinator
-
         # Lazily load in case the run coordinator requires dependencies that are not available
         # everywhere that loads the instance
         if not self._run_coordinator:
             check.invariant(
                 self._ref, "Run coordinator not provided, and no instance ref available"
             )
             run_coordinator = cast(InstanceRef, self._ref).run_coordinator
             check.invariant(run_coordinator, "Run coordinator not configured in instance ref")
-            self._run_coordinator = cast(RunCoordinator, run_coordinator)
+            self._run_coordinator = cast("RunCoordinator", run_coordinator)
             self._run_coordinator.register_instance(self)
         return self._run_coordinator
 
     # run launcher
 
     @property
     def run_launcher(self) -> "RunLauncher":
-        from dagster._core.launcher import RunLauncher
-
         # Lazily load in case the launcher requires dependencies that are not available everywhere
         # that loads the instance (e.g. The EcsRunLauncher requires boto3)
         if not self._run_launcher:
             check.invariant(self._ref, "Run launcher not provided, and no instance ref available")
             launcher = cast(InstanceRef, self._ref).run_launcher
             check.invariant(launcher, "Run launcher not configured in instance ref")
-            self._run_launcher = cast(RunLauncher, launcher)
+            self._run_launcher = cast("RunLauncher", launcher)
             self._run_launcher.register_instance(self)
         return self._run_launcher
 
     # compute logs
 
     @property
     def compute_log_manager(self) -> "ComputeLogManager":
-        from dagster._core.storage.compute_log_manager import ComputeLogManager
-
         if not self._compute_log_manager:
             check.invariant(
                 self._ref, "Compute log manager not provided, and no instance ref available"
             )
             compute_log_manager = cast(InstanceRef, self._ref).compute_log_manager
             check.invariant(
                 compute_log_manager, "Compute log manager not configured in instance ref"
             )
-            self._compute_log_manager = cast(ComputeLogManager, compute_log_manager)
+            self._compute_log_manager = cast("ComputeLogManager", compute_log_manager)
             self._compute_log_manager.register_instance(self)
         return self._compute_log_manager
 
     def get_settings(self, settings_key: str) -> Any:
         check.str_param(settings_key, "settings_key")
         if self._settings and settings_key in self._settings:
             return self._settings.get(settings_key)
@@ -883,23 +912,23 @@
             self._event_storage.reindex_assets(print_fn=print_fn)
 
             if print_fn:
                 print_fn("Updating schedule storage...")
             self._schedule_storage.upgrade()  # type: ignore  # (possible none)
             self._schedule_storage.migrate(print_fn)  # type: ignore  # (possible none)
 
-    def optimize_for_dagit(self, statement_timeout: int, pool_recycle: int) -> None:
+    def optimize_for_webserver(self, statement_timeout: int, pool_recycle: int) -> None:
         if self._schedule_storage:
-            self._schedule_storage.optimize_for_dagit(
+            self._schedule_storage.optimize_for_webserver(
                 statement_timeout=statement_timeout, pool_recycle=pool_recycle
             )
-        self._run_storage.optimize_for_dagit(
+        self._run_storage.optimize_for_webserver(
             statement_timeout=statement_timeout, pool_recycle=pool_recycle
         )
-        self._event_storage.optimize_for_dagit(
+        self._event_storage.optimize_for_webserver(
             statement_timeout=statement_timeout, pool_recycle=pool_recycle
         )
 
     def reindex(self, print_fn: PrintFn = lambda _: None) -> None:
         print_fn("Checking for reindexing...")
         self._event_storage.reindex_events(print_fn)
         self._event_storage.reindex_assets(print_fn)
@@ -923,22 +952,40 @@
         if self in DagsterInstance._TEMP_DIRS:
             DagsterInstance._TEMP_DIRS[self].cleanup()
             del DagsterInstance._TEMP_DIRS[self]
 
     # run storage
     @public
     def get_run_by_id(self, run_id: str) -> Optional[DagsterRun]:
+        """Get a :py:class:`DagsterRun` matching the provided `run_id`.
+
+        Args:
+            run_id (str): The id of the run to retrieve.
+
+        Returns:
+            Optional[DagsterRun]: The run corresponding to the given id. If no run matching the id
+                is found, return `None`.
+        """
         record = self.get_run_record_by_id(run_id)
         if record is None:
             return None
         return record.dagster_run
 
     @public
     @traced
     def get_run_record_by_id(self, run_id: str) -> Optional[RunRecord]:
+        """Get a :py:class:`RunRecord` matching the provided `run_id`.
+
+        Args:
+            run_id (str): The id of the run record to retrieve.
+
+        Returns:
+            Optional[RunRecord]: The run record corresponding to the given id. If no run matching
+                the id is found, return `None`.
+        """
         records = self._run_storage.get_run_records(RunsFilter(run_ids=[run_id]))
         if not records:
             return None
         return records[0]
 
     @traced
     def get_job_snapshot(self, snapshot_id: str) -> "JobSnapshot":
@@ -1660,14 +1707,19 @@
     def wipe(self) -> None:
         self._run_storage.wipe()
         self._event_storage.wipe()
 
     @public
     @traced
     def delete_run(self, run_id: str) -> None:
+        """Delete a run and all events generated by that from storage.
+
+        Args:
+            run_id (str): The id of the run to delete.
+        """
         self._run_storage.delete_run(run_id)
         self._event_storage.delete_events(run_id)
 
     # event storage
     @traced
     def logs_after(
         self,
@@ -1734,30 +1786,54 @@
     @traced
     def get_asset_keys(
         self,
         prefix: Optional[Sequence[str]] = None,
         limit: Optional[int] = None,
         cursor: Optional[str] = None,
     ) -> Sequence[AssetKey]:
+        """Return a filtered subset of asset keys managed by this instance.
+
+        Args:
+            prefix (Optional[Sequence[str]]): Return only assets having this key prefix.
+            limit (Optional[int]): Maximum number of keys to return.
+            cursor (Optional[str]): Cursor to use for pagination.
+
+        Returns:
+            Sequence[AssetKey]: List of asset keys.
+        """
         return self._event_storage.get_asset_keys(prefix=prefix, limit=limit, cursor=cursor)
 
     @public
     @traced
     def has_asset_key(self, asset_key: AssetKey) -> bool:
+        """Return true if this instance manages the given asset key.
+
+        Args:
+            asset_key (AssetKey): Asset key to check.
+        """
         return self._event_storage.has_asset_key(asset_key)
 
     @traced
     def get_latest_materialization_events(
-        self, asset_keys: Sequence[AssetKey]
+        self, asset_keys: Iterable[AssetKey]
     ) -> Mapping[AssetKey, Optional["EventLogEntry"]]:
         return self._event_storage.get_latest_materialization_events(asset_keys)
 
     @public
     @traced
     def get_latest_materialization_event(self, asset_key: AssetKey) -> Optional["EventLogEntry"]:
+        """Fetch the latest materialization event for the given asset key.
+
+        Args:
+            asset_key (AssetKey): Asset key to return materialization for.
+
+        Returns:
+            Optional[AssetMaterialization]: The latest materialization event for the given asset
+                key, or `None` if the asset has not been materialized.
+        """
         return self._event_storage.get_latest_materialization_events([asset_key]).get(asset_key)
 
     @public
     @traced
     def get_event_records(
         self,
         event_records_filter: "EventRecordsFilter",
@@ -1776,17 +1852,76 @@
         Returns:
             List[EventLogRecord]: List of event log records stored in the event log storage.
         """
         return self._event_storage.get_event_records(event_records_filter, limit, ascending)
 
     @public
     @traced
+    def get_status_by_partition(
+        self,
+        asset_key: AssetKey,
+        partition_keys: Sequence[str],
+        partitions_def: PartitionsDefinition,
+    ) -> Optional[Mapping[str, AssetPartitionStatus]]:
+        """Get the current status of provided partition_keys for the provided asset.
+
+        Args:
+            asset_key (AssetKey): The asset to get per-partition status for.
+            partition_keys (Sequence[str]): The partitions to get status for.
+            partitions_def (PartitionsDefinition): The PartitionsDefinition of the asset to get
+                per-partition status for.
+
+        Returns:
+            Optional[Mapping[str, AssetPartitionStatus]]: status for each partition key
+
+        """
+        from dagster._core.storage.partition_status_cache import (
+            AssetPartitionStatus,
+            AssetStatusCacheValue,
+            get_and_update_asset_status_cache_value,
+        )
+
+        cached_value = get_and_update_asset_status_cache_value(self, asset_key, partitions_def)
+
+        if isinstance(cached_value, AssetStatusCacheValue):
+            materialized_partitions = cached_value.deserialize_materialized_partition_subsets(
+                partitions_def
+            )
+            failed_partitions = cached_value.deserialize_failed_partition_subsets(partitions_def)
+            in_progress_partitions = cached_value.deserialize_in_progress_partition_subsets(
+                partitions_def
+            )
+
+            status_by_partition = {}
+
+            for partition_key in partition_keys:
+                if partition_key in in_progress_partitions:
+                    status_by_partition[partition_key] = AssetPartitionStatus.IN_PROGRESS
+                elif partition_key in failed_partitions:
+                    status_by_partition[partition_key] = AssetPartitionStatus.FAILED
+                elif partition_key in materialized_partitions:
+                    status_by_partition[partition_key] = AssetPartitionStatus.MATERIALIZED
+                else:
+                    status_by_partition[partition_key] = None
+
+            return status_by_partition
+
+    @public
+    @traced
     def get_asset_records(
         self, asset_keys: Optional[Sequence[AssetKey]] = None
     ) -> Sequence["AssetRecord"]:
+        """Return an `AssetRecord` for each of the given asset keys.
+
+        Args:
+            asset_keys (Optional[Sequence[AssetKey]]): List of asset keys to retrieve records for.
+
+        Returns:
+            Sequence[AssetRecord]: List of asset records.
+        """
         return self._event_storage.get_asset_records(asset_keys)
 
     @traced
     def get_event_tags_for_asset(
         self,
         asset_key: AssetKey,
         filter_tags: Optional[Mapping[str, str]] = None,
@@ -1810,37 +1945,61 @@
     def run_ids_for_asset_key(self, asset_key: AssetKey) -> Sequence[str]:
         check.inst_param(asset_key, "asset_key", AssetKey)
         return self._event_storage.get_asset_run_ids(asset_key)
 
     @public
     @traced
     def wipe_assets(self, asset_keys: Sequence[AssetKey]) -> None:
+        """Wipes asset event history from the event log for the given asset keys.
+
+        Args:
+            asset_keys (Sequence[AssetKey]): Asset keys to wipe.
+        """
         check.list_param(asset_keys, "asset_keys", of_type=AssetKey)
         for asset_key in asset_keys:
             self._event_storage.wipe_asset(asset_key)
 
     @traced
     def get_materialization_count_by_partition(
         self, asset_keys: Sequence[AssetKey], after_cursor: Optional[int] = None
     ) -> Mapping[AssetKey, Mapping[str, int]]:
         return self._event_storage.get_materialization_count_by_partition(asset_keys, after_cursor)
 
+    @traced
+    def get_latest_storage_id_by_partition(
+        self, asset_key: AssetKey, event_type: DagsterEventType
+    ) -> Mapping[str, int]:
+        """Fetch the latest materialzation storage id for each partition for a given asset key.
+
+        Returns a mapping of partition to storage id.
+        """
+        return self._event_storage.get_latest_storage_id_by_partition(asset_key, event_type)
+
     @public
     @traced
     def get_dynamic_partitions(self, partitions_def_name: str) -> Sequence[str]:
+        """Get the set of partition keys for the specified :py:class:`DynamicPartitionsDefinition`.
+
+        Args:
+            partitions_def_name (str): The name of the `DynamicPartitionsDefinition`.
+        """
         check.str_param(partitions_def_name, "partitions_def_name")
         return self._event_storage.get_dynamic_partitions(partitions_def_name)
 
     @public
     @traced
     def add_dynamic_partitions(
         self, partitions_def_name: str, partition_keys: Sequence[str]
     ) -> None:
-        """Add partitions to the specified dynamic partitions definition idempotently.
+        """Add partitions to the specified :py:class:`DynamicPartitionsDefinition` idempotently.
         Does not add any partitions that already exist.
+
+        Args:
+            partitions_def_name (str): The name of the `DynamicPartitionsDefinition`.
+            partition_keys (Sequence[str]): Partition keys to add.
         """
         from dagster._core.definitions.partition import (
             raise_error_on_invalid_partition_key_substring,
         )
 
         check.str_param(partitions_def_name, "partitions_def_name")
         check.sequence_param(partition_keys, "partition_keys", of_type=str)
@@ -1849,25 +2008,34 @@
             raise DagsterInvalidInvocationError("partition_keys must be a sequence of strings")
         raise_error_on_invalid_partition_key_substring(partition_keys)
         return self._event_storage.add_dynamic_partitions(partitions_def_name, partition_keys)
 
     @public
     @traced
     def delete_dynamic_partition(self, partitions_def_name: str, partition_key: str) -> None:
-        """Delete a partition for the specified dynamic partitions definition.
+        """Delete a partition for the specified :py:class:`DynamicPartitionsDefinition`.
         If the partition does not exist, exits silently.
+
+        Args:
+            partitions_def_name (str): The name of the `DynamicPartitionsDefinition`.
+            partition_key (Sequence[str]): Partition key to delete.
         """
         check.str_param(partitions_def_name, "partitions_def_name")
         check.sequence_param(partition_key, "partition_key", of_type=str)
         self._event_storage.delete_dynamic_partition(partitions_def_name, partition_key)
 
     @public
     @traced
     def has_dynamic_partition(self, partitions_def_name: str, partition_key: str) -> bool:
-        """Checks if a partition key exists for the dynamic partitions definition."""
+        """Check if a partition key exists for the :py:class:`DynamicPartitionsDefinition`.
+
+        Args:
+            partitions_def_name (str): The name of the `DynamicPartitionsDefinition`.
+            partition_key (Sequence[str]): Partition key to check.
+        """
         check.str_param(partitions_def_name, "partitions_def_name")
         check.str_param(partition_key, "partition_key")
         return self._event_storage.has_dynamic_partition(partitions_def_name, partition_key)
 
     # event subscriptions
 
     def _get_yaml_python_handlers(self) -> Sequence[logging.Handler]:
@@ -2580,7 +2748,36 @@
                     asset_key=key,
                 ),
                 limit=1,
             )
             materialization = next(iter(materializations), None)
 
         return materialization or observation
+
+    @public
+    def get_latest_materialization_code_versions(
+        self, asset_keys: Iterable[AssetKey]
+    ) -> Mapping[AssetKey, Optional[str]]:
+        """Returns the code version used for the latest materialization of each of the provided
+        assets.
+
+        Args:
+            asset_keys (Iterable[AssetKey]): The asset keys to find latest materialization code
+                versions for.
+
+        Returns:
+            Mapping[AssetKey, Optional[str]]: A dictionary with a key for each of the provided asset
+                keys. The values will be None if the asset has no materializations. If an asset does
+                not have a code version explicitly assigned to its definitions, but was
+                materialized, Dagster assigns the run ID as its code version.
+        """
+        result: Dict[AssetKey, Optional[str]] = {}
+        latest_materialization_events = self.get_latest_materialization_events(asset_keys)
+        for asset_key in asset_keys:
+            event_log_entry = latest_materialization_events.get(asset_key)
+            if event_log_entry is None:
+                result[asset_key] = None
+            else:
+                data_provenance = extract_data_provenance_from_entry(event_log_entry)
+                result[asset_key] = data_provenance.code_version if data_provenance else None
+
+        return result
```

### Comparing `dagster-1.3.9rc0/dagster/_core/instance/config.py` & `dagster-1.4.0/dagster/_core/instance/config.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/instance/ref.py` & `dagster-1.4.0/dagster/_core/instance/ref.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/instance_for_test.py` & `dagster-1.4.0/dagster/_core/instance_for_test.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/launcher/base.py` & `dagster-1.4.0/dagster/_core/launcher/base.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/launcher/default_run_launcher.py` & `dagster-1.4.0/dagster/_core/launcher/default_run_launcher.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/launcher/sync_in_memory_run_launcher.py` & `dagster-1.4.0/dagster/_core/launcher/sync_in_memory_run_launcher.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/log_manager.py` & `dagster-1.4.0/dagster/_core/log_manager.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/nux.py` & `dagster-1.4.0/dagster/_core/nux.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/origin.py` & `dagster-1.4.0/dagster/_core/origin.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/run_coordinator/base.py` & `dagster-1.4.0/dagster/_core/run_coordinator/base.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/run_coordinator/default_run_coordinator.py` & `dagster-1.4.0/dagster/_core/run_coordinator/default_run_coordinator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/run_coordinator/queued_run_coordinator.py` & `dagster-1.4.0/dagster/_core/run_coordinator/queued_run_coordinator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/scheduler/__init__.py` & `dagster-1.4.0/dagster/_core/scheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/scheduler/execution.py` & `dagster-1.4.0/dagster/_core/scheduler/execution.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/scheduler/instigation.py` & `dagster-1.4.0/dagster/_core/scheduler/instigation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 from enum import Enum
-from typing import List, NamedTuple, Optional, Sequence, Union
+from typing import Any, List, NamedTuple, Optional, Sequence, Union
 
 from typing_extensions import TypeAlias
 
 import dagster._check as check
 from dagster._core.definitions.asset_reconciliation_sensor import AutoMaterializeAssetEvaluation
 
 # re-export
@@ -105,32 +105,42 @@
         )
 
 
 @whitelist_for_serdes(old_storage_names={"ScheduleJobData"})
 class ScheduleInstigatorData(
     NamedTuple(
         "_ScheduleInstigatorData",
-        [("cron_schedule", Union[str, Sequence[str]]), ("start_timestamp", Optional[float])],
+        [
+            ("cron_schedule", Union[str, Sequence[str]]),
+            ("start_timestamp", Optional[float]),
+            ("last_iteration_timestamp", Optional[float]),
+        ],
     )
 ):
     # removed scheduler, 1/5/2022 (0.13.13)
     def __new__(
-        cls, cron_schedule: Union[str, Sequence[str]], start_timestamp: Optional[float] = None
+        cls,
+        cron_schedule: Union[str, Sequence[str]],
+        start_timestamp: Optional[float] = None,
+        last_iteration_timestamp: Optional[float] = None,
     ):
         cron_schedule = check.inst_param(cron_schedule, "cron_schedule", (str, list))
         if not isinstance(cron_schedule, str):
             cron_schedule = check.sequence_param(cron_schedule, "cron_schedule", of_type=str)
 
         return super(ScheduleInstigatorData, cls).__new__(
             cls,
             cron_schedule,
             # Time in UTC at which the user started running the schedule (distinct from
             # `start_date` on partition-based schedules, which is used to define
             # the range of partitions)
             check.opt_float_param(start_timestamp, "start_timestamp"),
+            # Time in UTC at which the schedule was last evaluated.  This enables the cron schedule
+            # to change for running schedules and the previous iteration is not backfilled.
+            check.opt_float_param(last_iteration_timestamp, "last_iteration_timestamp"),
         )
 
 
 def check_instigator_data(
     instigator_type: InstigatorType,
     instigator_data: Optional[InstigatorData],
 ) -> Optional[InstigatorData]:
@@ -255,15 +265,15 @@
     def __new__(cls, tick_id: int, tick_data: "TickData"):
         return super(InstigatorTick, cls).__new__(
             cls,
             check.int_param(tick_id, "tick_id"),
             check.inst_param(tick_data, "tick_data", TickData),
         )
 
-    def with_status(self, status: TickStatus, **kwargs: object):
+    def with_status(self, status: TickStatus, **kwargs: Any):
         check.inst_param(status, "status", TickStatus)
         return self._replace(tick_data=self.tick_data.with_status(status, **kwargs))
 
     def with_reason(self, skip_reason: str) -> "InstigatorTick":
         check.opt_str_param(skip_reason, "skip_reason")
         return self._replace(tick_data=self.tick_data.with_reason(skip_reason))
```

### Comparing `dagster-1.3.9rc0/dagster/_core/scheduler/scheduler.py` & `dagster-1.4.0/dagster/_core/scheduler/scheduler.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/secrets/env_file.py` & `dagster-1.4.0/dagster/_core/secrets/env_file.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/selector/subset_selector.py` & `dagster-1.4.0/dagster/_core/selector/subset_selector.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/snap/__init__.py` & `dagster-1.4.0/dagster/_core/snap/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,30 +5,30 @@
 
 This will have a number of uses, but the most immediately germane are:
 
 1) Persist *historical* pipeline and repository structures. This
 will enable, in the short term, for the user to be able to go to a historical
 run and view the meta information at that point in time.
 2) Access metadata about dagster artifiacts that are resident in an external
-process or container. For example, dagit uses these classes load and represent
+process or container. For example, dagster-webserver uses these classes to load and represent
 metadata from user repositories that reside in different processes.
 
 There are a few varietals of classes:
 
 1) Snapshots. These are chunks of metadata that end up being persisted for
 historical purposes. Examples include the PipelineSnapshot and
 the ExecutionPlanSnapshot.
 
 2) Indexes. These are classes (not persistable) that build indexes over the
 snapshot data for fast access.
 
 3) "Active Data". These classes are serializable but not meant to be persisted.
 For example we do not persist preset configuration blocks since config
 can contain sensitive information. However this information needs to be
-communicated between user repositories and host processes such as dagit.
+communicated between user repositories and host processes such as dagster-webserver.
 
 """
 
 from dagster._config import (
     ConfigEnumValueSnap as ConfigEnumValueSnap,
     ConfigFieldSnap as ConfigFieldSnap,
     ConfigSchemaSnapshot as ConfigSchemaSnapshot,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/snap/dagster_types.py` & `dagster-1.4.0/dagster/_core/snap/dagster_types.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/snap/dep_snapshot.py` & `dagster-1.4.0/dagster/_core/snap/dep_snapshot.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/snap/execution_plan_snapshot.py` & `dagster-1.4.0/dagster/_core/snap/execution_plan_snapshot.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/snap/job_snapshot.py` & `dagster-1.4.0/dagster/_core/snap/job_snapshot.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/snap/mode.py` & `dagster-1.4.0/dagster/_core/snap/mode.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/snap/node.py` & `dagster-1.4.0/dagster/_core/snap/node.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/DEVELOPING.md` & `dagster-1.4.0/dagster/_core/storage/DEVELOPING.md`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/README.md` & `dagster-1.4.0/dagster/_core/storage/alembic/README.md`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/env.py` & `dagster-1.4.0/dagster/_core/storage/alembic/env.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/001_initial_1.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/001_initial_1.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/002_cascade_run_deletion_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/003_add_step_key_pipeline_name_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/004_add_snapshots_to_run_storage.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/004_add_snapshots_to_run_storage.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/005_add_asset_key_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/005_add_asset_key_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/005_add_asset_key_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/005_add_asset_key_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/006_scheduler_update_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/006_scheduler_update_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/007_create_run_id_idx_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/008_add_run_tags_index_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/009_add_partition_column_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/009_add_partition_column_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/009_add_partition_column_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/009_add_partition_column_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/010_add_run_partition_columns_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_1.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_1.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_2.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/011_wipe_schedules_table_for_0_10_0_sqlite_2.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/015_change_varchar_to_text_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/017_add_run_status_index_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/017_add_run_status_index_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/017_add_run_status_index_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/017_add_run_status_index_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/019_0_11_0_db_text_to_db_string_unique_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/021_add_column_mode_mysql.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/021_add_column_mode_mysql.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/021_add_column_mode_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/021_add_column_mode_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/021_add_column_mode_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/021_add_column_mode_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_mysql.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_mysql.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_postgres.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_postgres.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_sqlite.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/024_add_columns_start_time_and_end_time_sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/026_convert_start_end_times_format_mysql.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/026_convert_start_end_times_format_mysql.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/030_add_columns_action_type_and_selector_id_.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/030_add_columns_action_type_and_selector_id_.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/031_add_kvs_table.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/031_add_kvs_table.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/033_add_asset_event_tags_table.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/033_add_asset_event_tags_table.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/036_add_dynamic_partitions_table.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/036_add_dynamic_partitions_table.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/037_d9092588866f_add_primary_key_cols.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/037_d9092588866f_add_primary_key_cols.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/038_701913684cb4_add_postgres_pks.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/038_701913684cb4_add_postgres_pks.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/039_d3a4c9e87af3_add_asset_daemon_asset_evaluations_table.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/039_d3a4c9e87af3_add_asset_daemon_asset_evaluations_table.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/alembic/versions/040_add_in_progress_step_table.py` & `dagster-1.4.0/dagster/_core/storage/alembic/versions/040_add_in_progress_step_table.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/asset_value_loader.py` & `dagster-1.4.0/dagster/_core/storage/asset_value_loader.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/base_storage.py` & `dagster-1.4.0/dagster/_core/storage/base_storage.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 
 
 class DagsterStorage(ABC, MayHaveInstanceWeakref[T_DagsterInstance]):
     """Abstract base class for Dagster persistent storage, for reading and writing data for runs,
     events, and schedule/sensor state.
 
     Users should not directly instantiate concrete subclasses of this class; they are instantiated
-    by internal machinery when ``dagit`` and ``dagster-daemon`` load, based on the values in the
+    by internal machinery when ``dagster-webserver`` and ``dagster-daemon`` load, based on the values in the
     ``dagster.yaml`` file in ``$DAGSTER_HOME``. Configuration of concrete subclasses of this class
     should be done by setting values in that file.
     """
 
     @property
     @abstractmethod
     def event_log_storage(self) -> EventLogStorage[T_DagsterInstance]:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/branching/branching_io_manager.py` & `dagster-1.4.0/dagster/_core/storage/branching/branching_io_manager.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/captured_log_manager.py` & `dagster-1.4.0/dagster/_core/storage/captured_log_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -20,15 +20,15 @@
             ("external_url", Optional[str]),
             ("external_stdout_url", Optional[str]),
             ("external_stderr_url", Optional[str]),
         ],
     )
 ):
     """Object representing the context in which logs are captured.  Can be used by external logging
-    sidecar implementations to point dagit to an external url to view compute logs instead of a
+    sidecar implementations to point the Dagster UI to an external url to view compute logs instead of a
     Dagster-managed location.
     """
 
     def __new__(
         cls,
         log_key: Sequence[str],
         external_stdout_url: Optional[str] = None,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/cloud_storage_compute_log_manager.py` & `dagster-1.4.0/dagster/_core/storage/cloud_storage_compute_log_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -383,15 +383,15 @@
             subscription.fetch()
 
     def _poll(self, shutdown_event: threading.Event) -> None:
         while True:
             if shutdown_event.is_set():
                 return
             # need to do something smarter here that keeps track of updates
-            for _, subscriptions in self._subscriptions.items():
+            for subscriptions in self._subscriptions.values():
                 for subscription in subscriptions:
                     if shutdown_event.is_set():
                         return
                     subscription.fetch()
             time.sleep(SUBSCRIPTION_POLLING_INTERVAL)
 
     def dispose(self) -> None:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/compute_log_manager.py` & `dagster-1.4.0/dagster/_core/storage/compute_log_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -174,15 +174,15 @@
         Returns:
             Boolean
         """
         return True
 
     @abstractmethod
     def on_subscribe(self, subscription: "ComputeLogSubscription") -> None:
-        """Hook for managing streaming subscriptions for log data from `dagit`.
+        """Hook for managing streaming subscriptions for log data from `dagster-webserver`.
 
         Args:
             subscription (ComputeLogSubscription): subscription object which manages when to send
                 back data to the subscriber
         """
 
     def on_unsubscribe(self, subscription: "ComputeLogSubscription") -> None:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/config.py` & `dagster-1.4.0/dagster/_core/storage/config.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/dagster_run.py` & `dagster-1.4.0/dagster/_core/storage/dagster_run.py`

 * *Files 3% similar despite different names*

```diff
@@ -373,47 +373,52 @@
     def get_parent_run_id(self) -> Optional[str]:
         return self.tags.get(PARENT_RUN_ID_TAG)
 
     def tags_for_storage(self) -> Mapping[str, str]:
         repository_tags = {}
         if self.external_job_origin:
             # tag the run with a label containing the repository name / location name, to allow for
-            # per-repository filtering of runs from dagit.
+            # per-repository filtering of runs from the Dagster UI.
             repository_tags[
                 REPOSITORY_LABEL_TAG
             ] = self.external_job_origin.external_repository_origin.get_label()
 
         if not self.tags:
             return repository_tags
 
         return {**repository_tags, **self.tags}
 
     @public
     @property
     def is_finished(self) -> bool:
+        """bool: If this run has completely finished execution."""
         return self.status in FINISHED_STATUSES
 
     @public
     @property
     def is_success(self) -> bool:
+        """bool: If this run has successfully finished executing."""
         return self.status == DagsterRunStatus.SUCCESS
 
     @public
     @property
     def is_failure(self) -> bool:
+        """bool: If this run has failed."""
         return self.status == DagsterRunStatus.FAILURE
 
     @public
     @property
-    def is_failure_or_canceled(self):
+    def is_failure_or_canceled(self) -> bool:
+        """bool: If this run has either failed or was canceled."""
         return self.status == DagsterRunStatus.FAILURE or self.status == DagsterRunStatus.CANCELED
 
     @public
     @property
     def is_resume_retry(self) -> bool:
+        """bool: If this run was created from retrying another run from the point of failure."""
         return self.tags.get(RESUME_RETRY_TAG) == "true"
 
     @property
     def previous_run_id(self) -> Optional[str]:
         # Compat
         return self.parent_run_id
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/db_io_manager.py` & `dagster-1.4.0/dagster/_core/storage/db_io_manager.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/__init__.py` & `dagster-1.4.0/dagster/_core/storage/event_log/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/base.py` & `dagster-1.4.0/dagster/_core/storage/event_log/base.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import base64
 from abc import ABC, abstractmethod
 from enum import Enum
 from typing import (
     TYPE_CHECKING,
+    Iterable,
     Mapping,
     NamedTuple,
     Optional,
     Sequence,
     Set,
     Tuple,
     Union,
@@ -123,14 +124,20 @@
 
     @property
     def last_materialization(self) -> Optional["EventLogEntry"]:
         if self.last_materialization_record is None:
             return None
         return self.last_materialization_record.event_log_entry
 
+    @property
+    def last_materialization_storage_id(self) -> Optional[int]:
+        if self.last_materialization_record is None:
+            return None
+        return self.last_materialization_record.storage_id
+
 
 class AssetRecord(NamedTuple):
     """Internal representation of an asset record, as stored in a :py:class:`~dagster._core.storage.event_log.EventLogStorage`.
 
     Users should not invoke this class directly.
     """
 
@@ -141,15 +148,15 @@
 class EventLogStorage(ABC, MayHaveInstanceWeakref[T_DagsterInstance]):
     """Abstract base class for storing structured event logs from pipeline runs.
 
     Note that event log storages using SQL databases as backing stores should implement
     :py:class:`~dagster._core.storage.event_log.SqlEventLogStorage`.
 
     Users should not directly instantiate concrete subclasses of this class; they are instantiated
-    by internal machinery when ``dagit`` and ``dagster-graphql`` load, based on the values in the
+    by internal machinery when ``dagster-webserver`` and ``dagster-graphql`` load, based on the values in the
     ``dagster.yaml`` file in ``$DAGSTER_HOME``. Configuration of concrete subclasses of this class
     should be done by setting values in that file.
     """
 
     def get_logs_for_run(
         self,
         run_id: str,
@@ -252,16 +259,16 @@
     @abstractmethod
     def is_persistent(self) -> bool:
         """bool: Whether the storage is persistent."""
 
     def dispose(self) -> None:
         """Explicit lifecycle management."""
 
-    def optimize_for_dagit(self, statement_timeout: int, pool_recycle: int) -> None:
-        """Allows for optimizing database connection / use in the context of a long lived dagit process.
+    def optimize_for_webserver(self, statement_timeout: int, pool_recycle: int) -> None:
+        """Allows for optimizing database connection / use in the context of a long lived webserver process.
         """
 
     @abstractmethod
     def get_event_records(
         self,
         event_records_filter: EventRecordsFilter,
         limit: Optional[int] = None,
@@ -334,15 +341,15 @@
                 asset_keys = asset_keys[idx + 1 :]
         if limit:
             asset_keys = asset_keys[:limit]
         return asset_keys
 
     @abstractmethod
     def get_latest_materialization_events(
-        self, asset_keys: Sequence[AssetKey]
+        self, asset_keys: Iterable[AssetKey]
     ) -> Mapping[AssetKey, Optional["EventLogEntry"]]:
         pass
 
     def supports_add_asset_event_tags(self) -> bool:
         return False
 
     def add_asset_event_tags(
@@ -374,14 +381,32 @@
     @abstractmethod
     def get_materialization_count_by_partition(
         self, asset_keys: Sequence[AssetKey], after_cursor: Optional[int] = None
     ) -> Mapping[AssetKey, Mapping[str, int]]:
         pass
 
     @abstractmethod
+    def get_latest_storage_id_by_partition(
+        self, asset_key: AssetKey, event_type: DagsterEventType
+    ) -> Mapping[str, int]:
+        pass
+
+    @abstractmethod
+    def get_latest_tags_by_partition(
+        self,
+        asset_key: AssetKey,
+        event_type: DagsterEventType,
+        tag_keys: Sequence[str],
+        asset_partitions: Optional[Sequence[str]] = None,
+        before_cursor: Optional[int] = None,
+        after_cursor: Optional[int] = None,
+    ) -> Mapping[str, Mapping[str, str]]:
+        pass
+
+    @abstractmethod
     def get_latest_asset_partition_materialization_attempts_without_materializations(
         self, asset_key: AssetKey
     ) -> Mapping[str, Tuple[str, int]]:
         pass
 
     @abstractmethod
     def get_dynamic_partitions(self, partitions_def_name: str) -> Sequence[str]:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/in_memory.py` & `dagster-1.4.0/dagster/_core/storage/event_log/in_memory.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 from .schema import SqlEventLogStorageMetadata
 from .sql_event_log import SqlEventLogStorage
 
 
 class InMemoryEventLogStorage(SqlEventLogStorage, ConfigurableClass):
     """In memory only event log storage. Used by ephemeral DagsterInstance or for testing purposes.
 
-    WARNING: Dagit and other core functionality will not work if this is used on a real DagsterInstance
+    WARNING: The Dagster UI and other core functionality will not work if this is used on a real DagsterInstance
     """
 
     def __init__(self, inst_data: Optional[ConfigurableClassData] = None, preload=None):
         self._inst_data = inst_data
         self._engine = create_engine(
             create_in_memory_conn_string(f"events-{uuid.uuid4()}"),
             poolclass=NullPool,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/migration.py` & `dagster-1.4.0/dagster/_core/storage/event_log/migration.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/polling_event_watcher.py` & `dagster-1.4.0/dagster/_core/storage/event_log/polling_event_watcher.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/schema.py` & `dagster-1.4.0/dagster/_core/storage/event_log/schema.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,15 +27,15 @@
     db.Column("migration_completed", db.DateTime),
 )
 
 # The AssetKeyTable contains a `last_materialization_timestamp` column that is exclusively
 # used to determine if an asset exists (last materialization timestamp > wipe timestamp).
 # This column is used nowhere else, and as of AssetObservation creation, we want to extend
 # this functionality to ensure that assets with observation OR materialization timestamp
-# > wipe timestamp display in Dagit.
+# > wipe timestamp display in the Dagster UI.
 
 # As of the following PR, we update last_materialization_timestamp to store the timestamp
 # of the latest asset observation or materialization that has occurred.
 # https://github.com/dagster-io/dagster/pull/6885
 AssetKeyTable = db.Table(
     "asset_keys",
     SqlEventLogStorageMetadata,
@@ -70,15 +70,20 @@
     db.Column("event_timestamp", db.types.TIMESTAMP),
 )
 
 
 DynamicPartitionsTable = db.Table(
     "dynamic_partitions",
     SqlEventLogStorageMetadata,
-    db.Column("id", db.Integer, primary_key=True, autoincrement=True),
+    db.Column(
+        "id",
+        db.BigInteger().with_variant(sqlite.INTEGER(), "sqlite"),
+        primary_key=True,
+        autoincrement=True,
+    ),
     db.Column("partitions_def_name", db.Text, nullable=False),
     db.Column("partition", db.Text, nullable=False),
     db.Column("create_timestamp", db.DateTime, server_default=get_current_timestamp()),
 )
 
 ConcurrencySlotsTable = db.Table(
     "concurrency_slots",
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/sql_event_log.py` & `dagster-1.4.0/dagster/_core/storage/event_log/sql_event_log.py`

 * *Files 2% similar despite different names*

```diff
@@ -206,15 +206,15 @@
         self, event: EventLogEntry, event_id: int, has_asset_key_index_cols: bool
     ) -> Dict[str, Any]:
         # The AssetKeyTable contains a `last_materialization_timestamp` column that is exclusively
         # used to determine if an asset exists (last materialization timestamp > wipe timestamp).
         # This column is used nowhere else, and as of AssetObservation/AssetMaterializationPlanned
         # event creation, we want to extend this functionality to ensure that assets with any event
         # (observation, materialization, or materialization planned) yielded with timestamp
-        # > wipe timestamp display in Dagit.
+        # > wipe timestamp display in the Dagster UI.
 
         # As of the following PRs, we update last_materialization_timestamp to store the timestamp
         # of the latest asset observation, materialization, or materialization_planned that has occurred.
         # https://github.com/dagster-io/dagster/pull/6885
         # https://github.com/dagster-io/dagster/pull/7319
 
         entry_values: Dict[str, Any] = {}
@@ -333,33 +333,31 @@
                     ],
                 )
 
     def store_asset_event_tags(self, event: EventLogEntry, event_id: int) -> None:
         check.inst_param(event, "event", EventLogEntry)
         check.int_param(event_id, "event_id")
 
-        if (
-            event.dagster_event
-            and event.dagster_event.asset_key
-            and event.dagster_event.is_step_materialization
-            and isinstance(
-                event.dagster_event.step_materialization_data.materialization, AssetMaterialization
-            )
-            and event.dagster_event.step_materialization_data.materialization.tags
-        ):
-            if not self.has_table(AssetEventTagsTable.name):
+        if event.dagster_event and event.dagster_event.asset_key:
+            if event.dagster_event.is_step_materialization:
+                tags = event.dagster_event.step_materialization_data.materialization.tags
+            elif event.dagster_event.is_asset_observation:
+                tags = event.dagster_event.asset_observation_data.asset_observation.tags
+            else:
+                tags = None
+
+            if not tags or not self.has_table(AssetEventTagsTable.name):
                 # If tags table does not exist, silently exit. This is to support OSS
                 # users who have not yet run the migration to create the table.
                 # On read, we will throw an error if the table does not exist.
                 return
 
             check.inst_param(event.dagster_event.asset_key, "asset_key", AssetKey)
             asset_key_str = event.dagster_event.asset_key.to_string()
 
-            tags = event.dagster_event.step_materialization_data.materialization.tags
             with self.index_connection() as conn:
                 conn.execute(
                     AssetEventTagsTable.insert(),
                     [
                         dict(
                             event_id=event_id,
                             asset_key=asset_key_str,
@@ -648,14 +646,17 @@
 
             if self.has_table("concurrency_slots"):
                 conn.execute(ConcurrencySlotsTable.delete())
 
             if self.has_table("pending_steps"):
                 conn.execute(PendingStepsTable.delete())
 
+        self._wipe_index()
+
+    def _wipe_index(self):
         with self.index_connection() as conn:
             conn.execute(SqlEventLogStorageTable.delete())
             conn.execute(AssetKeyTable.delete())
 
             if self.has_table("asset_event_tags"):
                 conn.execute(AssetEventTagsTable.delete())
 
@@ -1197,17 +1198,17 @@
         asset_keys = [
             AssetKey.from_db_string(row["asset_key"])
             for row in sorted(rows, key=lambda x: x["asset_key"])
         ]
         return [asset_key for asset_key in asset_keys if asset_key]
 
     def get_latest_materialization_events(
-        self, asset_keys: Sequence[AssetKey]
+        self, asset_keys: Iterable[AssetKey]
     ) -> Mapping[AssetKey, Optional[EventLogEntry]]:
-        check.sequence_param(asset_keys, "asset_keys", AssetKey)
+        check.iterable_param(asset_keys, "asset_keys", AssetKey)
         rows = self._fetch_asset_rows(asset_keys=asset_keys)
         return {
             asset_key: event_log_record.event_log_entry if event_log_record is not None else None
             for asset_key, event_log_record in self._get_latest_materialization_records(
                 rows
             ).items()
         }
@@ -1717,51 +1718,155 @@
         for row in results:
             asset_key = AssetKey.from_db_string(cast(Optional[str], row[0]))
             if asset_key:
                 materialization_count_by_partition[asset_key][cast(str, row[1])] = cast(int, row[2])
 
         return materialization_count_by_partition
 
-    def get_latest_asset_partition_materialization_attempts_without_materializations(
-        self, asset_key: AssetKey
-    ) -> Mapping[str, Tuple[str, int]]:
-        """Fetch the latest materialzation and materialization planned events for each partition of the given asset.
-        Return the partitions that have a materialization planned event but no matching (same run) materialization event.
-        These materializations could be in progress, or they could have failed. A separate query checking the run status
-        is required to know.
+    def _latest_event_ids_by_partition_subquery(
+        self,
+        asset_key: AssetKey,
+        event_types: Sequence[DagsterEventType],
+        asset_partitions: Optional[Sequence[str]] = None,
+        before_cursor: Optional[int] = None,
+        after_cursor: Optional[int] = None,
+    ):
+        """Subquery for locating the latest event ids by partition for a given asset key and set
+        of event types.
+        """
+        query = db_select(
+            [
+                SqlEventLogStorageTable.c.dagster_event_type,
+                SqlEventLogStorageTable.c.partition,
+                db.func.max(SqlEventLogStorageTable.c.id).label("id"),
+            ]
+        ).where(
+            db.and_(
+                SqlEventLogStorageTable.c.asset_key == asset_key.to_string(),
+                SqlEventLogStorageTable.c.partition != None,  # noqa: E711
+                SqlEventLogStorageTable.c.dagster_event_type.in_(
+                    [event_type.value for event_type in event_types]
+                ),
+            )
+        )
+        if asset_partitions is not None:
+            query = query.where(SqlEventLogStorageTable.c.partition.in_(asset_partitions))
+        if before_cursor is not None:
+            query = query.where(SqlEventLogStorageTable.c.id < before_cursor)
+        if after_cursor is not None:
+            query = query.where(SqlEventLogStorageTable.c.id > after_cursor)
 
-        Returns a mapping of partition to [run id, event id].
+        latest_event_ids_subquery = query.group_by(
+            SqlEventLogStorageTable.c.dagster_event_type, SqlEventLogStorageTable.c.partition
+        )
+
+        assets_details = self._get_assets_details([asset_key])
+        return db_subquery(
+            self._add_assets_wipe_filter_to_query(
+                latest_event_ids_subquery, assets_details, [asset_key]
+            ),
+            "latest_event_ids_by_partition_subquery",
+        )
+
+    def get_latest_storage_id_by_partition(
+        self, asset_key: AssetKey, event_type: DagsterEventType
+    ) -> Mapping[str, int]:
+        """Fetch the latest materialzation storage id for each partition for a given asset key.
+
+        Returns a mapping of partition to storage id.
         """
         check.inst_param(asset_key, "asset_key", AssetKey)
 
-        latest_event_ids_subquery = (
+        latest_event_ids_by_partition_subquery = self._latest_event_ids_by_partition_subquery(
+            asset_key, [event_type]
+        )
+        latest_event_ids_by_partition = db_select(
+            [
+                latest_event_ids_by_partition_subquery.c.partition,
+                latest_event_ids_by_partition_subquery.c.id,
+            ]
+        )
+
+        with self.index_connection() as conn:
+            rows = conn.execute(latest_event_ids_by_partition).fetchall()
+
+        latest_materialization_storage_id_by_partition: Dict[str, int] = {}
+        for row in rows:
+            latest_materialization_storage_id_by_partition[cast(str, row[0])] = cast(int, row[1])
+        return latest_materialization_storage_id_by_partition
+
+    def get_latest_tags_by_partition(
+        self,
+        asset_key: AssetKey,
+        event_type: DagsterEventType,
+        tag_keys: Sequence[str],
+        asset_partitions: Optional[Sequence[str]] = None,
+        before_cursor: Optional[int] = None,
+        after_cursor: Optional[int] = None,
+    ) -> Mapping[str, Mapping[str, str]]:
+        check.inst_param(asset_key, "asset_key", AssetKey)
+        check.inst_param(event_type, "event_type", DagsterEventType)
+        check.sequence_param(tag_keys, "tag_keys", of_type=str)
+        check.opt_nullable_sequence_param(asset_partitions, "asset_partitions", of_type=str)
+        check.opt_int_param(before_cursor, "before_cursor")
+        check.opt_int_param(after_cursor, "after_cursor")
+
+        latest_event_ids_subquery = self._latest_event_ids_by_partition_subquery(
+            asset_key=asset_key,
+            event_types=[event_type],
+            asset_partitions=asset_partitions,
+            before_cursor=before_cursor,
+            after_cursor=after_cursor,
+        )
+
+        latest_tags_by_partition_query = (
             db_select(
                 [
-                    SqlEventLogStorageTable.c.dagster_event_type,
-                    SqlEventLogStorageTable.c.partition,
-                    db.func.max(SqlEventLogStorageTable.c.id).label("id"),
+                    latest_event_ids_subquery.c.partition,
+                    AssetEventTagsTable.c.key,
+                    AssetEventTagsTable.c.value,
                 ]
             )
-            .where(
-                db.and_(
-                    SqlEventLogStorageTable.c.asset_key == asset_key.to_string(),
-                    SqlEventLogStorageTable.c.partition != None,  # noqa: E711
+            .select_from(
+                latest_event_ids_subquery.join(
+                    AssetEventTagsTable,
+                    AssetEventTagsTable.c.event_id == latest_event_ids_subquery.c.id,
                 )
             )
-            .group_by(
-                SqlEventLogStorageTable.c.dagster_event_type, SqlEventLogStorageTable.c.partition
-            )
+            .where(AssetEventTagsTable.c.key.in_(tag_keys))
         )
 
-        assets_details = self._get_assets_details([asset_key])
-        latest_event_ids_subquery = db_subquery(
-            self._add_assets_wipe_filter_to_query(
-                latest_event_ids_subquery, assets_details, [asset_key]
-            ),
-            "latest_event_ids_subquery",
+        latest_tags_by_partition: Dict[str, Dict[str, str]] = defaultdict(dict)
+        with self.index_connection() as conn:
+            rows = conn.execute(latest_tags_by_partition_query).fetchall()
+
+        for row in rows:
+            latest_tags_by_partition[cast(str, row[0])][cast(str, row[1])] = cast(str, row[2])
+
+        # convert defaultdict to dict
+        return dict(latest_tags_by_partition)
+
+    def get_latest_asset_partition_materialization_attempts_without_materializations(
+        self, asset_key: AssetKey
+    ) -> Mapping[str, Tuple[str, int]]:
+        """Fetch the latest materialzation and materialization planned events for each partition of the given asset.
+        Return the partitions that have a materialization planned event but no matching (same run) materialization event.
+        These materializations could be in progress, or they could have failed. A separate query checking the run status
+        is required to know.
+
+        Returns a mapping of partition to [run id, event id].
+        """
+        check.inst_param(asset_key, "asset_key", AssetKey)
+
+        latest_event_ids_subquery = self._latest_event_ids_by_partition_subquery(
+            asset_key,
+            [
+                DagsterEventType.ASSET_MATERIALIZATION,
+                DagsterEventType.ASSET_MATERIALIZATION_PLANNED,
+            ],
         )
 
         latest_events_subquery = db_subquery(
             db_select(
                 [
                     SqlEventLogStorageTable.c.dagster_event_type,
                     SqlEventLogStorageTable.c.partition,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/alembic/alembic.ini` & `dagster-1.4.0/dagster/_core/storage/event_log/sqlite/alembic/alembic.ini`

 * *Files 15% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 # on newly generated revision scripts.  See the documentation for further
 # detail and examples
 
 # format using "black" - use the console_scripts runner, against the "black" entrypoint
 hooks = black
 black.type = console_scripts
 black.entrypoint = black
-black.options = --line-length 100 --target-version py36 --target-version py37 --target-version py38 -S --fast
+black.options = --line-length 100 --target-version py38 --target-version py39 --target-version py310 --target-version py311 -S --fast
 
 # Logging configuration
 [loggers]
 keys = root,sqlalchemy,alembic
 
 [handlers]
 keys = console
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/consolidated_sqlite_event_log.py` & `dagster-1.4.0/dagster/_core/storage/event_log/sqlite/consolidated_sqlite_event_log.py`

 * *Files 2% similar despite different names*

```diff
@@ -31,15 +31,15 @@
 SQLITE_EVENT_LOG_FILENAME = "event_log"
 
 
 class ConsolidatedSqliteEventLogStorage(SqlEventLogStorage, ConfigurableClass):
     """SQLite-backed consolidated event log storage intended for test cases only.
 
     Users should not directly instantiate this class; it is instantiated by internal machinery when
-    ``dagit`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file in
+    ``dagster-webserver`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file in
     ``$DAGSTER_HOME``. Configuration of this class should be done by setting values in that file.
 
     To explicitly specify the consolidated SQLite for event log storage, you can add a block such as
     the following to your ``dagster.yaml``:
 
     .. code-block:: YAML
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/event_log/sqlite/sqlite_event_log.py` & `dagster-1.4.0/dagster/_core/storage/event_log/sqlite/sqlite_event_log.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import contextlib
 import glob
 import logging
 import os
 import re
 import sqlite3
 import threading
 import time
@@ -54,15 +55,15 @@
 INDEX_SHARD_NAME = "index"
 
 
 class SqliteEventLogStorage(SqlEventLogStorage, ConfigurableClass):
     """SQLite-backed event log storage.
 
     Users should not directly instantiate this class; it is instantiated by internal machinery when
-    ``dagit`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file in
+    ``dagster-webserver`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file insqliteve
     ``$DAGSTER_HOME``. Configuration of this class should be done by setting values in that file.
 
     This is the default event log storage when none is specified in the ``dagster.yaml``.
 
     To explicitly specify SQLite for event log storage, you can add a block such as the following
     to your ``dagster.yaml``:
 
@@ -171,15 +172,15 @@
                         SqlEventLogStorageMetadata.create_all(engine)
                         connection.execute(db.text("PRAGMA journal_mode=WAL;"))
                         stamp_alembic_rev(alembic_config, connection)
 
                 break
             except (db_exc.DatabaseError, sqlite3.DatabaseError, sqlite3.OperationalError) as exc:
                 # This is SQLite-specific handling for concurrency issues that can arise when
-                # multiple processes (e.g. the dagit process and user code process) contend with
+                # multiple processes (e.g. the dagster-webserver process and user code process) contend with
                 # each other to init the db. When we hit the following errors, we know that another
                 # process is on the case and we should retry.
                 err_msg = str(exc)
 
                 if not (
                     re.search(r"table [A-Za-z_]* already exists", err_msg)
                     or "database is locked" in err_msg
@@ -364,23 +365,30 @@
             self.delete_events_for_run(conn, run_id)
 
         # delete the mirrored event in the cross-run index database
         with self.index_connection() as conn:
             self.delete_events_for_run(conn, run_id)
 
     def wipe(self) -> None:
-        # should delete all the run-sharded dbs as well as the index db
+        # should delete all the run-sharded db files and drop the contents of the index
         for filename in (
             glob.glob(os.path.join(self._base_dir, "*.db"))
             + glob.glob(os.path.join(self._base_dir, "*.db-wal"))
             + glob.glob(os.path.join(self._base_dir, "*.db-shm"))
         ):
-            os.unlink(filename)
+            if (
+                not filename.endswith(f"{INDEX_SHARD_NAME}.db")
+                and not filename.endswith(f"{INDEX_SHARD_NAME}.db-wal")
+                and not filename.endswith(f"{INDEX_SHARD_NAME}.db-shm")
+            ):
+                with contextlib.suppress(FileNotFoundError):
+                    os.unlink(filename)
 
         self._initialized_dbs = set()
+        self._wipe_index()
 
     def _delete_mirrored_events_for_asset_key(self, asset_key: AssetKey) -> None:
         with self.index_connection() as conn:
             conn.execute(
                 SqlEventLogStorageTable.delete().where(
                     SqlEventLogStorageTable.c.asset_key == asset_key.to_string(),
                 )
@@ -432,15 +440,15 @@
 class SqliteEventLogStorageWatchdog(PatternMatchingEventHandler):
     def __init__(
         self,
         event_log_storage: SqliteEventLogStorage,
         run_id: str,
         callback: EventHandlerFn,
         cursor: Optional[str],
-        **kwargs: object,
+        **kwargs: Any,
     ):
         self._event_log_storage = check.inst_param(
             event_log_storage, "event_log_storage", SqliteEventLogStorage
         )
         self._run_id = check.str_param(run_id, "run_id")
         self._cb = check.callable_param(callback, "callback")
         self._log_path = event_log_storage.path_for_shard(run_id)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/file_manager.py` & `dagster-1.4.0/dagster/_core/storage/file_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from typing import BinaryIO, ContextManager, Iterator, Optional, TextIO, Union
 
 from typing_extensions import TypeAlias
 
 import dagster._check as check
 from dagster._annotations import public
 from dagster._config import Field, StringSource
-from dagster._core.definitions.resource_definition import resource
+from dagster._core.definitions.resource_definition import dagster_maintained_resource, resource
 from dagster._core.execution.context.init import InitResourceContext
 from dagster._core.instance import DagsterInstance
 from dagster._utils import mkdir_p
 
 from .temp_file_manager import TempfileManager
 
 IOStream: TypeAlias = Union[TextIO, BinaryIO]
@@ -163,14 +163,15 @@
 
         Returns:
             FileHandle: A handle to the newly created file.
         """
         raise NotImplementedError()
 
 
+@dagster_maintained_resource
 @resource(config_schema={"base_dir": Field(StringSource, is_required=False)})
 def local_file_manager(init_context: InitResourceContext) -> "LocalFileManager":
     """FileManager that provides abstract access to a local filesystem.
 
     By default, files will be stored in `<local_artifact_storage>/storage/file_manager` where
     `<local_artifact_storage>` can be configured the ``dagster.yaml`` file in ``$DAGSTER_HOME``.
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/fs_io_manager.py` & `dagster-1.4.0/dagster/_core/storage/fs_io_manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,31 +1,34 @@
 import os
 import pickle
-from typing import Any, Optional
+from typing import TYPE_CHECKING, Any, Optional
 
 from pydantic import Field
-from upath import UPath
 
 import dagster._check as check
 from dagster import (
     DagsterInvariantViolationError,
     Field as DagsterField,
 )
 from dagster._annotations import experimental
 from dagster._config import StringSource
 from dagster._config.pythonic_config import ConfigurableIOManagerFactory
 from dagster._core.definitions.events import AssetKey, AssetMaterialization
 from dagster._core.definitions.metadata import MetadataValue
 from dagster._core.execution.context.init import InitResourceContext
 from dagster._core.execution.context.input import InputContext
 from dagster._core.execution.context.output import OutputContext
-from dagster._core.storage.io_manager import IOManager, io_manager
+from dagster._core.storage.io_manager import IOManager, dagster_maintained_io_manager, io_manager
 from dagster._core.storage.upath_io_manager import UPathIOManager
 from dagster._utils import PICKLE_PROTOCOL, mkdir_p
 
+if TYPE_CHECKING:
+    from typing_extensions import Literal
+    from upath import UPath
+
 
 class FilesystemIOManager(ConfigurableIOManagerFactory["PickledObjectFilesystemIOManager"]):
     """Built-in filesystem IO manager that stores and retrieves values using pickling.
 
     The base directory that the pickle files live inside is determined by:
 
     * The IO manager's "base_dir" configuration value, if specified. Otherwise...
@@ -116,19 +119,24 @@
         def job():
             op_b(op_a())
 
     """
 
     base_dir: Optional[str] = Field(default=None, description="Base directory for storing files.")
 
+    @classmethod
+    def _is_dagster_maintained(cls) -> bool:
+        return True
+
     def create_io_manager(self, context: InitResourceContext) -> "PickledObjectFilesystemIOManager":
         base_dir = self.base_dir or check.not_none(context.instance).storage_directory()
         return PickledObjectFilesystemIOManager(base_dir=base_dir)
 
 
+@dagster_maintained_io_manager
 @io_manager(
     config_schema=FilesystemIOManager.to_config_schema(),
     description="Built-in filesystem IO manager that stores and retrieves values using pickling.",
 )
 def fs_io_manager(init_context: InitResourceContext) -> "PickledObjectFilesystemIOManager":
     """Built-in filesystem IO manager that stores and retrieves values using pickling.
 
@@ -236,19 +244,21 @@
             manager will be stored in.
         **kwargs: additional keyword arguments for `universal_pathlib.UPath`.
     """
 
     extension: str = ""  # TODO: maybe change this to .pickle? Leaving blank for compatibility.
 
     def __init__(self, base_dir=None, **kwargs):
+        from upath import UPath
+
         self.base_dir = check.opt_str_param(base_dir, "base_dir")
 
         super().__init__(base_path=UPath(base_dir, **kwargs))
 
-    def dump_to_path(self, context: OutputContext, obj: Any, path: UPath):
+    def dump_to_path(self, context: OutputContext, obj: Any, path: "UPath"):
         try:
             with path.open("wb") as file:
                 pickle.dump(obj, file, PICKLE_PROTOCOL)
         except (AttributeError, RecursionError, ImportError, pickle.PicklingError) as e:
             executor = context.step_context.job_def.executor_def
 
             if isinstance(e, RecursionError):
@@ -265,15 +275,15 @@
                 "mem_io_manager with the in_process_executor.\n"
                 "For more information on io managers, visit "
                 "https://docs.dagster.io/concepts/io-management/io-managers \n"
                 "For more information on executors, vist "
                 "https://docs.dagster.io/deployment/executors#overview"
             ) from e
 
-    def load_from_path(self, context: InputContext, path: UPath) -> Any:
+    def load_from_path(self, context: InputContext, path: "UPath") -> Any:
         with path.open("rb") as file:
             return pickle.load(file)
 
 
 class CustomPathPickledObjectFilesystemIOManager(IOManager):
     """Built-in filesystem IO managerthat stores and retrieves values using pickling and
     allow users to specify file path for outputs.
@@ -281,16 +291,16 @@
     Args:
         base_dir (Optional[str]): base directory where all the step outputs which use this object
             manager will be stored in.
     """
 
     def __init__(self, base_dir: Optional[str] = None):
         self.base_dir = check.opt_str_param(base_dir, "base_dir")
-        self.write_mode = "wb"
-        self.read_mode = "rb"
+        self.write_mode: Literal["wb"] = "wb"
+        self.read_mode: Literal["rb"] = "rb"
 
     def _get_path(self, path: str) -> str:
         return os.path.join(self.base_dir, path)  # type: ignore  # (possible none)
 
     def handle_output(self, context: OutputContext, obj: object):
         """Pickle the data and store the object to a custom file path.
 
@@ -323,14 +333,15 @@
         filepath = self._get_path(path)
         context.log.debug(f"Loading file from: {filepath}")
 
         with open(filepath, self.read_mode) as read_obj:
             return pickle.load(read_obj)
 
 
+@dagster_maintained_io_manager
 @io_manager(config_schema={"base_dir": DagsterField(StringSource, is_required=True)})
 @experimental
 def custom_path_fs_io_manager(
     init_context: InitResourceContext,
 ) -> CustomPathPickledObjectFilesystemIOManager:
     """Built-in IO manager that allows users to custom output file path per output definition.
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/input_manager.py` & `dagster-1.4.0/dagster/_core/storage/input_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -142,21 +142,21 @@
             manager.
         version (Optional[str]): (Experimental) the version of the input manager definition.
 
     **Examples:**
 
     .. code-block:: python
 
-        from dagster import root_input_manager, op, job, In
+        from dagster import input_manager, op, job, In
 
         @input_manager
         def csv_loader(_):
             return read_csv("some/path")
 
-        @op(ins={"input1": In(root_manager_key="csv_loader_key")})
+        @op(ins={"input1": In(input_manager_key="csv_loader_key")})
         def my_op(_, input1):
             do_stuff(input1)
 
         @job(resource_defs={"csv_loader_key": csv_loader})
         def my_job():
             my_op()
 
@@ -226,20 +226,20 @@
 
     def __call__(self, load_fn: InputLoadFn) -> InputManagerDefinition:
         check.callable_param(load_fn, "load_fn")
 
         def _resource_fn(_):
             return InputManagerWrapper(load_fn)
 
-        root_input_manager_def = InputManagerDefinition(
+        input_manager_def = InputManagerDefinition(
             resource_fn=_resource_fn,
             config_schema=self.config_schema,
             description=self.description,
             version=self.version,
             input_config_schema=self.input_config_schema,
             required_resource_keys=self.required_resource_keys,
         )
 
         # `update_wrapper` typing cannot currently handle a Union of Callables correctly
-        update_wrapper(root_input_manager_def, wrapped=load_fn)  # type: ignore
+        update_wrapper(input_manager_def, wrapped=load_fn)  # type: ignore
 
-        return root_input_manager_def
+        return input_manager_def
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/io_manager.py` & `dagster-1.4.0/dagster/_core/storage/io_manager.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,17 +10,16 @@
 from dagster._core.definitions.config import is_callable_valid_config_arg
 from dagster._core.definitions.definition_config_schema import (
     CoercableToConfigSchema,
     IDefinitionConfigSchema,
     convert_user_facing_definition_config_schema,
 )
 from dagster._core.definitions.resource_definition import ResourceDefinition
-from dagster._core.storage.input_manager import InputManager
+from dagster._core.storage.input_manager import IInputManagerDefinition, InputManager
 from dagster._core.storage.output_manager import IOutputManagerDefinition, OutputManager
-from dagster._core.storage.root_input_manager import IInputManagerDefinition
 
 from ..decorator_utils import get_function_params
 
 if TYPE_CHECKING:
     from dagster._core.execution.context.init import InitResourceContext
     from dagster._core.execution.context.input import InputContext
     from dagster._core.execution.context.output import OutputContext
@@ -88,23 +87,27 @@
         return self._output_config_schema
 
     def copy_for_configured(
         self,
         description: Optional[str],
         config_schema: CoercableToConfigSchema,
     ) -> "IOManagerDefinition":
-        return IOManagerDefinition(
+        io_def = IOManagerDefinition(
             config_schema=config_schema,
             description=description or self.description,
             resource_fn=self.resource_fn,
             required_resource_keys=self.required_resource_keys,
             input_config_schema=self.input_config_schema,
             output_config_schema=self.output_config_schema,
         )
 
+        io_def._dagster_maintained = self._is_dagster_maintained()  # noqa: SLF001
+
+        return io_def
+
     @public
     @staticmethod
     def hardcoded_io_manager(
         value: "IOManager", description: Optional[str] = None
     ) -> "IOManagerDefinition":
         """A helper function that creates an ``IOManagerDefinition`` with a hardcoded IOManager.
 
@@ -236,14 +239,19 @@
             output_config_schema=output_config_schema,
             input_config_schema=input_config_schema,
         )(resource_fn)
 
     return _wrap
 
 
+def dagster_maintained_io_manager(io_manager_def: IOManagerDefinition) -> IOManagerDefinition:
+    io_manager_def._dagster_maintained = True  # noqa: SLF001
+    return io_manager_def
+
+
 class _IOManagerDecoratorCallable:
     def __init__(
         self,
         config_schema: CoercableToConfigSchema = None,
         description: Optional[str] = None,
         output_config_schema: CoercableToConfigSchema = None,
         input_config_schema: CoercableToConfigSchema = None,
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/legacy_storage.py` & `dagster-1.4.0/dagster/_core/storage/legacy_storage.py`

 * *Files 2% similar despite different names*

```diff
@@ -296,16 +296,16 @@
 
     def optimize(self, print_fn: Optional[PrintFn] = None, force_rebuild_all: bool = False) -> None:
         return self._storage.run_storage.optimize(print_fn, force_rebuild_all)
 
     def dispose(self) -> None:
         return self._storage.run_storage.dispose()
 
-    def optimize_for_dagit(self, statement_timeout: int, pool_recycle: int) -> None:
-        return self._storage.run_storage.optimize_for_dagit(statement_timeout, pool_recycle)
+    def optimize_for_webserver(self, statement_timeout: int, pool_recycle: int) -> None:
+        return self._storage.run_storage.optimize_for_webserver(statement_timeout, pool_recycle)
 
     def add_daemon_heartbeat(self, daemon_heartbeat: "DaemonHeartbeat") -> None:
         return self._storage.run_storage.add_daemon_heartbeat(daemon_heartbeat)
 
     def get_daemon_heartbeats(self) -> Mapping[str, "DaemonHeartbeat"]:
         return self._storage.run_storage.get_daemon_heartbeats()
 
@@ -428,16 +428,18 @@
     @property
     def is_persistent(self) -> bool:
         return self._storage.event_log_storage.is_persistent
 
     def dispose(self) -> None:
         return self._storage.event_log_storage.dispose()
 
-    def optimize_for_dagit(self, statement_timeout: int, pool_recycle: int) -> None:
-        return self._storage.event_log_storage.optimize_for_dagit(statement_timeout, pool_recycle)
+    def optimize_for_webserver(self, statement_timeout: int, pool_recycle: int) -> None:
+        return self._storage.event_log_storage.optimize_for_webserver(
+            statement_timeout, pool_recycle
+        )
 
     def get_event_records(
         self,
         event_records_filter: Optional[EventRecordsFilter] = None,
         limit: Optional[int] = None,
         ascending: bool = False,
     ) -> Iterable[EventLogRecord]:
@@ -463,15 +465,15 @@
         prefix: Optional[Sequence[str]] = None,
         limit: Optional[int] = None,
         cursor: Optional[str] = None,
     ) -> Iterable["AssetKey"]:
         return self._storage.event_log_storage.get_asset_keys(prefix, limit, cursor)
 
     def get_latest_materialization_events(
-        self, asset_keys: Sequence["AssetKey"]
+        self, asset_keys: Iterable["AssetKey"]
     ) -> Mapping["AssetKey", Optional["EventLogEntry"]]:
         return self._storage.event_log_storage.get_latest_materialization_events(asset_keys)
 
     def get_asset_run_ids(self, asset_key: "AssetKey") -> Iterable[str]:
         return self._storage.event_log_storage.get_asset_run_ids(asset_key)
 
     def wipe_asset(self, asset_key: "AssetKey") -> None:
@@ -480,14 +482,34 @@
     def get_materialization_count_by_partition(
         self, asset_keys: Sequence["AssetKey"], after_cursor: Optional[int] = None
     ) -> Mapping["AssetKey", Mapping[str, int]]:
         return self._storage.event_log_storage.get_materialization_count_by_partition(
             asset_keys, after_cursor
         )
 
+    def get_latest_storage_id_by_partition(
+        self, asset_key: "AssetKey", event_type: "DagsterEventType"
+    ) -> Mapping[str, int]:
+        return self._storage.event_log_storage.get_latest_storage_id_by_partition(
+            asset_key, event_type
+        )
+
+    def get_latest_tags_by_partition(
+        self,
+        asset_key: "AssetKey",
+        event_type: "DagsterEventType",
+        tag_keys: Sequence[str],
+        asset_partitions: Optional[Sequence[str]] = None,
+        before_cursor: Optional[int] = None,
+        after_cursor: Optional[int] = None,
+    ) -> Mapping[str, Mapping[str, str]]:
+        return self._storage.event_log_storage.get_latest_tags_by_partition(
+            asset_key, event_type, tag_keys, asset_partitions, before_cursor, after_cursor
+        )
+
     def get_latest_asset_partition_materialization_attempts_without_materializations(
         self, asset_key: "AssetKey"
     ) -> Mapping[str, Tuple[str, int]]:
         return self._storage.event_log_storage.get_latest_asset_partition_materialization_attempts_without_materializations(
             asset_key
         )
 
@@ -694,21 +716,26 @@
     def get_auto_materialize_asset_evaluations(
         self, asset_key: "AssetKey", limit: int, cursor: Optional[int] = None
     ) -> Sequence["AutoMaterializeAssetEvaluationRecord"]:
         return self._storage.schedule_storage.get_auto_materialize_asset_evaluations(
             asset_key, limit, cursor
         )
 
+    def purge_asset_evaluations(self, before: float):
+        return self._storage.schedule_storage.purge_asset_evaluations(before)
+
     def upgrade(self) -> None:
         return self._storage.schedule_storage.upgrade()
 
     def migrate(self, print_fn: Optional[PrintFn] = None, force_rebuild_all: bool = False) -> None:
         return self._storage.schedule_storage.migrate(print_fn, force_rebuild_all)
 
     def optimize(self, print_fn: Optional[PrintFn] = None, force_rebuild_all: bool = False) -> None:
         return self._storage.schedule_storage.optimize(print_fn, force_rebuild_all)
 
-    def optimize_for_dagit(self, statement_timeout: int, pool_recycle: int) -> None:
-        return self._storage.schedule_storage.optimize_for_dagit(statement_timeout, pool_recycle)
+    def optimize_for_webserver(self, statement_timeout: int, pool_recycle: int) -> None:
+        return self._storage.schedule_storage.optimize_for_webserver(
+            statement_timeout, pool_recycle
+        )
 
     def dispose(self) -> None:
         return self._storage.schedule_storage.dispose()
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/local_compute_log_manager.py` & `dagster-1.4.0/dagster/_core/storage/local_compute_log_manager.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/mem_io_manager.py` & `dagster-1.4.0/dagster/_core/storage/mem_io_manager.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,24 +1,29 @@
 from typing import Dict, Tuple
 
 from dagster._core.execution.context.input import InputContext
 from dagster._core.execution.context.output import OutputContext
-from dagster._core.storage.io_manager import IOManager, io_manager
+from dagster._core.storage.io_manager import IOManager, dagster_maintained_io_manager, io_manager
 
 
 class InMemoryIOManager(IOManager):
+    """I/O manager that stores and retrieves values in memory. After execution is complete, the values will
+    be garbage-collected. Note that this means that each run will not have access to values from previous runs.
+    """
+
     def __init__(self):
         self.values: Dict[Tuple[object, ...], object] = {}
 
     def handle_output(self, context: OutputContext, obj: object):
         keys = tuple(context.get_identifier())
         self.values[keys] = obj
 
     def load_input(self, context: InputContext) -> object:
         keys = tuple(context.get_identifier())
         return self.values[keys]
 
 
+@dagster_maintained_io_manager
 @io_manager(description="Built-in IO manager that stores and retrieves values in memory.")
 def mem_io_manager(_) -> InMemoryIOManager:
     """Built-in IO manager that stores and retrieves values in memory."""
     return InMemoryIOManager()
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/memoizable_io_manager.py` & `dagster-1.4.0/dagster/_core/storage/memoizable_io_manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 
 import dagster._check as check
 from dagster._annotations import experimental, public
 from dagster._config import Field, StringSource
 from dagster._core.errors import DagsterInvariantViolationError
 from dagster._core.execution.context.input import InputContext
 from dagster._core.execution.context.output import OutputContext
-from dagster._core.storage.io_manager import IOManager, io_manager
+from dagster._core.storage.io_manager import IOManager, dagster_maintained_io_manager, io_manager
 from dagster._utils import PICKLE_PROTOCOL, mkdir_p
 
 
 class MemoizableIOManager(IOManager):
     """Base class for IO manager enabled to work with memoized execution. Users should implement
     the ``load_input`` and ``handle_output`` methods described in the ``IOManager`` API, and the
     ``has_output`` method, which returns a boolean representing whether a data object can be found.
@@ -88,14 +88,15 @@
         filepath = self._get_path(context)
 
         context.log.debug(f"Checking for file at: {filepath}")
 
         return os.path.exists(filepath) and not os.path.isdir(filepath)
 
 
+@dagster_maintained_io_manager
 @io_manager(config_schema={"base_dir": Field(StringSource, is_required=False)})
 @experimental
 def versioned_filesystem_io_manager(init_context):
     """Filesystem IO manager that utilizes versioning of stored objects.
 
     It requires users to specify a base directory where all the step outputs will be stored in. It
     serializes and deserializes output values (assets) using pickling and automatically constructs
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/migration/utils.py` & `dagster-1.4.0/dagster/_core/storage/migration/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/noop_compute_log_manager.py` & `dagster-1.4.0/dagster/_core/storage/noop_compute_log_manager.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/output_manager.py` & `dagster-1.4.0/dagster/_core/storage/output_manager.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/partition_status_cache.py` & `dagster-1.4.0/dagster/_core/storage/partition_status_cache.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-from typing import Dict, List, NamedTuple, Optional, Sequence, Set, Tuple
+from enum import Enum
+from typing import TYPE_CHECKING, Dict, List, NamedTuple, Optional, Sequence, Set, Tuple
 
 from dagster import (
     AssetKey,
     DagsterEventType,
     DagsterInstance,
     DagsterRunStatus,
     EventLogRecord,
@@ -26,22 +27,34 @@
     MULTIDIMENSIONAL_PARTITION_PREFIX,
     get_dimension_from_partition_tag,
 )
 from dagster._serdes import whitelist_for_serdes
 from dagster._serdes.errors import DeserializationError
 from dagster._serdes.serdes import deserialize_value
 
+if TYPE_CHECKING:
+    from dagster._core.storage.event_log.base import AssetRecord
+
+
 CACHEABLE_PARTITION_TYPES = (
     TimeWindowPartitionsDefinition,
     MultiPartitionsDefinition,
     StaticPartitionsDefinition,
     DynamicPartitionsDefinition,
 )
 
 
+class AssetPartitionStatus(Enum):
+    """The status of asset partition."""
+
+    MATERIALIZED = "MATERIALIZED"
+    IN_PROGRESS = "IN_PROGRESS"
+    FAILED = "FAILED"
+
+
 def is_cacheable_partition_type(partitions_def: PartitionsDefinition) -> bool:
     check.inst_param(partitions_def, "partitions_def", PartitionsDefinition)
     if not isinstance(partitions_def, CACHEABLE_PARTITION_TYPES):
         return False
     if isinstance(partitions_def, MultiPartitionsDefinition):
         return all(
             is_cacheable_partition_type(dimension_def.partitions_def)
@@ -375,66 +388,70 @@
         cursor,
     )
 
 
 def _get_updated_status_cache(
     instance: DagsterInstance,
     asset_key: AssetKey,
-    current_status_cache_value: AssetStatusCacheValue,
+    stored_cache_value: AssetStatusCacheValue,
     partitions_def: Optional[PartitionsDefinition],
     dynamic_partitions_store: DynamicPartitionsStore,
+    latest_materialization_storage_id: Optional[int],
 ) -> AssetStatusCacheValue:
     """This method accepts the current asset status cache value, and fetches unevaluated
     records from the event log. It then updates the cache value with the new materializations.
     """
     # if earliest_in_progress_materialization_event_id is set, we fetch all events including the
     # materialization planned event at that id (hence the - 1). We'll use this to determine if the
     # materialization is still in progress.
     cursor = (
-        current_status_cache_value.earliest_in_progress_materialization_event_id - 1
-        if current_status_cache_value.earliest_in_progress_materialization_event_id
-        else current_status_cache_value.latest_storage_id
+        stored_cache_value.earliest_in_progress_materialization_event_id - 1
+        if stored_cache_value.earliest_in_progress_materialization_event_id
+        else stored_cache_value.latest_storage_id
     )
     unevaluated_planned_event_records = instance.get_event_records(
         event_records_filter=EventRecordsFilter(
             event_type=DagsterEventType.ASSET_MATERIALIZATION_PLANNED,
             asset_key=asset_key,
             after_cursor=cursor,
         )
     )
-    unevaluated_materialization_event_records = instance.get_event_records(
-        event_records_filter=EventRecordsFilter(
-            event_type=DagsterEventType.ASSET_MATERIALIZATION,
-            asset_key=asset_key,
-            after_cursor=cursor,
+    unevaluated_materialization_event_records = (
+        instance.get_event_records(
+            event_records_filter=EventRecordsFilter(
+                event_type=DagsterEventType.ASSET_MATERIALIZATION,
+                asset_key=asset_key,
+                after_cursor=cursor,
+            )
         )
+        if cursor < (latest_materialization_storage_id or 0)
+        else []
     )
 
     if not (unevaluated_materialization_event_records or unevaluated_planned_event_records):
-        return current_status_cache_value
+        return stored_cache_value
 
     unevaluated_event_records = list(unevaluated_planned_event_records)
     unevaluated_event_records.extend(list(unevaluated_materialization_event_records))
 
     latest_storage_id = max([record.storage_id for record in unevaluated_event_records])
     if not partitions_def or not is_cacheable_partition_type(partitions_def):
         return AssetStatusCacheValue(latest_storage_id=latest_storage_id)
 
     check.invariant(
-        current_status_cache_value.partitions_def_id
+        stored_cache_value.partitions_def_id
         == partitions_def.get_serializable_unique_identifier(
             dynamic_partitions_store=dynamic_partitions_store
         )
     )
     materialized_subset: PartitionsSubset = (
         partitions_def.deserialize_subset(
-            current_status_cache_value.serialized_materialized_partition_subset
+            stored_cache_value.serialized_materialized_partition_subset
         )
-        if current_status_cache_value
-        and current_status_cache_value.serialized_materialized_partition_subset
+        if stored_cache_value and stored_cache_value.serialized_materialized_partition_subset
         else partitions_def.empty_subset()
     )
     newly_materialized_partitions = set()
 
     for record in unevaluated_event_records:
         if not record.event_log_entry.dagster_event:
             check.failed("Expected dagster event")
@@ -448,19 +465,16 @@
     materialized_subset = materialized_subset.with_partition_keys(
         get_validated_partition_keys(
             dynamic_partitions_store, partitions_def, newly_materialized_partitions
         )
     )
 
     failed_subset: PartitionsSubset = (
-        partitions_def.deserialize_subset(
-            current_status_cache_value.serialized_failed_partition_subset
-        )
-        if current_status_cache_value
-        and current_status_cache_value.serialized_failed_partition_subset
+        partitions_def.deserialize_subset(stored_cache_value.serialized_failed_partition_subset)
+        if stored_cache_value and stored_cache_value.serialized_failed_partition_subset
         else partitions_def.empty_subset()
     )
 
     (
         failed_subset,
         in_progress_subset,
         new_cursor,
@@ -471,104 +485,94 @@
         failed_subset,
         unevaluated_event_records,
         dynamic_partitions_store=dynamic_partitions_store,
     )
 
     return AssetStatusCacheValue(
         latest_storage_id=latest_storage_id,
-        partitions_def_id=current_status_cache_value.partitions_def_id,
+        partitions_def_id=stored_cache_value.partitions_def_id,
         serialized_materialized_partition_subset=materialized_subset.serialize(),
         serialized_failed_partition_subset=failed_subset.serialize(),
         serialized_in_progress_partition_subset=in_progress_subset.serialize(),
         earliest_in_progress_materialization_event_id=new_cursor,
     )
 
 
-def _fetch_stored_asset_status_cache_value(
-    instance: DagsterInstance, asset_key: AssetKey
-) -> Optional[AssetStatusCacheValue]:
-    asset_records = (
-        instance.get_asset_records()
-        if not asset_key
-        else instance.get_asset_records(asset_keys=[asset_key])
-    )
-    if not asset_records:
-        return None
-    else:
-        return list(asset_records)[0].asset_entry.cached_status
-
-
 def _get_fresh_asset_status_cache_value(
     instance: DagsterInstance,
     asset_key: AssetKey,
     dynamic_partitions_store: DynamicPartitionsStore,
-    partitions_def: Optional[PartitionsDefinition] = None,
+    partitions_def: Optional[PartitionsDefinition],
+    stored_cache_value: Optional[AssetStatusCacheValue],
+    latest_materialization_storage_id: Optional[int],
 ) -> Optional[AssetStatusCacheValue]:
-    cached_status_data = _fetch_stored_asset_status_cache_value(instance, asset_key)
-
     updated_cache_value = None
-    if cached_status_data is None or cached_status_data.partitions_def_id != (
+    if stored_cache_value is None or stored_cache_value.partitions_def_id != (
         partitions_def.get_serializable_unique_identifier(
             dynamic_partitions_store=dynamic_partitions_store
         )
         if partitions_def
         else None
     ):
         planned_event_records = instance.get_event_records(
             event_records_filter=EventRecordsFilter(
                 event_type=DagsterEventType.ASSET_MATERIALIZATION_PLANNED,
                 asset_key=asset_key,
             ),
             limit=1,
         )
-        materialized_event_records = instance.get_event_records(
-            event_records_filter=EventRecordsFilter(
-                event_type=DagsterEventType.ASSET_MATERIALIZATION,
-                asset_key=asset_key,
-            ),
-            limit=1,
-        )
 
-        if materialized_event_records or planned_event_records:
+        if latest_materialization_storage_id or planned_event_records:
             latest_storage_id = max(
-                next(iter(materialized_event_records)).storage_id
-                if materialized_event_records
-                else 0,
+                (latest_materialization_storage_id or 0),
                 next(iter(planned_event_records)).storage_id if planned_event_records else 0,
             )
             updated_cache_value = _build_status_cache(
                 instance=instance,
                 asset_key=asset_key,
                 partitions_def=partitions_def,
                 latest_storage_id=latest_storage_id,
                 dynamic_partitions_store=dynamic_partitions_store,
             )
     else:
         updated_cache_value = _get_updated_status_cache(
             instance=instance,
             asset_key=asset_key,
             partitions_def=partitions_def,
-            current_status_cache_value=cached_status_data,
+            stored_cache_value=stored_cache_value,
             dynamic_partitions_store=dynamic_partitions_store,
+            latest_materialization_storage_id=latest_materialization_storage_id,
         )
 
     return updated_cache_value
 
 
 def get_and_update_asset_status_cache_value(
     instance: DagsterInstance,
     asset_key: AssetKey,
     partitions_def: Optional[PartitionsDefinition] = None,
     dynamic_partitions_loader: Optional[DynamicPartitionsStore] = None,
+    asset_record: Optional["AssetRecord"] = None,
 ) -> Optional[AssetStatusCacheValue]:
+    asset_record = asset_record or next(
+        iter(instance.get_asset_records(asset_keys=[asset_key])), None
+    )
+    if asset_record is None:
+        stored_cache_value, latest_materialization_storage_id = None, None
+    else:
+        stored_cache_value = asset_record.asset_entry.cached_status
+        latest_materialization_storage_id = asset_record.asset_entry.last_materialization_storage_id
+
     updated_cache_value = _get_fresh_asset_status_cache_value(
         instance=instance,
         asset_key=asset_key,
         partitions_def=partitions_def,
         dynamic_partitions_store=dynamic_partitions_loader
         if dynamic_partitions_loader
         else instance,
+        stored_cache_value=stored_cache_value,
+        latest_materialization_storage_id=latest_materialization_storage_id,
     )
-    if updated_cache_value:
+    if updated_cache_value is not None and updated_cache_value != stored_cache_value:
         instance.update_asset_cached_status_data(asset_key, updated_cache_value)
 
     return updated_cache_value
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/root.py` & `dagster-1.4.0/dagster/_core/storage/root.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/runs/base.py` & `dagster-1.4.0/dagster/_core/storage/runs/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -33,15 +33,15 @@
 class RunStorage(ABC, MayHaveInstanceWeakref[T_DagsterInstance], DaemonCursorStorage):
     """Abstract base class for storing pipeline run history.
 
     Note that run storages using SQL databases as backing stores should implement
     :py:class:`~dagster._core.storage.runs.SqlRunStorage`.
 
     Users should not directly instantiate concrete subclasses of this class; they are instantiated
-    by internal machinery when ``dagit`` and ``dagster-graphql`` load, based on the values in the
+    by internal machinery when ``dagster-webserver`` and ``dagster-graphql`` load, based on the values in the
     ``dagster.yaml`` file in ``$DAGSTER_HOME``. Configuration of concrete subclasses of this class
     should be done by setting values in that file.
     """
 
     @abstractmethod
     def add_run(self, dagster_run: DagsterRun) -> DagsterRun:
         """Add a run to storage.
@@ -347,16 +347,16 @@
 
     def optimize(self, print_fn: Optional[PrintFn] = None, force_rebuild_all: bool = False) -> None:
         """Call this method to run any optional data migrations for optimized reads."""
 
     def dispose(self) -> None:
         """Explicit lifecycle management."""
 
-    def optimize_for_dagit(self, statement_timeout: int, pool_recycle: int) -> None:
-        """Allows for optimizing database connection / use in the context of a long lived dagit process.
+    def optimize_for_webserver(self, statement_timeout: int, pool_recycle: int) -> None:
+        """Allows for optimizing database connection / use in the context of a long lived webserver process.
         """
 
     # Daemon Heartbeat Storage
     #
     # Holds heartbeats from the Dagster Daemon so that other system components can alert when it's not
     # alive.
     # This is temporarily placed along with run storage to avoid adding a new instance concept. It
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/runs/in_memory.py` & `dagster-1.4.0/dagster/_core/storage/runs/in_memory.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 from .schema import InstanceInfo, RunStorageSqlMetadata
 from .sql_run_storage import SqlRunStorage
 
 
 class InMemoryRunStorage(SqlRunStorage):
     """In memory only run storage. Used by ephemeral DagsterInstance or for testing purposes.
 
-    WARNING: Dagit and other core functionality will not work if this is used on a real DagsterInstance
+    WARNING: The Dagster UI and other core functionality will not work if this is used on a real DagsterInstance
     """
 
     def __init__(self, preload: Optional[Sequence[DebugRunPayload]] = None):
         self._engine = create_engine(
             create_in_memory_conn_string(f"runs-{uuid.uuid4()}"),
             poolclass=NullPool,
         )
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/runs/migration.py` & `dagster-1.4.0/dagster/_core/storage/runs/migration.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/runs/schema.py` & `dagster-1.4.0/dagster/_core/storage/runs/schema.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/runs/sql_run_storage.py` & `dagster-1.4.0/dagster/_core/storage/runs/sql_run_storage.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/runs/sqlite/alembic/alembic.ini` & `dagster-1.4.0/dagster/_core/storage/runs/sqlite/alembic/alembic.ini`

 * *Files 15% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 # on newly generated revision scripts.  See the documentation for further
 # detail and examples
 
 # format using "black" - use the console_scripts runner, against the "black" entrypoint
 hooks = black
 black.type = console_scripts
 black.entrypoint = black
-black.options = --line-length 100 --target-version py36 --target-version py37 --target-version py38 -S --fast
+black.options = --line-length 100 --target-version py38 --target-version py39 --target-version py310 --target-version py311 -S --fast
 
 # Logging configuration
 [loggers]
 keys = root,sqlalchemy,alembic
 
 [handlers]
 keys = console
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/runs/sqlite/sqlite_run_storage.py` & `dagster-1.4.0/dagster/_core/storage/runs/sqlite/sqlite_run_storage.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,15 +34,15 @@
 MINIMUM_SQLITE_BUCKET_VERSION = [3, 25, 0]
 
 
 class SqliteRunStorage(SqlRunStorage, ConfigurableClass):
     """SQLite-backed run storage.
 
     Users should not directly instantiate this class; it is instantiated by internal machinery when
-    ``dagit`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file in
+    ``dagster-webserver`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file in
     ``$DAGSTER_HOME``. Configuration of this class should be done by setting values in that file.
 
     This is the default run storage when none is specified in the ``dagster.yaml``.
 
     To explicitly specify SQLite for run storage, you can add a block such as the following to your
     ``dagster.yaml``:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/schedules/base.py` & `dagster-1.4.0/dagster/_core/storage/schedules/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -158,25 +158,33 @@
         Args:
             asset_key (AssetKey): The asset key to query
             limit (Optional[int]): The maximum number of evaluations to return
             cursor (Optional[int]): The cursor to paginate from
         """
 
     @abc.abstractmethod
+    def purge_asset_evaluations(self, before: float) -> None:
+        """Wipe evaluations before a certain timestamp.
+
+        Args:
+            before (datetime): All evaluations before this datetime will get purged
+        """
+
+    @abc.abstractmethod
     def upgrade(self) -> None:
         """Perform any needed migrations."""
 
     def migrate(self, print_fn: Optional[PrintFn] = None, force_rebuild_all: bool = False) -> None:
         """Call this method to run any required data migrations."""
 
     def optimize(self, print_fn: Optional[PrintFn] = None, force_rebuild_all: bool = False) -> None:
         """Call this method to run any optional data migrations for optimized reads."""
 
-    def optimize_for_dagit(self, statement_timeout: int, pool_recycle: int) -> None:
-        """Allows for optimizing database connection / use in the context of a long lived dagit process.
+    def optimize_for_webserver(self, statement_timeout: int, pool_recycle: int) -> None:
+        """Allows for optimizing database connection / use in the context of a long lived webserver process.
         """
 
     def alembic_version(self) -> Optional[AlembicVersion]:
         return None
 
     def dispose(self) -> None:
         """Explicit lifecycle management."""
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/schedules/migration.py` & `dagster-1.4.0/dagster/_core/storage/schedules/migration.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/schedules/schema.py` & `dagster-1.4.0/dagster/_core/storage/schedules/schema.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/schedules/sql_schedule_storage.py` & `dagster-1.4.0/dagster/_core/storage/schedules/sql_schedule_storage.py`

 * *Files 2% similar despite different names*

```diff
@@ -507,14 +507,25 @@
 
             if cursor:
                 query = query.where(AssetDaemonAssetEvaluationsTable.c.evaluation_id < cursor)
 
             rows = db_fetch_mappings(conn, query)
             return [AutoMaterializeAssetEvaluationRecord.from_db_row(row) for row in rows]
 
+    def purge_asset_evaluations(self, before: float):
+        check.float_param(before, "before")
+
+        utc_before = utc_datetime_from_timestamp(before)
+        query = AssetDaemonAssetEvaluationsTable.delete().where(
+            AssetDaemonAssetEvaluationsTable.c.create_timestamp < utc_before
+        )
+
+        with self.connect() as conn:
+            conn.execute(query)
+
     def wipe(self) -> None:
         """Clears the schedule storage."""
         with self.connect() as conn:
             # https://stackoverflow.com/a/54386260/324449
             conn.execute(JobTable.delete())
             conn.execute(JobTickTable.delete())
             if self._has_instigators_table(conn):
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/schedules/sqlite/alembic/alembic.ini` & `dagster-1.4.0/dagster/_core/storage/schedules/sqlite/alembic/alembic.ini`

 * *Files 16% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 # on newly generated revision scripts.  See the documentation for further
 # detail and examples
 
 # format using "black" - use the console_scripts runner, against the "black" entrypoint
 hooks = black
 black.type = console_scripts
 black.entrypoint = black
-black.options = --line-length 100 --target-version py36 --target-version py37 --target-version py38 -S --fast
+black.options = --line-length 100 --target-version py38 --target-version py39 --target-version py310 --target-version py311 -S --fast
 
 # Logging configuration
 [loggers]
 keys = root,sqlalchemy,alembic
 
 [handlers]
 keys = console
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/schedules/sqlite/sqlite_schedule_storage.py` & `dagster-1.4.0/dagster/_core/storage/schedules/sqlite/sqlite_schedule_storage.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/sql.py` & `dagster-1.4.0/dagster/_core/storage/sql.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/sqlalchemy_compat.py` & `dagster-1.4.0/dagster/_core/storage/sqlalchemy_compat.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/sqlite.py` & `dagster-1.4.0/dagster/_core/storage/sqlite.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/sqlite_storage.py` & `dagster-1.4.0/dagster/_core/storage/sqlite_storage.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,15 +38,15 @@
     return os.path.join(base, "schedules")
 
 
 class DagsterSqliteStorage(DagsterStorage, ConfigurableClass):
     """SQLite-backed run storage.
 
     Users should not directly instantiate this class; it is instantiated by internal machinery when
-    ``dagit`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file in
+    ``dagster-webserver`` and ``dagster-graphql`` load, based on the values in the ``dagster.yaml`` file in
     ``$DAGSTER_HOME``. Configuration of this class should be done by setting values in that file.
 
     This is the default run storage when none is specified in the ``dagster.yaml``.
 
     To explicitly specify SQLite for run storage, you can add a block such as the following to your
     ``dagster.yaml``:
```

### Comparing `dagster-1.3.9rc0/dagster/_core/storage/tags.py` & `dagster-1.4.0/dagster/_core/storage/tags.py`

 * *Files 3% similar despite different names*

```diff
@@ -37,16 +37,14 @@
 
 MEMOIZED_RUN_TAG = f"{SYSTEM_TAG_PREFIX}is_memoized_run"
 
 STEP_SELECTION_TAG = f"{SYSTEM_TAG_PREFIX}step_selection"
 
 OP_SELECTION_TAG = f"{SYSTEM_TAG_PREFIX}solid_selection"
 
-PRESET_NAME_TAG = f"{SYSTEM_TAG_PREFIX}preset_name"
-
 GRPC_INFO_TAG = f"{HIDDEN_TAG_PREFIX}grpc_info"
 
 SCHEDULED_EXECUTION_TIME_TAG = f"{HIDDEN_TAG_PREFIX}scheduled_execution_time"
 
 RUN_KEY_TAG = f"{SYSTEM_TAG_PREFIX}run_key"
 
 PRIORITY_TAG = f"{SYSTEM_TAG_PREFIX}priority"
@@ -56,14 +54,16 @@
 MAX_RETRIES_TAG = f"{SYSTEM_TAG_PREFIX}max_retries"
 RETRY_NUMBER_TAG = f"{SYSTEM_TAG_PREFIX}retry_number"
 RETRY_STRATEGY_TAG = f"{SYSTEM_TAG_PREFIX}retry_strategy"
 
 MAX_RUNTIME_SECONDS_TAG = f"{SYSTEM_TAG_PREFIX}max_runtime"
 
 AUTO_MATERIALIZE_TAG = f"{SYSTEM_TAG_PREFIX}auto_materialize"
+ASSET_EVALUATION_ID_TAG = f"{SYSTEM_TAG_PREFIX}asset_evaluation_id"
+AUTO_OBSERVE_TAG = f"{SYSTEM_TAG_PREFIX}auto_observe"
 
 USER_EDITABLE_SYSTEM_TAGS = [
     PRIORITY_TAG,
     MAX_RETRIES_TAG,
     RETRY_STRATEGY_TAG,
     MAX_RUNTIME_SECONDS_TAG,
 ]
```

### Comparing `dagster-1.3.9rc0/dagster/_core/system_config/composite_descent.py` & `dagster-1.4.0/dagster/_core/system_config/composite_descent.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/system_config/objects.py` & `dagster-1.4.0/dagster/_core/system_config/objects.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/telemetry.py` & `dagster-1.4.0/dagster/_core/telemetry.py`

 * *Files 6% similar despite different names*

```diff
@@ -50,24 +50,28 @@
 from dagster._core.execution.context.system import PlanOrchestrationContext
 from dagster._core.execution.plan.objects import StepSuccessData
 from dagster._core.instance import DagsterInstance
 from dagster._utils.merger import merge_dicts
 from dagster.version import __version__ as dagster_module_version
 
 if TYPE_CHECKING:
-    from dagster._core.host_representation.external import ExternalJob, ExternalRepository
+    from dagster._core.host_representation.external import (
+        ExternalJob,
+        ExternalRepository,
+        ExternalResource,
+    )
     from dagster._core.workspace.context import IWorkspaceProcessContext
 
 TELEMETRY_STR = ".telemetry"
 INSTANCE_ID_STR = "instance_id"
 ENABLED_STR = "enabled"
 DAGSTER_HOME_FALLBACK = "~/.dagster"
 MAX_BYTES = 10485760  # 10 MB = 10 * 1024 * 1024 bytes
 UPDATE_REPO_STATS = "update_repo_stats"
-START_DAGIT_WEBSERVER = "start_dagit_webserver"
+START_DAGSTER_WEBSERVER = "start_dagit_webserver"
 DAEMON_ALIVE = "daemon_alive"
 SCHEDULED_RUN_CREATED = "scheduled_run_created"
 SENSOR_RUN_CREATED = "sensor_run_created"
 BACKFILL_RUN_CREATED = "backfill_run_created"
 STEP_START_EVENT = "step_start_event"
 STEP_SUCCESS_EVENT = "step_success_event"
 STEP_FAILURE_EVENT = "step_failure_event"
@@ -461,25 +465,27 @@
     )
 
     num_pipelines_in_repo = len(external_repo.get_all_external_jobs())
     num_schedules_in_repo = len(external_repo.get_external_schedules())
     num_sensors_in_repo = len(external_repo.get_external_sensors())
     external_asset_nodes = external_repo.get_external_asset_nodes()
     num_assets_in_repo = len(external_asset_nodes)
+    external_resources = external_repo.get_external_resources()
 
     num_partitioned_assets_in_repo = 0
     num_multi_partitioned_assets_in_repo = 0
     num_dynamic_partitioned_assets_in_repo = 0
     num_assets_with_freshness_policies_in_repo = 0
     num_assets_with_eager_auto_materialize_policies_in_repo = 0
     num_assets_with_lazy_auto_materialize_policies_in_repo = 0
     num_source_assets_in_repo = 0
     num_observable_source_assets_in_repo = 0
     num_dbt_assets_in_repo = 0
     num_assets_with_code_versions_in_repo = 0
+
     for asset in external_asset_nodes:
         if asset.partitions_def_data:
             num_partitioned_assets_in_repo += 1
 
             if isinstance(asset.partitions_def_data, ExternalDynamicPartitionsDefinitionData):
                 num_dynamic_partitioned_assets_in_repo += 1
 
@@ -510,14 +516,15 @@
     num_asset_reconciliation_sensors_in_repo = sum(
         1
         for external_sensor in external_repo.get_external_sensors()
         if external_sensor.name == "asset_reconciliation_sensor"
     )
 
     return {
+        **get_resource_stats(external_resources=list(external_resources)),
         "num_pipelines_in_repo": str(num_pipelines_in_repo),
         "num_schedules_in_repo": str(num_schedules_in_repo),
         "num_sensors_in_repo": str(num_sensors_in_repo),
         "num_assets_in_repo": str(num_assets_in_repo),
         "num_source_assets_in_repo": str(num_source_assets_in_repo),
         "num_partitioned_assets_in_repo": str(num_partitioned_assets_in_repo),
         "num_dynamic_partitioned_assets_in_repo": str(num_dynamic_partitioned_assets_in_repo),
@@ -534,14 +541,35 @@
         "num_observable_source_assets_in_repo": str(num_observable_source_assets_in_repo),
         "num_dbt_assets_in_repo": str(num_dbt_assets_in_repo),
         "num_assets_with_code_versions_in_repo": str(num_assets_with_code_versions_in_repo),
         "num_asset_reconciliation_sensors_in_repo": str(num_asset_reconciliation_sensors_in_repo),
     }
 
 
+def get_resource_stats(external_resources: Sequence["ExternalResource"]) -> Mapping[str, Any]:
+    used_dagster_resources = []
+    used_custom_resources = False
+
+    for resource in external_resources:
+        resource_type = resource.resource_type
+        split_resource_type = resource_type.split(".")
+        module_name = split_resource_type[0]
+        class_name = split_resource_type[-1]
+
+        if resource.is_dagster_maintained:
+            used_dagster_resources.append({"module_name": module_name, "class_name": class_name})
+        else:
+            used_custom_resources = True
+
+    return {
+        "dagster_resources": used_dagster_resources,
+        "has_custom_resources": str(used_custom_resources),
+    }
+
+
 def log_external_repo_stats(
     instance: DagsterInstance,
     source: str,
     external_repo: "ExternalRepository",
     external_job: Optional["ExternalJob"] = None,
 ):
     from dagster._core.host_representation.external import ExternalJob, ExternalRepository
```

### Comparing `dagster-1.3.9rc0/dagster/_core/telemetry_upload.py` & `dagster-1.4.0/dagster/_core/telemetry_upload.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/test_utils.py` & `dagster-1.4.0/dagster/_core/test_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 import re
 import time
 import warnings
 from collections import defaultdict
 from concurrent.futures import Future, ThreadPoolExecutor
 from contextlib import contextmanager
 from signal import Signals
+from threading import Event
 from typing import (
     TYPE_CHECKING,
     AbstractSet,
     Any,
     Callable,
     Dict,
     Iterator,
@@ -622,15 +623,61 @@
 
 class SingleThreadPoolExecutor(ThreadPoolExecutor):
     """Utility class for testing threadpool executor logic which executes functions in a single
     thread, for easier unit testing.
     """
 
     def __init__(self):
-        super().__init__(max_workers=1, thread_name_prefix="sensor_daemon_worker")
+        super().__init__(max_workers=1, thread_name_prefix="single_threaded_worker")
+
+
+class SynchronousThreadPoolExecutor:
+    """Utility class for testing threadpool executor logic which executes functions synchronously for
+    easier unit testing.
+    """
+
+    def __init__(self, **kwargs):
+        pass
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_value, exc_traceback):
+        pass
+
+    def submit(self, fn, *args, **kwargs):
+        future = Future()
+        future.set_result(fn(*args, **kwargs))
+        return future
+
+    def shutdown(self, wait=True):
+        pass
+
+
+class BlockingThreadPoolExecutor(ThreadPoolExecutor):
+    """Utility class for testing thread timing by allowing for manual unblocking of the submitted threaded work.
+    """
+
+    def __init__(self) -> None:
+        self._proceed = Event()
+        super().__init__()
+
+    def submit(self, fn, *args, **kwargs):
+        def _blocked_fn():
+            proceed = self._proceed.wait(60)
+            assert proceed
+            return fn(*args, **kwargs)
+
+        return super().submit(_blocked_fn)
+
+    def allow(self):
+        self._proceed.set()
+
+    def block(self):
+        self._proceed.clear()
 
 
 def ignore_warning(message_substr: str):
     """Ignores warnings within the decorated function that contain the given string."""
 
     def decorator(func: Callable):
         def wrapper(*args, **kwargs):
```

### Comparing `dagster-1.3.9rc0/dagster/_core/types/builtin_config_schemas.py` & `dagster-1.4.0/dagster/_core/types/builtin_config_schemas.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/config_schema.py` & `dagster-1.4.0/dagster/_core/types/config_schema.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/dagster_type.py` & `dagster-1.4.0/dagster/_core/types/dagster_type.py`

 * *Files 1% similar despite different names*

```diff
@@ -155,14 +155,23 @@
 
         self._metadata = normalize_metadata(
             check.opt_mapping_param(metadata, "metadata", key_type=str),
         )
 
     @public
     def type_check(self, context: "TypeCheckContext", value: object) -> TypeCheck:
+        """Type check the value against the type.
+
+        Args:
+            context (TypeCheckContext): The context of the type check.
+            value (Any): The value to check.
+
+        Returns:
+            TypeCheck: The result of the type check.
+        """
         retval = self._type_check_fn(context, value)
 
         if not isinstance(retval, (bool, TypeCheck)):
             raise DagsterInvariantViolationError(
                 (
                     "You have returned {retval} of type {retval_type} from the type "
                     'check function of type "{type_key}". Return value must be instance '
@@ -189,14 +198,15 @@
     @property
     def metadata(self) -> t.Mapping[str, MetadataValue]:
         return self._metadata
 
     @public
     @property
     def required_resource_keys(self) -> TypingAbstractSet[str]:
+        """AbstractSet[str]: Set of resource keys required by the type check function."""
         return self._required_resource_keys
 
     @public
     @property
     def display_name(self) -> str:
         """Either the name or key (if name is `None`) of the type, overridden in many subclasses."""
         return cast(str, self._name or self.key)
@@ -212,29 +222,33 @@
             f"unique_name requested but is None for type {self.display_name}",
         )
         return self._name
 
     @public
     @property
     def has_unique_name(self) -> bool:
+        """bool: Whether the type has a unique name."""
         return self._name is not None
 
     @public
     @property
     def typing_type(self) -> t.Any:
+        """Any: The python typing type for this type."""
         return self._typing_type
 
     @public
     @property
     def loader(self) -> t.Optional[DagsterTypeLoader]:
+        """Optional[DagsterTypeLoader]: Loader for this type, if any."""
         return self._loader
 
     @public
     @property
     def description(self) -> t.Optional[str]:
+        """Optional[str]: Description of the type, or None if not provided."""
         return self._description
 
     @property
     def inner_types(self) -> t.Sequence["DagsterType"]:
         return []
 
     @property
```

### Comparing `dagster-1.3.9rc0/dagster/_core/types/decorator.py` & `dagster-1.4.0/dagster/_core/types/decorator.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/loadable_target_origin.py` & `dagster-1.4.0/dagster/_core/types/loadable_target_origin.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/primitive_mapping.py` & `dagster-1.4.0/dagster/_core/types/primitive_mapping.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/python_dict.py` & `dagster-1.4.0/dagster/_core/types/python_dict.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/python_set.py` & `dagster-1.4.0/dagster/_core/types/python_set.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/python_tuple.py` & `dagster-1.4.0/dagster/_core/types/python_tuple.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/types/transform_typing.py` & `dagster-1.4.0/dagster/_core/types/transform_typing.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/utility_ops.py` & `dagster-1.4.0/dagster/_core/utility_ops.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/utils.py` & `dagster-1.4.0/dagster/_core/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/workspace/autodiscovery.py` & `dagster-1.4.0/dagster/_core/workspace/autodiscovery.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/workspace/config_schema.py` & `dagster-1.4.0/dagster/_core/workspace/config_schema.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/workspace/context.py` & `dagster-1.4.0/dagster/_core/workspace/context.py`

 * *Files 2% similar despite different names*

```diff
@@ -62,15 +62,15 @@
         ExternalPartitionNamesData,
         ExternalPartitionSetExecutionParamData,
         ExternalPartitionTagsData,
     )
 
 T = TypeVar("T")
 
-DAGIT_GRPC_SERVER_HEARTBEAT_TTL = 45
+WEBSERVER_GRPC_SERVER_HEARTBEAT_TTL = 45
 
 
 class BaseWorkspaceRequestContext(IWorkspace):
     """This class is a request-scoped object that stores (1) a reference to all repository locations
     that exist on the `IWorkspaceProcessContext` at the start of the request and (2) a snapshot of the
     workspace at the start of the request.
 
@@ -308,21 +308,25 @@
         self,
         instance: DagsterInstance,
         workspace_snapshot: Mapping[str, CodeLocationEntry],
         process_context: "IWorkspaceProcessContext",
         version: Optional[str],
         source: Optional[object],
         read_only: bool,
+        read_only_locations: Optional[Mapping[str, bool]] = None,
     ):
         self._instance = instance
         self._workspace_snapshot = workspace_snapshot
         self._process_context = process_context
         self._version = version
         self._source = source
         self._read_only = read_only
+        self._read_only_locations = check.opt_mapping_param(
+            read_only_locations, "read_only_locations"
+        )
         self._checked_permissions: Set[str] = set()
 
     @property
     def instance(self) -> DagsterInstance:
         return self._instance
 
     def get_workspace_snapshot(self) -> Mapping[str, CodeLocationEntry]:
@@ -350,14 +354,16 @@
         return self._read_only
 
     @property
     def permissions(self) -> Mapping[str, PermissionResult]:
         return get_user_permissions(self._read_only)
 
     def permissions_for_location(self, *, location_name: str) -> Mapping[str, PermissionResult]:
+        if location_name in self._read_only_locations:
+            return get_location_scoped_user_permissions(self._read_only_locations[location_name])
         return get_location_scoped_user_permissions(self._read_only)
 
     def has_permission(self, permission: str) -> bool:
         permissions = self.permissions
         check.invariant(
             permission in permissions, f"Permission {permission} not listed in permissions map"
         )
@@ -370,21 +376,21 @@
 
     def was_permission_checked(self, permission: str) -> bool:
         return permission in self._checked_permissions
 
     @property
     def source(self) -> Optional[object]:
         """The source of the request this WorkspaceRequestContext originated from.
-        For example in Dagit this object represents the web request.
+        For example in the webserver this object represents the web request.
         """
         return self._source
 
 
 class IWorkspaceProcessContext(ABC):
-    """Class that stores process-scoped information about a dagit session.
+    """Class that stores process-scoped information about a webserver session.
     In most cases, you will want to create a `BaseWorkspaceRequestContext` to create a request-scoped
     object.
     """
 
     @abstractmethod
     def create_request_context(self, source: Optional[Any] = None) -> BaseWorkspaceRequestContext:
         """Create a usable fixed context for the scope of a request.
@@ -478,15 +484,15 @@
                 grpc_server_registry, "grpc_server_registry", GrpcServerRegistry
             )
         else:
             self._grpc_server_registry = self._stack.enter_context(
                 GrpcServerRegistry(
                     instance_ref=self._instance.get_ref(),
                     reload_interval=0,
-                    heartbeat_ttl=DAGIT_GRPC_SERVER_HEARTBEAT_TTL,
+                    heartbeat_ttl=WEBSERVER_GRPC_SERVER_HEARTBEAT_TTL,
                     startup_timeout=instance.code_server_process_startup_timeout,
                     log_level=code_server_log_level,
                     wait_for_processes_on_shutdown=instance.wait_for_local_code_server_processes_on_shutdown,
                 )
             )
 
         self._location_entry_dict: Dict[str, CodeLocationEntry] = {}
@@ -719,21 +725,21 @@
             LocationStateChangeEventType.LOCATION_UPDATED,
             LocationStateChangeEventType.LOCATION_ERROR,
         ):
             # In case of an updated location, reload the handle to get updated repository data and
             # re-attach a subscriber
             # In case of a location error, just reload the handle in order to update the workspace
             # with the correct error messages
-            logging.getLogger("dagit").info(
+            logging.getLogger("dagster-webserver").info(
                 f"Received {event.event_type} event for location {event.location_name}, refreshing"
             )
             self.refresh_code_location(event.location_name)
 
     def refresh_code_location(self, name: str) -> None:
-        # This method reloads Dagit's copy of the code from the remote gRPC server without
+        # This method reloads the webserver's copy of the code from the remote gRPC server without
         # restarting it, and returns a new request context created from the updated process context
         new = self._load_location(self._location_entry_dict[name].origin, reload=False)
         with self._lock:
             # Relying on GC to clean up the old location once nothing else
             # is referencing it
             self._location_entry_dict[name] = new
```

### Comparing `dagster-1.3.9rc0/dagster/_core/workspace/load.py` & `dagster-1.4.0/dagster/_core/workspace/load.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/workspace/load_target.py` & `dagster-1.4.0/dagster/_core/workspace/load_target.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
 
         dagster_block = data.get("tool", {}).get("dagster", {})
         if "module_name" in dagster_block:
             return ModuleTarget(
                 module_name=dagster_block["module_name"],
                 attribute=None,
                 working_directory=os.getcwd(),
-                location_name=None,
+                location_name=dagster_block.get("code_location_name"),
             ).create_origins()
         return []
 
 
 class PyProjectFileTarget(NamedTuple("PyProjectFileTarget", [("path", str)]), WorkspaceLoadTarget):
     def create_origins(self) -> Sequence[CodeLocationOrigin]:
         return get_origins_from_toml(self.path)
```

### Comparing `dagster-1.3.9rc0/dagster/_core/workspace/permissions.py` & `dagster-1.4.0/dagster/_core/workspace/permissions.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_core/workspace/workspace.py` & `dagster-1.4.0/dagster/_core/workspace/workspace.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_daemon/__init__.py` & `dagster-1.4.0/dagster/_daemon/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from .auto_run_reexecution.event_log_consumer import (
     EventLogConsumerDaemon as EventLogConsumerDaemon,
     get_new_cursor as get_new_cursor,
 )
 from .backfill import execute_backfill_iteration as execute_backfill_iteration
-from .cli import run_command as run_command
 from .controller import (
     DEFAULT_DAEMON_ERROR_INTERVAL_SECONDS as DEFAULT_DAEMON_ERROR_INTERVAL_SECONDS,
     DEFAULT_DAEMON_HEARTBEAT_TOLERANCE_SECONDS as DEFAULT_DAEMON_HEARTBEAT_TOLERANCE_SECONDS,
     DagsterDaemonController as DagsterDaemonController,
     all_daemons_healthy as all_daemons_healthy,
     all_daemons_live as all_daemons_live,
     create_daemon_grpc_server_registry as create_daemon_grpc_server_registry,
```

### Comparing `dagster-1.3.9rc0/dagster/_daemon/asset_daemon.py` & `dagster-1.4.0/dagster/_daemon/asset_daemon.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,25 +1,35 @@
 from typing import Optional
 
+import pendulum
+
 import dagster._check as check
 from dagster._core.definitions.asset_reconciliation_sensor import (
     AssetReconciliationCursor,
     reconcile,
 )
 from dagster._core.definitions.external_asset_graph import ExternalAssetGraph
+from dagster._core.definitions.run_request import RunRequest
 from dagster._core.definitions.selector import JobSubsetSelector
 from dagster._core.instance import DagsterInstance
-from dagster._core.storage.dagster_run import DagsterRunStatus
-from dagster._core.storage.tags import AUTO_MATERIALIZE_TAG
+from dagster._core.storage.dagster_run import DagsterRun, DagsterRunStatus
+from dagster._core.storage.tags import (
+    ASSET_EVALUATION_ID_TAG,
+    AUTO_MATERIALIZE_TAG,
+    AUTO_OBSERVE_TAG,
+)
 from dagster._core.workspace.context import IWorkspaceProcessContext
+from dagster._core.workspace.workspace import IWorkspace
 from dagster._daemon.daemon import DaemonIterator, IntervalDaemon
 
 CURSOR_KEY = "ASSET_DAEMON_CURSOR"
 ASSET_DAEMON_PAUSED_KEY = "ASSET_DAEMON_PAUSED"
 
+EVALUATIONS_TTL_DAYS = 7
+
 
 def get_auto_materialize_paused(instance: DagsterInstance) -> bool:
     return (
         instance.daemon_cursor_storage.get_cursor_values({ASSET_DAEMON_PAUSED_KEY}).get(
             ASSET_DAEMON_PAUSED_KEY
         )
         != "false"
@@ -74,19 +84,24 @@
                 " migrate` to enable."
             )
 
         workspace = workspace_process_context.create_request_context()
         asset_graph = ExternalAssetGraph.from_workspace(workspace)
         target_asset_keys = {
             target_key
-            for target_key in asset_graph.non_source_asset_keys
+            for target_key in asset_graph.materializable_asset_keys
             if asset_graph.get_auto_materialize_policy(target_key) is not None
         }
 
-        if not target_asset_keys:
+        has_auto_observe_assets = any(
+            asset_graph.get_auto_observe_interval_minutes(key) is not None
+            for key in asset_graph.source_asset_keys
+        )
+
+        if not target_asset_keys and not has_auto_observe_assets:
             yield
             return
 
         raw_cursor = _get_raw_cursor(instance)
         cursor = (
             AssetReconciliationCursor.from_serialized(raw_cursor, asset_graph)
             if raw_cursor
@@ -94,80 +109,115 @@
         )
 
         run_requests, new_cursor, evaluations = reconcile(
             asset_graph=asset_graph,
             target_asset_keys=target_asset_keys,
             instance=instance,
             cursor=cursor,
-            run_tags=None,
+            materialize_run_tags={
+                AUTO_MATERIALIZE_TAG: "true",
+                **instance.auto_materialize_run_tags,
+            },
+            observe_run_tags={AUTO_OBSERVE_TAG: "true"},
+            auto_observe=True,
         )
 
+        evaluations_by_asset_key = {evaluation.asset_key: evaluation for evaluation in evaluations}
+
         for run_request in run_requests:
             yield
 
             asset_keys = check.not_none(run_request.asset_selection)
-            check.invariant(len(asset_keys) > 0)
-
-            repo_handle = asset_graph.get_repository_handle(asset_keys[0])
-
-            # Check that all asset keys are from the same repo
-            for key in asset_keys[1:]:
-                check.invariant(repo_handle == asset_graph.get_repository_handle(key))
-
-            location_name = repo_handle.code_location_origin.location_name
-            repository_name = repo_handle.repository_name
-            job_name = check.not_none(asset_graph.get_implicit_job_name_for_assets(asset_keys))
-
-            code_location = workspace.get_code_location(location_name)
-            external_job = code_location.get_external_job(
-                JobSubsetSelector(
-                    location_name=location_name,
-                    repository_name=repository_name,
-                    job_name=job_name,
-                    op_selection=None,
-                    asset_selection=asset_keys,
-                )
-            )
-
-            tags = {
-                **run_request.tags,
-                AUTO_MATERIALIZE_TAG: "true",
-                **instance.auto_materialize_run_tags,
-            }
 
-            external_execution_plan = code_location.get_external_execution_plan(
-                external_job,
-                run_request.run_config,
-                step_keys_to_execute=None,
-                known_state=None,
-                instance=instance,
+            run = submit_asset_run(
+                run_request._replace(
+                    tags={
+                        **run_request.tags,
+                        ASSET_EVALUATION_ID_TAG: str(new_cursor.evaluation_id),
+                    }
+                ),
+                instance,
+                workspace,
+                asset_graph,
             )
-            execution_plan_snapshot = external_execution_plan.execution_plan_snapshot
 
-            run = instance.create_run(
-                job_name=external_job.name,
-                run_id=None,
-                run_config=None,
-                resolved_op_selection=None,
-                step_keys_to_execute=None,
-                status=DagsterRunStatus.NOT_STARTED,
-                op_selection=None,
-                root_run_id=None,
-                parent_run_id=None,
-                tags=tags,
-                job_snapshot=external_job.job_snapshot,
-                execution_plan_snapshot=execution_plan_snapshot,
-                parent_job_snapshot=external_job.parent_job_snapshot,
-                external_job_origin=external_job.get_external_origin(),
-                job_code_origin=external_job.get_python_origin(),
-                asset_selection=frozenset(asset_keys),
-            )
-            instance.submit_run(run.run_id, workspace)
+            # add run id to evaluations
+            for asset_key in asset_keys:
+                # asset keys for observation runs don't have evaluations
+                if asset_key in evaluations_by_asset_key:
+                    evaluation = evaluations_by_asset_key[asset_key]
+                    evaluations_by_asset_key[asset_key] = evaluation._replace(
+                        run_ids=evaluation.run_ids | {run.run_id}
+                    )
 
         instance.daemon_cursor_storage.set_cursor_values({CURSOR_KEY: new_cursor.serialize()})
 
         # We enforce uniqueness per (asset key, evaluation id). Store the evaluations after the cursor,
         # so that if the daemon crashes and doesn't update the cursor we don't try to write duplicates.
         if schedule_storage.supports_auto_materialize_asset_evaluations:
             schedule_storage.add_auto_materialize_asset_evaluations(
-                check.not_none(new_cursor.evaluation_id), evaluations
+                new_cursor.evaluation_id, list(evaluations_by_asset_key.values())
             )
+            schedule_storage.purge_asset_evaluations(
+                before=pendulum.now("UTC").subtract(days=EVALUATIONS_TTL_DAYS).timestamp(),
+            )
+
+
+def submit_asset_run(
+    run_request: RunRequest,
+    instance: DagsterInstance,
+    workspace: IWorkspace,
+    asset_graph: ExternalAssetGraph,
+) -> DagsterRun:
+    asset_keys = check.not_none(run_request.asset_selection)
+    check.invariant(len(asset_keys) > 0)
+
+    repo_handle = asset_graph.get_repository_handle(asset_keys[0])
+
+    # Check that all asset keys are from the same repo
+    for key in asset_keys[1:]:
+        check.invariant(repo_handle == asset_graph.get_repository_handle(key))
+
+    location_name = repo_handle.code_location_origin.location_name
+    repository_name = repo_handle.repository_name
+    job_name = check.not_none(asset_graph.get_implicit_job_name_for_assets(asset_keys))
+
+    code_location = workspace.get_code_location(location_name)
+    external_job = code_location.get_external_job(
+        JobSubsetSelector(
+            location_name=location_name,
+            repository_name=repository_name,
+            job_name=job_name,
+            op_selection=None,
+            asset_selection=asset_keys,
+        )
+    )
+
+    external_execution_plan = code_location.get_external_execution_plan(
+        external_job,
+        run_request.run_config,
+        step_keys_to_execute=None,
+        known_state=None,
+        instance=instance,
+    )
+    execution_plan_snapshot = external_execution_plan.execution_plan_snapshot
+
+    run = instance.create_run(
+        job_name=external_job.name,
+        run_id=None,
+        run_config=None,
+        resolved_op_selection=None,
+        step_keys_to_execute=None,
+        status=DagsterRunStatus.NOT_STARTED,
+        op_selection=None,
+        root_run_id=None,
+        parent_run_id=None,
+        tags=run_request.tags,
+        job_snapshot=external_job.job_snapshot,
+        execution_plan_snapshot=execution_plan_snapshot,
+        parent_job_snapshot=external_job.parent_job_snapshot,
+        external_job_origin=external_job.get_external_origin(),
+        job_code_origin=external_job.get_python_origin(),
+        asset_selection=frozenset(asset_keys),
+    )
+    instance.submit_run(run.run_id, workspace)
+    return run
```

### Comparing `dagster-1.3.9rc0/dagster/_daemon/auto_run_reexecution/auto_run_reexecution.py` & `dagster-1.4.0/dagster/_daemon/auto_run_reexecution/auto_run_reexecution.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_daemon/auto_run_reexecution/event_log_consumer.py` & `dagster-1.4.0/dagster/_daemon/auto_run_reexecution/event_log_consumer.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_daemon/backfill.py` & `dagster-1.4.0/dagster/_daemon/backfill.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,15 +28,15 @@
         backfill_id = backfill_job.backfill_id
 
         # refetch, in case the backfill was updated in the meantime
         backfill = cast(PartitionBackfill, instance.get_backfill(backfill_id))
         try:
             if backfill.is_asset_backfill:
                 yield from execute_asset_backfill_iteration(
-                    backfill, workspace_process_context, instance
+                    backfill, logger, workspace_process_context, instance
                 )
             else:
                 yield from execute_job_backfill_iteration(
                     backfill, logger, workspace_process_context, debug_crash_flags, instance
                 )
         except Exception:
             error_info = serializable_error_info_from_exc_info(sys.exc_info())
```

### Comparing `dagster-1.3.9rc0/dagster/_daemon/cli/__init__.py` & `dagster-1.4.0/dagster/_daemon/cli/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -147,8 +147,8 @@
     return group
 
 
 cli = create_dagster_daemon_cli()
 
 
 def main() -> None:
-    cli(obj={})  # pylint:disable=E1123
+    cli(obj={})
```

### Comparing `dagster-1.3.9rc0/dagster/_daemon/controller.py` & `dagster-1.4.0/dagster/_daemon/controller.py`

 * *Files 1% similar despite different names*

```diff
@@ -44,16 +44,18 @@
 
 THREAD_CHECK_INTERVAL = 5
 
 HEARTBEAT_CHECK_INTERVAL = 15
 
 RELOAD_WORKSPACE_INTERVAL = 60
 
-DAEMON_GRPC_SERVER_RELOAD_INTERVAL = 60
-DAEMON_GRPC_SERVER_HEARTBEAT_TTL = 120
+# Amount of time that a local code server spun up by the daemon will keep running
+# after it is no longer receiving any heartbeat pings - for this duration there may be
+# multiple code server processes running
+DAEMON_GRPC_SERVER_HEARTBEAT_TTL = 20
 
 
 def _sorted_quoted(strings: Iterable[str]) -> str:
     return "[" + ", ".join([f"'{s}'" for s in sorted(list(strings))]) + "]"
 
 
 def create_daemons_from_instance(instance: DagsterInstance) -> Sequence[DagsterDaemon]:
```

### Comparing `dagster-1.3.9rc0/dagster/_daemon/daemon.py` & `dagster-1.4.0/dagster/_daemon/daemon.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_daemon/monitoring/concurrency.py` & `dagster-1.4.0/dagster/_daemon/monitoring/concurrency.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_daemon/monitoring/run_monitoring.py` & `dagster-1.4.0/dagster/_daemon/monitoring/run_monitoring.py`

 * *Files 0% similar despite different names*

```diff
@@ -197,15 +197,15 @@
                 monitor_canceling_run(instance, run_record, logger)
                 pass
             else:
                 check.invariant(False, f"Unexpected run status: {run_record.dagster_run.status}")
         except Exception:
             error_info = serializable_error_info_from_exc_info(sys.exc_info())
             logger.error(
-                f"Hit error while monitoring run {run_record.dagster_run.run_id}: {str(error_info)}"
+                f"Hit error while monitoring run {run_record.dagster_run.run_id}: {error_info}"
             )
             yield error_info
         else:
             yield
 
 
 def check_run_timeout(
```

### Comparing `dagster-1.3.9rc0/dagster/_daemon/run_coordinator/queued_run_coordinator_daemon.py` & `dagster-1.4.0/dagster/_daemon/run_coordinator/queued_run_coordinator_daemon.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_daemon/sensor.py` & `dagster-1.4.0/dagster/_daemon/sensor.py`

 * *Files 2% similar despite different names*

```diff
@@ -376,29 +376,29 @@
             if (
                 location_name not in workspace_snapshot
                 or not workspace_snapshot[location_name].code_location
             ):
                 logger.warning(
                     f"Sensor {sensor_name} was started from a location "
                     f"{location_name} that can no longer be found in the workspace. "
-                    "You can turn off this sensor in the Dagit UI from the Status tab."
+                    "You can turn off this sensor in the Dagster UI from the Status tab."
                 )
             elif not check.not_none(  # checked above
                 workspace_snapshot[location_name].code_location
             ).has_repository(repo_name):
                 logger.warning(
                     f"Could not find repository {repo_name} in location {location_name} to "
                     + f"run sensor {sensor_name}. If this repository no longer exists, you can "
-                    + "turn off the sensor in the Dagit UI from the Status tab.",
+                    + "turn off the sensor in the Dagster UI from the Status tab.",
                 )
             else:
                 logger.warning(
                     (
                         f"Could not find sensor {sensor_name} in repository {repo_name}. If this "
-                        "sensor no longer exists, you can turn it off in the Dagit UI from the "
+                        "sensor no longer exists, you can turn it off in the Dagster UI from the "
                         "Status tab."
                     ),
                 )
 
     if not sensors:
         if log_verbose_checks:
             logger.info("Not checking for any runs since no sensors have been started.")
@@ -585,26 +585,29 @@
     run: Union[SkippedSensorRun, DagsterRun]
 
 
 def _submit_run_request(
     run_request: RunRequest,
     workspace_process_context: IWorkspaceProcessContext,
     external_sensor: ExternalSensor,
-    code_location: CodeLocation,
     existing_runs_by_key,
     logger,
     sensor_debug_crash_flags,
 ) -> SubmitRunRequestResult:
     instance = workspace_process_context.instance
     sensor_origin = external_sensor.get_external_origin()
 
     target_data: ExternalTargetData = check.not_none(
         external_sensor.get_target_data(run_request.job_name)
     )
 
+    # reload the code_location on each submission, request_context derived data can become out date
+    # * non-threaded: if number of serial submissions is too many
+    # * threaded: if thread sits pending in pool too long
+    code_location = _get_code_location_for_sensor(workspace_process_context, external_sensor)
     job_subset_selector = JobSubsetSelector(
         location_name=code_location.name,
         repository_name=sensor_origin.external_repository_origin.repository_name,
         job_name=target_data.job_name,
         op_selection=target_data.op_selection,
         asset_selection=run_request.asset_selection,
     )
@@ -632,39 +635,42 @@
         logger.info(
             "Completed launch of run {run_id} for {sensor_name}".format(
                 run_id=run.run_id, sensor_name=external_sensor.name
             )
         )
     except Exception:
         error_info = serializable_error_info_from_exc_info(sys.exc_info())
-        logger.error(
-            f"Run {run.run_id} created successfully but failed to launch: {str(error_info)}"
-        )
+        logger.error(f"Run {run.run_id} created successfully but failed to launch: {error_info}")
 
     _check_for_debug_crash(sensor_debug_crash_flags, "RUN_LAUNCHED")
     return SubmitRunRequestResult(run_key=run_request.run_key, error_info=error_info, run=run)
 
 
+def _get_code_location_for_sensor(
+    workspace_process_context: IWorkspaceProcessContext,
+    external_sensor: ExternalSensor,
+):
+    sensor_origin = external_sensor.get_external_origin()
+    return workspace_process_context.create_request_context().get_code_location(
+        sensor_origin.external_repository_origin.code_location_origin.location_name
+    )
+
+
 def _evaluate_sensor(
     workspace_process_context: IWorkspaceProcessContext,
     context: SensorLaunchContext,
     external_sensor: ExternalSensor,
     state: InstigatorState,
     submit_threadpool_executor: Optional[ThreadPoolExecutor],
     sensor_debug_crash_flags: Optional[SingleInstigatorDebugCrashFlags] = None,
 ):
     instance = workspace_process_context.instance
     context.logger.info(f"Checking for new runs for sensor: {external_sensor.name}")
-
-    sensor_origin = external_sensor.get_external_origin()
+    code_location = _get_code_location_for_sensor(workspace_process_context, external_sensor)
     repository_handle = external_sensor.handle.repository_handle
-    code_location = workspace_process_context.create_request_context().get_code_location(
-        sensor_origin.external_repository_origin.code_location_origin.location_name
-    )
-
     instigator_data = _sensor_instigator_data(state)
 
     sensor_runtime_data = code_location.get_external_sensor_execution_data(
         instance,
         repository_handle,
         external_sensor.name,
         instigator_data.last_tick_timestamp if instigator_data else None,
@@ -823,15 +829,14 @@
 
         run_requests.append(run_request)
 
     submit_run_request = lambda run_request: _submit_run_request(
         run_request,
         workspace_process_context,
         external_sensor,
-        code_location,
         existing_runs_by_key,
         context.logger,
         sensor_debug_crash_flags,
     )
 
     if submit_threadpool_executor:
         gen_run_request_results = submit_threadpool_executor.map(submit_run_request, run_requests)
```

### Comparing `dagster-1.3.9rc0/dagster/_daemon/types.py` & `dagster-1.4.0/dagster/_daemon/types.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_daemon/workspace.py` & `dagster-1.4.0/dagster/_daemon/workspace.py`

 * *Files 1% similar despite different names*

```diff
@@ -95,15 +95,15 @@
     def __exit__(self, exception_type, exception_value, traceback):
         self.cleanup(cleanup_locations=True)
 
 
 class DaemonIterationWorkspace(BaseDaemonWorkspace):
     """A copy of the main workspace's locations that can be called from a background thread
     in a daemon without worrying that the main thread will clean up the locations underneath us.
-    Analagous to WorkspaceRequestContext in Dagit.
+    Analagous to WorkspaceRequestContext in the webserver.
 
     Daemons that call this should be careful to set cleanup_locations=False when calling cleanup
     on the parent workspace that get_workspace_copy_for_iteration() was called on to create
     this workspace.
     """
 
     def __init__(self, location_entries_copy):
```

### Comparing `dagster-1.3.9rc0/dagster/_generate/download.py` & `dagster-1.4.0/dagster/_generate/download.py`

 * *Files 13% similar despite different names*

```diff
@@ -4,31 +4,30 @@
 from io import BytesIO
 
 import click
 import requests
 
 from .generate import _should_skip_file
 
-# Currently we only download from 'master' branch
-DEFAULT_GITHUB_URL = "https://codeload.github.com/dagster-io/dagster/tar.gz/master"
 # Examples aren't that can't be downloaded from the dagster project CLI
-EXAMPLES_TO_IGNORE = ["docs_snippets", "experimental"]
+EXAMPLES_TO_IGNORE = ["docs_snippets", "experimental", "temp_pins.txt"]
 # Hardcoded list of available examples. The list is tested against the examples folder in this mono
 # repo to make sure it's up-to-date.
 AVAILABLE_EXAMPLES = [
     "assets_dbt_python",
     "assets_dynamic_partitions",
     "assets_modern_data_stack",
     "assets_pandas_pyspark",
     "assets_pandas_type_metadata",
     "assets_smoke_test",
     "quickstart_aws",
     "quickstart_etl",
     "quickstart_gcp",
     "quickstart_snowflake",
+    "tutorial",
     "tutorial_dbt_dagster",
     "tutorial_notebook_assets",
     "deploy_docker",
     "deploy_ecs",
     "deploy_k8s",
     "development_to_production",
     "feature_graph_backed_assets",
@@ -38,43 +37,55 @@
     "with_great_expectations",
     "with_pyspark",
     "with_pyspark_emr",
     "with_wandb",
 ]
 
 
-def download_example_from_github(path: str, example: str):
+def _get_target_for_version(version: str) -> str:
+    if version == "1!0+dev":
+        target = "master"
+    else:
+        target = version
+    return target
+
+
+def _get_url_for_version(version: str) -> str:
+    return (
+        f"https://codeload.github.com/dagster-io/dagster/tar.gz/{_get_target_for_version(version)}"
+    )
+
+
+def download_example_from_github(path: str, example: str, version: str):
     if example not in AVAILABLE_EXAMPLES:
         click.echo(
             click.style(
                 f'Example "{example}" not available from the `dagster project` CLI. ', fg="red"
             )
             + "\nPlease specify the name of an official Dagster example. "
             + "You can find the available examples via `dagster project list-examples`."
         )
         sys.exit(1)
 
-    path_to_new_project = os.path.normpath(path)
-    path_to_selected_example = f"dagster-master/examples/{example}"
-
+    path_to_selected_example = f"dagster-{_get_target_for_version(version)}/examples/{example}/"
     click.echo(f"Downloading example '{example}'. This may take a while.")
 
-    response = requests.get(DEFAULT_GITHUB_URL, stream=True)
+    response = requests.get(_get_url_for_version(version), stream=True)
     with tarfile.open(fileobj=BytesIO(response.raw.read()), mode="r:gz") as tar_file:
         # Extract the selected example folder to destination
         subdir_and_files = [
             tarinfo
             for tarinfo in tar_file.getmembers()
             if tarinfo.name.startswith(path_to_selected_example)
         ]
         for member in subdir_and_files:
             if _should_skip_file(member.name):
                 continue
 
-            dest = member.name.replace(path_to_selected_example, path_to_new_project)
+            dest = member.name.replace(path_to_selected_example, path)
 
             if member.isdir():
                 os.mkdir(dest)
             elif member.isreg():
                 fileobject = tar_file.extractfile(member)
 
                 with open(dest, "wb") as f:
```

### Comparing `dagster-1.3.9rc0/dagster/_generate/generate.py` & `dagster-1.4.0/dagster/_generate/generate.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_generate/templates/PROJECT_NAME_PLACEHOLDER/README.md` & `dagster-1.4.0/dagster/_generate/templates/PROJECT_NAME_PLACEHOLDER/README.md`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/__generated__/api_pb2.py` & `dagster-1.4.0/dagster/_grpc/__generated__/api_pb2.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/__generated__/api_pb2_grpc.py` & `dagster-1.4.0/dagster/_grpc/__generated__/api_pb2_grpc.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/__init__.py` & `dagster-1.4.0/dagster/_grpc/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Tools for accessing core Dagster APIs over a GRPC mechanism.
 
 GRPC is intended to be used in all cases where host processes communicate with user processes, both
 locally (over UDS on MacOS and Unix, and over a local port on Windows) and when communicating with
 remote Dagster user proceses (e.g., containers).
 
 The GRPC layer is not intended to supplant the dagster-graphql layer, which should still be used to
-drive web frontends like dagit.
+drive web frontends like the Dagster UI.
 """
 
 from .client import (
     DagsterGrpcClient as DagsterGrpcClient,
     client_heartbeat_thread as client_heartbeat_thread,
     ephemeral_grpc_api_client as ephemeral_grpc_api_client,
 )
```

### Comparing `dagster-1.3.9rc0/dagster/_grpc/client.py` & `dagster-1.4.0/dagster/_grpc/client.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/compile.py` & `dagster-1.4.0/dagster/_grpc/compile.py`

 * *Files 2% similar despite different names*

```diff
@@ -101,21 +101,21 @@
             [
                 sys.executable,
                 "-m",
                 "black",
                 "-l",
                 "100",
                 "-t",
-                "py35",
+                "py38",
                 "-t",
-                "py36",
+                "py39",
                 "-t",
-                "py37",
+                "py310",
                 "-t",
-                "py38",
+                "py311",
                 generated_dir,
             ]
         )
 
     # Run isort if it's available. This is under a conditional because isort may not be available in
     # a test environment.
     if "isort" in installed_pkgs:
```

### Comparing `dagster-1.3.9rc0/dagster/_grpc/impl.py` & `dagster-1.4.0/dagster/_grpc/impl.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/protos/api.proto` & `dagster-1.4.0/dagster/_grpc/protos/api.proto`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/proxy_server.py` & `dagster-1.4.0/dagster/_grpc/proxy_server.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/server.py` & `dagster-1.4.0/dagster/_grpc/server.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 import warnings
 from concurrent.futures import ThreadPoolExecutor
 from contextlib import ExitStack
 from multiprocessing.synchronize import Event as MPEvent
 from subprocess import Popen
 from threading import Event as ThreadingEventType
 from time import sleep
-from typing import Any, Dict, Iterator, List, Mapping, Optional, Sequence, Tuple, cast
+from typing import Any, Dict, Iterable, Iterator, List, Mapping, Optional, Sequence, Tuple, cast
 
 import grpc
 from grpc_health.v1 import health, health_pb2, health_pb2_grpc
 
 import dagster._check as check
 import dagster._seven as seven
 from dagster._core.code_pointer import CodePointer
@@ -251,15 +251,15 @@
             else DEFAULT_DAGSTER_ENTRY_POINT
         )
 
         self._container_image = check.opt_str_param(container_image, "container_image")
         self._container_context = check.opt_dict_param(container_context, "container_context")
 
         # When will this be set in a gRPC server?
-        #  - When running `dagster dev` (or `dagit`) in the gRPC server subprocesses that are spun up
+        #  - When running `dagster dev` (or `dagster-webserver`) in the gRPC server subprocesses that are spun up
         #  - When running code in Dagster Cloud on 1.1 or later
         # When will it not be set?
         #  - When running your own grpc server with `dagster api grpc`
         #  - When using an integration that spins up gRPC servers (for example, the Dagster Helm
         #    chart or the deploy_docker example)
         self._instance_ref = check.opt_inst_param(instance_ref, "instance_ref", InstanceRef)
         self._exit_stack = ExitStack()
@@ -377,48 +377,48 @@
             raise Exception(
                 f'Could not find a repository called "{external_repo_origin.repository_name}"'
             )
         return loaded_repos.definitions_by_name[external_repo_origin.repository_name]
 
     def Ping(self, request, _context) -> api_pb2.PingReply:
         echo = request.echo
-        return api_pb2.PingReply(echo=echo)
+        return api_pb2.PingReply(echo=echo)  # type: ignore  # (grpc generated)
 
     def StreamingPing(self, request, _context) -> Iterator[api_pb2.StreamingPingEvent]:
         sequence_length = request.sequence_length
         echo = request.echo
         for sequence_number in range(sequence_length):
-            yield api_pb2.StreamingPingEvent(sequence_number=sequence_number, echo=echo)
+            yield api_pb2.StreamingPingEvent(sequence_number=sequence_number, echo=echo)  # type: ignore  # (grpc generated)
 
     def Heartbeat(self, request, _context) -> api_pb2.PingReply:
         self.__last_heartbeat_time = time.time()
         echo = request.echo
-        return api_pb2.PingReply(echo=echo)
+        return api_pb2.PingReply(echo=echo)  # type: ignore  # (grpc generated)
 
     def GetServerId(self, _request, _context) -> api_pb2.GetServerIdReply:
-        return api_pb2.GetServerIdReply(server_id=self._server_id)
+        return api_pb2.GetServerIdReply(server_id=self._server_id)  # type: ignore  # (grpc generated)
 
-    def ExecutionPlanSnapshot(self, request, _context):
+    def ExecutionPlanSnapshot(self, request, _context) -> api_pb2.ExecutionPlanSnapshotReply:
         execution_plan_args = deserialize_value(
             request.serialized_execution_plan_snapshot_args,
             ExecutionPlanSnapshotArgs,
         )
 
         execution_plan_snapshot_or_error = get_external_execution_plan_snapshot(
             self._get_repo_for_origin(execution_plan_args.job_origin.external_repository_origin),
             execution_plan_args.job_origin.job_name,
             execution_plan_args,
         )
-        return api_pb2.ExecutionPlanSnapshotReply(
+        return api_pb2.ExecutionPlanSnapshotReply(  # type: ignore  # (grpc generated)
             serialized_execution_plan_snapshot=serialize_value(execution_plan_snapshot_or_error)
         )
 
     def ListRepositories(self, request, _context) -> api_pb2.ListRepositoriesReply:
         if self._serializable_load_error:
-            return api_pb2.ListRepositoriesReply(
+            return api_pb2.ListRepositoriesReply(  # type: ignore  # (grpc generated)
                 serialized_list_repositories_response_or_error=serialize_value(
                     self._serializable_load_error
                 )
             )
         try:
             loaded_repositories = check.not_none(self._loaded_repositories)
             serialized_response = serialize_value(
@@ -435,15 +435,15 @@
                 )
             )
         except Exception:
             serialized_response = serialize_value(
                 serializable_error_info_from_exc_info(sys.exc_info())
             )
 
-        return api_pb2.ListRepositoriesReply(
+        return api_pb2.ListRepositoriesReply(  # type: ignore  # (grpc generated)
             serialized_list_repositories_response_or_error=serialized_response
         )
 
     def ExternalPartitionNames(self, request, _context) -> api_pb2.ExternalPartitionNamesReply:
         try:
             partition_names_args = deserialize_value(
                 request.serialized_partition_names_args,
@@ -458,22 +458,22 @@
         except Exception:
             serialized_response = serialize_value(
                 ExternalPartitionExecutionErrorData(
                     serializable_error_info_from_exc_info(sys.exc_info())
                 )
             )
 
-        return api_pb2.ExternalPartitionNamesReply(
+        return api_pb2.ExternalPartitionNamesReply(  # type: ignore  # (grpc generated)
             serialized_external_partition_names_or_external_partition_execution_error=serialized_response
         )
 
     def ExternalNotebookData(self, request, _context) -> api_pb2.ExternalNotebookDataReply:
         notebook_path = request.notebook_path
         check.str_param(notebook_path, "notebook_path")
-        return api_pb2.ExternalNotebookDataReply(content=get_notebook_data(notebook_path))
+        return api_pb2.ExternalNotebookDataReply(content=get_notebook_data(notebook_path))  # type: ignore  # (grpc generated)
 
     def ExternalPartitionSetExecutionParams(self, request, _context):
         try:
             args = deserialize_value(
                 request.serialized_partition_set_execution_param_args,
                 PartitionSetExecutionParamArgs,
             )
@@ -493,15 +493,15 @@
                 ExternalPartitionExecutionErrorData(
                     serializable_error_info_from_exc_info(sys.exc_info())
                 )
             )
 
         yield from self._split_serialized_data_into_chunk_events(serialized_data)
 
-    def ExternalPartitionConfig(self, request, _context):
+    def ExternalPartitionConfig(self, request, _context) -> api_pb2.ExternalPartitionConfigReply:
         try:
             args = deserialize_value(request.serialized_partition_args, PartitionArgs)
 
             instance_ref = args.instance_ref if args.instance_ref else self._instance_ref
 
             serialized_data = serialize_value(
                 get_partition_config(
@@ -514,15 +514,15 @@
         except Exception:
             serialized_data = serialize_value(
                 ExternalPartitionExecutionErrorData(
                     serializable_error_info_from_exc_info(sys.exc_info())
                 )
             )
 
-        return api_pb2.ExternalPartitionConfigReply(
+        return api_pb2.ExternalPartitionConfigReply(  # type: ignore  # (grpc generated)
             serialized_external_partition_config_or_external_partition_execution_error=serialized_data
         )
 
     def ExternalPartitionTags(self, request, _context) -> api_pb2.ExternalPartitionTagsReply:
         try:
             partition_args = deserialize_value(request.serialized_partition_args, PartitionArgs)
 
@@ -541,15 +541,15 @@
         except Exception:
             serialized_data = serialize_value(
                 ExternalPartitionExecutionErrorData(
                     serializable_error_info_from_exc_info(sys.exc_info())
                 )
             )
 
-        return api_pb2.ExternalPartitionTagsReply(
+        return api_pb2.ExternalPartitionTagsReply(  # type: ignore  # (grpc generated)
             serialized_external_partition_tags_or_external_partition_execution_error=serialized_data
         )
 
     def ExternalPipelineSubsetSnapshot(
         self, request: Any, _context
     ) -> api_pb2.ExternalPipelineSubsetSnapshotReply:
         try:
@@ -570,15 +570,15 @@
         except Exception:
             serialized_external_pipeline_subset_result = serialize_value(
                 ExternalJobSubsetResult(
                     success=False, error=serializable_error_info_from_exc_info(sys.exc_info())
                 )
             )
 
-        return api_pb2.ExternalPipelineSubsetSnapshotReply(
+        return api_pb2.ExternalPipelineSubsetSnapshotReply(  # type: ignore  # (grpc generated)
             serialized_external_pipeline_subset_result=serialized_external_pipeline_subset_result
         )
 
     def _get_serialized_external_repository_data(self, request):
         try:
             repository_origin = deserialize_value(
                 request.serialized_repository_python_origin,
@@ -592,68 +592,72 @@
                 )
             )
         except Exception:
             return serialize_value(
                 ExternalRepositoryErrorData(serializable_error_info_from_exc_info(sys.exc_info()))
             )
 
-    def ExternalRepository(self, request, _context):
+    def ExternalRepository(self, request, _context) -> api_pb2.ExternalRepositoryReply:
         serialized_external_repository_data = self._get_serialized_external_repository_data(request)
-        return api_pb2.ExternalRepositoryReply(
+        return api_pb2.ExternalRepositoryReply(  # type: ignore  # (grpc generated)
             serialized_external_repository_data=serialized_external_repository_data,
         )
 
     def ExternalJob(self, request, _context) -> api_pb2.ExternalJobReply:
         try:
             repository_origin = deserialize_value(
                 request.serialized_repository_origin,
                 ExternalRepositoryOrigin,
             )
 
             job_def = self._get_repo_for_origin(repository_origin).get_job(request.job_name)
             ser_job_data = serialize_value(external_job_data_from_def(job_def))
-            return api_pb2.ExternalJobReply(serialized_job_data=ser_job_data)
+            return api_pb2.ExternalJobReply(serialized_job_data=ser_job_data)  # type: ignore  # (grpc generated)
         except Exception:
-            return api_pb2.ExternalJobReply(
+            return api_pb2.ExternalJobReply(  # type: ignore  # (grpc generated)
                 serialized_error=serialize_value(
                     serializable_error_info_from_exc_info(sys.exc_info())
                 )
             )
 
-    def StreamingExternalRepository(self, request, _context):
+    def StreamingExternalRepository(
+        self, request, _context
+    ) -> Iterable[api_pb2.StreamingExternalRepositoryEvent]:
         serialized_external_repository_data = self._get_serialized_external_repository_data(request)
 
         num_chunks = int(
             math.ceil(float(len(serialized_external_repository_data)) / STREAMING_CHUNK_SIZE)
         )
 
         for i in range(num_chunks):
             start_index = i * STREAMING_CHUNK_SIZE
             end_index = min(
                 (i + 1) * STREAMING_CHUNK_SIZE,
                 len(serialized_external_repository_data),
             )
 
-            yield api_pb2.StreamingExternalRepositoryEvent(
+            yield api_pb2.StreamingExternalRepositoryEvent(  # type: ignore  # (grpc generated)
                 sequence_number=i,
                 serialized_external_repository_chunk=serialized_external_repository_data[
                     start_index:end_index
                 ],
             )
 
-    def _split_serialized_data_into_chunk_events(self, serialized_data):
+    def _split_serialized_data_into_chunk_events(
+        self, serialized_data
+    ) -> Iterable[api_pb2.StreamingChunkEvent]:
         num_chunks = int(math.ceil(float(len(serialized_data)) / STREAMING_CHUNK_SIZE))
         for i in range(num_chunks):
             start_index = i * STREAMING_CHUNK_SIZE
             end_index = min(
                 (i + 1) * STREAMING_CHUNK_SIZE,
                 len(serialized_data),
             )
 
-            yield api_pb2.StreamingChunkEvent(
+            yield api_pb2.StreamingChunkEvent(  # type: ignore  # (grpc generated)
                 sequence_number=i,
                 serialized_chunk=serialized_data[start_index:end_index],
             )
 
     def ExternalScheduleExecution(self, request, _context):
         try:
             args = deserialize_value(
@@ -703,33 +707,33 @@
             )
 
         yield from self._split_serialized_data_into_chunk_events(serialized_sensor_data)
 
     def ShutdownServer(self, request, _context) -> api_pb2.ShutdownServerReply:
         try:
             self._shutdown_once_executions_finish_event.set()
-            return api_pb2.ShutdownServerReply(
+            return api_pb2.ShutdownServerReply(  # type: ignore  # (grpc generated)
                 serialized_shutdown_server_result=serialize_value(
                     ShutdownServerResult(success=True, serializable_error_info=None)
                 )
             )
         except:
             self._logger.exception("Failed to shut down server")
-            return api_pb2.ShutdownServerReply(
+            return api_pb2.ShutdownServerReply(  # type: ignore  # (grpc generated)
                 serialized_shutdown_server_result=serialize_value(
                     ShutdownServerResult(
                         success=False,
                         serializable_error_info=serializable_error_info_from_exc_info(
                             sys.exc_info()
                         ),
                     )
                 )
             )
 
-    def CancelExecution(self, request, _context):
+    def CancelExecution(self, request, _context) -> api_pb2.CancelExecutionReply:
         success = False
         message = None
         serializable_error_info = None
         try:
             cancel_execution_request = deserialize_value(
                 request.serialized_cancel_execution_request,
                 CancelExecutionRequest,
@@ -739,15 +743,15 @@
                     self._termination_events[cancel_execution_request.run_id].set()
                     self._termination_times[cancel_execution_request.run_id] = time.time()
                     success = True
 
         except:
             serializable_error_info = serializable_error_info_from_exc_info(sys.exc_info())
 
-        return api_pb2.CancelExecutionReply(
+        return api_pb2.CancelExecutionReply(  # type: ignore  # (grpc generated)
             serialized_cancel_execution_result=serialize_value(
                 CancelExecutionResult(
                     success=success,
                     message=message,
                     serializable_error_info=serializable_error_info,
                 )
             )
@@ -760,23 +764,23 @@
         )
         with self._execution_lock:
             run_id = can_cancel_execution_request.run_id
             can_cancel = (
                 run_id in self._executions and not self._termination_events[run_id].is_set()
             )
 
-        return api_pb2.CanCancelExecutionReply(
+        return api_pb2.CanCancelExecutionReply(  # type: ignore  # (grpc generated)
             serialized_can_cancel_execution_result=serialize_value(
                 CanCancelExecutionResult(can_cancel=can_cancel)
             )
         )
 
     def StartRun(self, request, _context) -> api_pb2.StartRunReply:
         if self._shutdown_once_executions_finish_event.is_set():
-            return api_pb2.StartRunReply(
+            return api_pb2.StartRunReply(  # type: ignore  # (grpc generated)
                 serialized_start_run_result=serialize_value(
                     StartRunResult(
                         success=False,
                         message="Tried to start a run on a server after telling it to shut down",
                         serializable_error_info=None,
                     )
                 )
@@ -794,15 +798,15 @@
                 execute_external_job_args.job_origin.external_repository_origin.repository_name
             ]
             recon_job = recon_repo.get_reconstructable_job(
                 execute_external_job_args.job_origin.job_name
             )
 
         except:
-            return api_pb2.StartRunReply(
+            return api_pb2.StartRunReply(  # type: ignore  # (grpc generated)
                 serialized_start_run_result=serialize_value(
                     StartRunResult(
                         success=False,
                         message=None,
                         serializable_error_info=serializable_error_info_from_exc_info(
                             sys.exc_info()
                         ),
@@ -872,36 +876,36 @@
 
         # Ensure that if the run failed, we remove it from the executions map before
         # returning so that CanCancel will never return True
         if not success:
             with self._execution_lock:
                 self._clear_run(run_id)
 
-        return api_pb2.StartRunReply(
+        return api_pb2.StartRunReply(  # type: ignore  # (grpc generated)
             serialized_start_run_result=serialize_value(
                 StartRunResult(
                     success=success,
                     message=message,
                     serializable_error_info=serializable_error_info,
                 )
             )
         )
 
-    def GetCurrentImage(self, request, _context):
-        return api_pb2.GetCurrentImageReply(
+    def GetCurrentImage(self, request, _context) -> api_pb2.GetCurrentImageReply:
+        return api_pb2.GetCurrentImageReply(  # type: ignore  # (grpc generated)
             serialized_current_image=serialize_value(
                 GetCurrentImageResult(
                     current_image=self._container_image, serializable_error_info=None
                 )
             )
         )
 
     def GetCurrentRuns(self, request, context) -> api_pb2.GetCurrentRunsReply:
         with self._execution_lock:
-            return api_pb2.GetCurrentRunsReply(
+            return api_pb2.GetCurrentRunsReply(  # type: ignore  # (grpc generated)
                 serialized_current_runs=serialize_value(
                     GetCurrentRunsResult(
                         current_runs=list(self._executions.keys()), serializable_error_info=None
                     )
                 )
             )
 
@@ -1034,15 +1038,15 @@
             return
         except DagsterUserCodeUnreachableError:
             last_error = serializable_error_info_from_exc_info(sys.exc_info())
 
         if timeout > 0 and (time.time() - start_time > timeout):
             raise Exception(
                 f"Timed out waiting for gRPC server to start after {timeout}s with arguments:"
-                f" \"{' '.join(subprocess_args)}\". Most recent connection error: {str(last_error)}"
+                f" \"{' '.join(subprocess_args)}\". Most recent connection error: {last_error}"
             )
 
         if server_process.poll() is not None:
             raise Exception(
                 f"gRPC server exited with return code {server_process.returncode} while starting up"
                 f" with the command: \"{' '.join(subprocess_args)}\""
             )
```

### Comparing `dagster-1.3.9rc0/dagster/_grpc/server_watcher.py` & `dagster-1.4.0/dagster/_grpc/server_watcher.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/types.py` & `dagster-1.4.0/dagster/_grpc/types.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_grpc/utils.py` & `dagster-1.4.0/dagster/_grpc/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_loggers/__init__.py` & `dagster-1.4.0/dagster/_loggers/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_module_alias_map.py` & `dagster-1.4.0/dagster/_module_alias_map.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_scheduler/scheduler.py` & `dagster-1.4.0/dagster/_scheduler/scheduler.py`

 * *Files 6% similar despite different names*

```diff
@@ -45,14 +45,20 @@
 
 if TYPE_CHECKING:
     from pendulum.datetime import DateTime
 
     from dagster._daemon.daemon import DaemonIterator
 
 
+# how often do we update the job row in the database with the last iteration timestamp.  This
+# creates a checkpoint so that if the cron schedule changes, we don't try to backfill schedule ticks
+# from the start of the schedule, just since the last recorded iteration interval.
+LAST_RECORDED_ITERATION_INTERVAL_SECONDS = 3600
+
+
 class _ScheduleLaunchContext:
     def __init__(
         self,
         external_schedule: ExternalSchedule,
         tick: InstigatorTick,
         instance: DagsterInstance,
         logger: logging.Logger,
@@ -273,30 +279,30 @@
             if (
                 code_location_origin.location_name not in workspace_snapshot
                 or not workspace_snapshot[code_location_origin.location_name].code_location
             ):
                 logger.warning(
                     f"Schedule {schedule_name} was started from a location "
                     f"{code_location_name} that can no longer be found in the workspace. You can "
-                    "turn off this schedule in the Dagit UI from the Status tab."
+                    "turn off this schedule in the Dagster UI from the Status tab."
                 )
             elif not check.not_none(  # checked in case above
                 workspace_snapshot[code_location_origin.location_name].code_location
             ).has_repository(repo_name):
                 logger.warning(
                     f"Could not find repository {repo_name} in location {code_location_name} to "
                     + f"run schedule {schedule_name}. If this repository no longer exists, you can "
-                    + "turn off the schedule in the Dagit UI from the Status tab.",
+                    + "turn off the schedule in the Dagster UI from the Status tab.",
                 )
             else:
                 logger.warning(
                     (
                         f"Could not find schedule {schedule_name} in repository {repo_name}. If"
-                        " this schedule no longer exists, you can turn it off in the Dagit UI from"
-                        " the Status tab."
+                        " this schedule no longer exists, you can turn it off in the Dagster UI"
+                        " from the Status tab."
                     ),
                 )
 
     if not schedules:
         logger.debug("Not checking for any runs since no schedules have been started.")
         yield
         return
@@ -434,37 +440,38 @@
 
     with schedule_state_lock:
         instigator_origin_id = external_schedule.get_external_origin_id()
         ticks = instance.get_ticks(instigator_origin_id, external_schedule.selector_id, limit=1)
         latest_tick: Optional[InstigatorTick] = ticks[0] if ticks else None
 
     instigator_data = cast(ScheduleInstigatorData, schedule_state.instigator_data)
-    start_timestamp_utc = instigator_data.start_timestamp if schedule_state else None
+    start_timestamp_utc: float = instigator_data.start_timestamp or 0
 
     if latest_tick:
         if latest_tick.status == TickStatus.STARTED or (
             latest_tick.status == TickStatus.FAILURE
             and latest_tick.failure_count <= max_tick_retries
         ):
             # Scheduler was interrupted while performing this tick, re-do it
-            start_timestamp_utc = (
-                max(start_timestamp_utc, latest_tick.timestamp)
-                if start_timestamp_utc
-                else latest_tick.timestamp
+            start_timestamp_utc = max(
+                start_timestamp_utc,
+                latest_tick.timestamp,
+                instigator_data.last_iteration_timestamp or 0.0,
             )
         else:
-            start_timestamp_utc = (
-                max(start_timestamp_utc, latest_tick.timestamp + 1)
-                if start_timestamp_utc
-                else latest_tick.timestamp + 1
+            start_timestamp_utc = max(
+                start_timestamp_utc,
+                latest_tick.timestamp + 1,
+                instigator_data.last_iteration_timestamp or 0.0,
             )
     else:
-        start_timestamp_utc = instigator_data.start_timestamp
-
-    start_timestamp_utc = check.not_none(start_timestamp_utc)
+        start_timestamp_utc = max(
+            start_timestamp_utc,
+            instigator_data.last_iteration_timestamp or 0.0,
+        )
 
     schedule_name = external_schedule.name
 
     timezone_str = external_schedule.execution_timezone
     if not timezone_str:
         timezone_str = "UTC"
         if log_verbose_checks:
@@ -479,14 +486,22 @@
             break
 
         tick_times.append(next_time)
 
     if not tick_times:
         if log_verbose_checks:
             logger.info(f"No new tick times to evaluate for {schedule_name}")
+
+        _log_iteration_timestamp(
+            instance,
+            schedule_state,
+            schedule_state_lock,
+            instigator_data,
+            end_datetime_utc.timestamp(),
+        )
         return
 
     if not external_schedule.partition_set_name and len(tick_times) > 1:
         logger.warning(f"{schedule_name} has no partition set, so not trying to catch up")
         tick_times = tick_times[-1:]
     elif len(tick_times) > max_catchup_runs:
         logger.warning(f"{schedule_name} has fallen behind, only launching {max_catchup_runs} runs")
@@ -570,14 +585,23 @@
                         TickStatus.FAILURE,
                         error=error_data,
                         failure_count=tick_context.failure_count + 1,
                     )
                     yield error_data
                     return
 
+    # now log the iteration timestamp
+    _log_iteration_timestamp(
+        instance,
+        schedule_state,
+        schedule_state_lock,
+        instigator_data,
+        end_datetime_utc.timestamp(),
+    )
+
 
 def _check_for_debug_crash(
     debug_crash_flags: Optional[SingleInstigatorDebugCrashFlags], key: str
 ) -> None:
     if not debug_crash_flags:
         return
 
@@ -598,15 +622,14 @@
 
 
 def _submit_run_request(
     run_request: RunRequest,
     workspace_process_context: IWorkspaceProcessContext,
     external_schedule: ExternalSchedule,
     schedule_time: datetime.datetime,
-    code_location: CodeLocation,
     logger,
     debug_crash_flags,
 ) -> SubmitRunRequestResult:
     instance = workspace_process_context.instance
     schedule_origin = external_schedule.get_external_origin()
 
     run = _get_existing_run_for_request(instance, external_schedule, schedule_time, run_request)
@@ -629,14 +652,22 @@
         job_subset_selector = JobSubsetSelector(
             location_name=schedule_origin.external_repository_origin.code_location_origin.location_name,
             repository_name=schedule_origin.external_repository_origin.repository_name,
             job_name=external_schedule.job_name,
             op_selection=external_schedule.op_selection,
             asset_selection=run_request.asset_selection,
         )
+
+        # reload the code_location on each submission, request_context derived data can become out date
+        # * non-threaded: if number of serial submissions is too many
+        # * threaded: if thread sits pending in pool too long
+        code_location = _get_code_location_for_schedule(
+            workspace_process_context, external_schedule
+        )
+
         external_job = code_location.get_external_job(job_subset_selector)
 
         run = _create_scheduler_run(
             instance,
             schedule_time,
             code_location,
             external_schedule,
@@ -662,30 +693,37 @@
         run_key=run_request.run_key,
         error_info=error_info,
         existing_run=None,
         submitted_run=run,
     )
 
 
+def _get_code_location_for_schedule(
+    workspace_process_context: IWorkspaceProcessContext,
+    external_schedule: ExternalSchedule,
+):
+    schedule_origin = external_schedule.get_external_origin()
+    return workspace_process_context.create_request_context().get_code_location(
+        schedule_origin.external_repository_origin.code_location_origin.location_name
+    )
+
+
 def _schedule_runs_at_time(
     workspace_process_context: IWorkspaceProcessContext,
     logger: logging.Logger,
     external_schedule: ExternalSchedule,
     schedule_time: datetime.datetime,
     tick_context: _ScheduleLaunchContext,
     submit_threadpool_executor: Optional[ThreadPoolExecutor],
     debug_crash_flags: Optional[SingleInstigatorDebugCrashFlags] = None,
 ) -> "DaemonIterator":
     instance = workspace_process_context.instance
-    schedule_origin = external_schedule.get_external_origin()
     repository_handle = external_schedule.handle.repository_handle
 
-    code_location = workspace_process_context.create_request_context().get_code_location(
-        schedule_origin.external_repository_origin.code_location_origin.location_name
-    )
+    code_location = _get_code_location_for_schedule(workspace_process_context, external_schedule)
 
     schedule_execution_data = code_location.get_external_schedule_execution_data(
         instance=instance,
         repository_handle=repository_handle,
         schedule_name=external_schedule.name,
         scheduled_execution_time=schedule_time,
     )
@@ -726,15 +764,14 @@
         run_requests.append(run_request)
 
     submit_run_request = lambda run_request: _submit_run_request(
         run_request,
         workspace_process_context,
         external_schedule,
         schedule_time,
-        code_location,
         logger,
         debug_crash_flags,
     )
 
     if submit_threadpool_executor:
         gen_run_request_results = submit_threadpool_executor.map(submit_run_request, run_requests)
     else:
@@ -850,7 +887,40 @@
         parent_job_snapshot=external_job.parent_job_snapshot,
         external_job_origin=external_job.get_external_origin(),
         job_code_origin=external_job.get_python_origin(),
         asset_selection=frozenset(run_request.asset_selection)
         if run_request.asset_selection
         else None,
     )
+
+
+def _log_iteration_timestamp(
+    instance: DagsterInstance,
+    schedule_state: InstigatorState,
+    schedule_state_lock: threading.Lock,
+    instigator_data: ScheduleInstigatorData,
+    iteration_timestamp: float,
+):
+    # Utility function that logs iteration timestamps for schedules that are running, to record a
+    # successful iteration, regardless of whether or not a tick was processed or not.  This is so
+    # that when a cron schedule changes, we can modify the evaluation "start time" from the moment
+    # that the schedule was turned on to the last time that the schedule was processed in a valid
+    # state (even in between ticks).
+
+    # Rather than logging every single iteration, we log every hour.  This means that if the cron
+    # schedule changes to run to a time that is less than an hour ago, when the code location is
+    # deployed, a tick might be registered for that time, with a run kicking off.
+    if (
+        not instigator_data.last_iteration_timestamp
+        or instigator_data.last_iteration_timestamp + LAST_RECORDED_ITERATION_INTERVAL_SECONDS
+        < iteration_timestamp
+    ):
+        with schedule_state_lock:
+            instance.update_instigator_state(
+                schedule_state.with_data(
+                    ScheduleInstigatorData(
+                        cron_schedule=instigator_data.cron_schedule,
+                        start_timestamp=instigator_data.start_timestamp,
+                        last_iteration_timestamp=iteration_timestamp,
+                    )
+                )
+            )
```

### Comparing `dagster-1.3.9rc0/dagster/_scheduler/stale.py` & `dagster-1.4.0/dagster/_scheduler/stale.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_serdes/__init__.py` & `dagster-1.4.0/dagster/_serdes/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_serdes/config_class.py` & `dagster-1.4.0/dagster/_serdes/config_class.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_serdes/ipc.py` & `dagster-1.4.0/dagster/_serdes/ipc.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import signal
 import subprocess
 import sys
 from contextlib import contextmanager
 from io import TextIOWrapper
 from subprocess import Popen
 from time import sleep
-from typing import Iterator, NamedTuple, Optional, Sequence, Tuple
+from typing import Any, Iterator, NamedTuple, Optional, Sequence, Tuple
 
 import dagster._check as check
 from dagster._core.errors import DagsterError
 from dagster._serdes.serdes import (
     deserialize_value,
     serialize_value,
     whitelist_for_serdes,
@@ -193,23 +193,23 @@
             message = _process_line(file_pointer)
 
 
 # Windows subprocess termination utilities
 # https://stefan.sofa-rockers.org/2013/08/15/handling-sub-process-hierarchies-python-linux-os-x/
 
 
-def open_ipc_subprocess(parts: Sequence[str], **kwargs: object) -> Popen[bytes]:
+def open_ipc_subprocess(parts: Sequence[str], **kwargs: Any) -> Popen[bytes]:
     """Sets the correct flags to support graceful termination."""
     check.list_param(parts, "parts", str)
 
     creationflags = 0
     if sys.platform == "win32":
         creationflags = subprocess.CREATE_NEW_PROCESS_GROUP
 
-    return subprocess.Popen(
+    return subprocess.Popen(  # type: ignore  # (unclear whether this is actually guaranteed to return Popen[bytes])
         parts,
         creationflags=creationflags,
         **kwargs,
     )
 
 
 def interrupt_ipc_subprocess(proc: Popen[bytes]) -> None:
```

### Comparing `dagster-1.3.9rc0/dagster/_serdes/serdes.py` & `dagster-1.4.0/dagster/_serdes/serdes.py`

 * *Files 0% similar despite different names*

```diff
@@ -596,15 +596,15 @@
 # Serialize / Pack
 ###################################################################################################
 
 
 def serialize_value(
     val: PackableValue,
     whitelist_map: WhitelistMap = _WHITELIST_MAP,
-    **json_kwargs: object,
+    **json_kwargs: Any,
 ) -> str:
     """Serialize an object to a JSON string.
 
     Objects are first converted to a JSON-serializable form with `pack_value`.
     """
     packed_value = pack_value(val, whitelist_map=whitelist_map)
     return seven.json.dumps(packed_value, **json_kwargs)
```

### Comparing `dagster-1.3.9rc0/dagster/_serdes/utils.py` & `dagster-1.4.0/dagster/_serdes/utils.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_seven/__init__.py` & `dagster-1.4.0/dagster/_seven/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_seven/abc.py` & `dagster-1.4.0/dagster/_seven/abc.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_seven/compat/pendulum.py` & `dagster-1.4.0/dagster/_seven/compat/pendulum.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/__init__.py` & `dagster-1.4.0/dagster/_utils/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -637,14 +637,15 @@
 
 T_Callable = TypeVar("T_Callable", bound=Callable)
 
 
 def traced(func: T_Callable) -> T_Callable:
     """A decorator that keeps track of how many times a function is called."""
 
+    @functools.wraps(func)
     def inner(*args, **kwargs):
         counter = traced_counter.get()
         if counter and isinstance(counter, Counter):
             counter.increment(func.__qualname__)
 
         return func(*args, **kwargs)
```

### Comparing `dagster-1.3.9rc0/dagster/_utils/alert.py` & `dagster-1.4.0/dagster/_utils/alert.py`

 * *Files 2% similar despite different names*

```diff
@@ -82,15 +82,15 @@
     email_to: Sequence[str],
     email_body_fn: Callable[["RunFailureSensorContext"], str] = _default_failure_email_body,
     email_subject_fn: Callable[["RunFailureSensorContext"], str] = _default_failure_email_subject,
     smtp_host: str = "smtp.gmail.com",
     smtp_type: str = "SSL",
     smtp_port: Optional[int] = None,
     name: Optional[str] = None,
-    dagit_base_url: Optional[str] = None,
+    webserver_base_url: Optional[str] = None,
     monitored_jobs: Optional[
         Sequence[
             Union[
                 "JobDefinition",
                 "GraphDefinition",
                 "UnresolvedAssetJobDefinition",
                 "RepositorySelector",
@@ -124,28 +124,28 @@
         email_subject_fn (Optional(Callable[[RunFailureSensorContext], str])): Function which
             takes in the ``RunFailureSensorContext`` outputs the email subject you want to send.
             Defaults to "Dagster Run Failed: <job_name>".
         smtp_host (str): The hostname of the SMTP server. Defaults to "smtp.gmail.com".
         smtp_type (str): The protocol; either "SSL" or "STARTTLS". Defaults to SSL.
         smtp_port (Optional[int]): The SMTP port. Defaults to 465 for SSL, 587 for STARTTLS.
         name: (Optional[str]): The name of the sensor. Defaults to "email_on_job_failure".
-        dagit_base_url: (Optional[str]): The base url of your Dagit instance. Specify this to allow
+        webserver_base_url: (Optional[str]): The base url of your dagster-webserver instance. Specify this to allow
             messages to include deeplinks to the failed run.
         monitored_jobs (Optional[List[Union[JobDefinition, GraphDefinition, JobDefinition, RepositorySelector, JobSelector]]]):
             The jobs that will be monitored by this failure sensor. Defaults to None, which means the alert will
             be sent when any job in the repository fails. To monitor jobs in external repositories,
             use RepositorySelector and JobSelector.
         monitor_all_repositories (bool): If set to True, the sensor will monitor all runs in the
             Dagster instance. If set to True, an error will be raised if you also specify
             monitored_jobs or job_selection. Defaults to False.
         job_selection (Optional[List[Union[JobDefinition, GraphDefinition, JobDefinition,  RepositorySelector, JobSelector]]]):
             (deprecated in favor of monitored_jobs) The jobs that will be monitored by this failure
             sensor. Defaults to None, which means the alert will be sent when any job in the repository fails.
         default_status (DefaultSensorStatus): Whether the sensor starts as running or not. The default
-            status can be overridden from Dagit or via the GraphQL API.
+            status can be overridden from the Dagster UI or via the GraphQL API.
 
     Examples:
         .. code-block:: python
 
             email_on_run_failure = make_email_on_run_failure_sensor(
                 email_from="no-reply@example.com",
                 email_password=os.getenv("ALERT_EMAIL_PASSWORD"),
@@ -166,15 +166,15 @@
 
             email_on_run_failure = make_email_on_run_failure_sensor(
                 email_from="no-reply@example.com",
                 email_password=os.getenv("ALERT_EMAIL_PASSWORD"),
                 email_to=["xxx@example.com"],
                 email_body_fn=my_message_fn,
                 email_subject_fn=lambda _: "Dagster Alert",
-                dagit_base_url="http://mycoolsite.com",
+                webserver_base_url="http://mycoolsite.com",
             )
 
 
     """
     from dagster._core.definitions.run_status_sensor_definition import (
         RunFailureSensorContext,
         run_failure_sensor,
@@ -188,18 +188,18 @@
         name=name,
         monitored_jobs=jobs,
         default_status=default_status,
         monitor_all_repositories=monitor_all_repositories,
     )
     def email_on_run_failure(context: RunFailureSensorContext):
         email_body = email_body_fn(context)
-        if dagit_base_url:
+        if webserver_base_url:
             email_body += (
-                f'<p><a href="{dagit_base_url}/runs/{context.dagster_run.run_id}">View in'
-                " Dagit</a></p>"
+                f'<p><a href="{webserver_base_url}/runs/{context.dagster_run.run_id}">View in'
+                " the Dagster UI</a></p>"
             )
 
         message = EMAIL_MESSAGE.format(
             email_to=",".join(email_to),
             email_from=email_from,
             email_subject=email_subject_fn(context),
             email_body=email_body,
```

### Comparing `dagster-1.3.9rc0/dagster/_utils/backcompat.py` & `dagster-1.4.0/dagster/_utils/backcompat.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/backoff.py` & `dagster-1.4.0/dagster/_utils/backoff.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/cached_method.py` & `dagster-1.4.0/dagster/_utils/cached_method.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/caching_instance_queryer.py` & `dagster-1.4.0/dagster/_utils/caching_instance_queryer.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,30 +1,34 @@
 from collections import defaultdict
 from datetime import datetime
 from typing import (
     TYPE_CHECKING,
     AbstractSet,
     Dict,
     Iterable,
+    List,
     Mapping,
     Optional,
     Sequence,
     Union,
     cast,
 )
 
 import pendulum
 
+import dagster._check as check
 from dagster._core.definitions.asset_graph import AssetGraph
+from dagster._core.definitions.asset_graph_subset import AssetGraphSubset
 from dagster._core.definitions.data_version import (
+    DATA_VERSION_TAG,
     DataVersion,
     extract_data_version_from_entry,
 )
 from dagster._core.definitions.events import AssetKey, AssetKeyPartitionKey
-from dagster._core.definitions.partition import DynamicPartitionsDefinition
+from dagster._core.definitions.partition import DynamicPartitionsDefinition, PartitionsSubset
 from dagster._core.definitions.time_window_partitions import TimeWindowPartitionsDefinition
 from dagster._core.events import DagsterEventType
 from dagster._core.instance import DagsterInstance, DynamicPartitionsStore
 from dagster._core.storage.dagster_run import (
     DagsterRun,
     RunRecord,
 )
@@ -41,285 +45,283 @@
     instance which will attempt to limit redundant expensive calls. Intended for use within the
     scope of a single "request" (e.g. GQL request, sensor tick).
 
     Args:
         instance (DagsterInstance): The instance to query.
     """
 
-    def __init__(self, instance: DagsterInstance, evaluation_time: Optional[datetime] = None):
+    def __init__(
+        self,
+        instance: DagsterInstance,
+        asset_graph: AssetGraph,
+        evaluation_time: Optional[datetime] = None,
+    ):
         self._instance = instance
+        self._asset_graph = asset_graph
 
         self._asset_record_cache: Dict[AssetKey, Optional[AssetRecord]] = {}
-        self._latest_materialization_record_cache: Dict[
-            AssetKeyPartitionKey, Optional[EventLogRecord]
-        ] = {}
-
         self._asset_partition_count_cache: Dict[
             Optional[int], Dict[AssetKey, Mapping[str, int]]
         ] = defaultdict(dict)
 
         self._dynamic_partitions_cache: Dict[str, Sequence[str]] = {}
 
         self._evaluation_time = evaluation_time if evaluation_time else pendulum.now("UTC")
 
     @property
     def instance(self) -> DagsterInstance:
         return self._instance
 
+    @property
+    def asset_graph(self) -> AssetGraph:
+        return self._asset_graph
+
+    @property
+    def evaluation_time(self) -> datetime:
+        return self._evaluation_time
+
     ####################
     # QUERY BATCHING
     ####################
 
     def prefetch_asset_partition_counts(
         self, asset_keys: Sequence[AssetKey], after_cursor: Optional[int]
     ):
         """For performance, batches together queries for selected assets."""
-        self._asset_partition_count_cache[None] = dict(
-            self.instance.get_materialization_count_by_partition(
-                asset_keys=asset_keys,
-                after_cursor=None,
-            )
-        )
         if after_cursor is not None:
             self._asset_partition_count_cache[after_cursor] = dict(
                 self.instance.get_materialization_count_by_partition(
                     asset_keys=asset_keys,
                     after_cursor=after_cursor,
                 )
             )
 
-    def prefetch_asset_records(self, asset_keys: Sequence[AssetKey]):
+    def prefetch_asset_records(self, asset_keys: Iterable[AssetKey]):
         """For performance, batches together queries for selected assets."""
-        # get all asset records for the selected assets
-        asset_records = self.instance.get_asset_records(asset_keys)
+        keys_to_fetch = set(asset_keys) - set(self._asset_record_cache.keys())
+        if len(keys_to_fetch) == 0:
+            return
+        # get all asset records for selected assets that aren't already cached
+        asset_records = self.instance.get_asset_records(list(keys_to_fetch))
         for asset_record in asset_records:
             self._asset_record_cache[asset_record.asset_entry.asset_key] = asset_record
+        for key in asset_keys:
+            if key not in self._asset_record_cache:
+                self._asset_record_cache[key] = None
 
-        for asset_key in asset_keys:
-            # if an asset has no materializations, it may not have an asset record
-            asset_record = self._asset_record_cache.get(asset_key)
-            if asset_record is None:
-                self._asset_record_cache[asset_key] = None
+    ####################
+    # ASSET STATUS CACHE
+    ####################
 
-            # use the asset record to determine the latest materialization record
-            latest_materialization_record = (
-                asset_record.asset_entry.last_materialization_record if asset_record else None
-            )
-            self._latest_materialization_record_cache[
-                AssetKeyPartitionKey(asset_key=asset_key)
-            ] = latest_materialization_record
-
-            # if we have a latest materialization record, then we also know what partition this
-            # record was associated with (if any)
-            if latest_materialization_record is not None:
-                self._latest_materialization_record_cache[
-                    AssetKeyPartitionKey(
-                        asset_key=asset_key,
-                        partition_key=latest_materialization_record.partition_key,
-                    )
-                ] = latest_materialization_record
+    @cached_method
+    def get_failed_or_in_progress_subset(self, *, asset_key: AssetKey) -> PartitionsSubset:
+        """Returns a PartitionsSubset representing the set of partitions that are either in progress
+        or whose last materialization attempt failed.
+        """
+        from dagster._core.storage.partition_status_cache import (
+            get_and_update_asset_status_cache_value,
+        )
+
+        partitions_def = check.not_none(self.asset_graph.get_partitions_def(asset_key))
+        asset_record = self.get_asset_record(asset_key)
+        cache_value = get_and_update_asset_status_cache_value(
+            instance=self.instance,
+            asset_key=asset_key,
+            partitions_def=partitions_def,
+            dynamic_partitions_loader=self,
+            asset_record=asset_record,
+        )
+        if cache_value is None:
+            return partitions_def.empty_subset()
+
+        return cache_value.deserialize_failed_partition_subsets(
+            partitions_def
+        ) | cache_value.deserialize_in_progress_partition_subsets(partitions_def)
 
     ####################
-    # MATERIALIZATION / ASSET RECORDS
+    # ASSET RECORDS / STORAGE IDS
     ####################
 
+    def has_cached_asset_record(self, asset_key: AssetKey) -> bool:
+        return asset_key in self._asset_record_cache
+
     def get_asset_record(self, asset_key: AssetKey) -> Optional["AssetRecord"]:
         if asset_key not in self._asset_record_cache:
             self._asset_record_cache[asset_key] = next(
                 iter(self.instance.get_asset_records([asset_key])), None
             )
         return self._asset_record_cache[asset_key]
 
+    def _event_type_for_key(self, asset_key: AssetKey) -> DagsterEventType:
+        if self.asset_graph.is_source(asset_key):
+            return DagsterEventType.ASSET_OBSERVATION
+        else:
+            return DagsterEventType.ASSET_MATERIALIZATION
+
     @cached_method
-    def _get_latest_materialization_record(
+    def _get_latest_materialization_or_observation_record(
         self, *, asset_partition: AssetKeyPartitionKey, before_cursor: Optional[int] = None
     ) -> Optional["EventLogRecord"]:
+        """Returns the latest event log record for the given asset partition of an asset. For
+        observable source assets, this will be an AssetObservation, otherwise it will be an
+        AssetMaterialization.
+        """
         from dagster._core.event_api import EventRecordsFilter
 
+        # in the simple case, just use the asset record
+        if (
+            before_cursor is None
+            and asset_partition.partition_key is None
+            and not self.asset_graph.is_observable(asset_partition.asset_key)
+        ):
+            asset_record = self.get_asset_record(asset_partition.asset_key)
+            if asset_record is None:
+                return None
+            return asset_record.asset_entry.last_materialization_record
+
         records = self.instance.get_event_records(
             EventRecordsFilter(
-                event_type=DagsterEventType.ASSET_MATERIALIZATION,
+                event_type=self._event_type_for_key(asset_partition.asset_key),
                 asset_key=asset_partition.asset_key,
                 asset_partitions=[asset_partition.partition_key]
                 if asset_partition.partition_key
                 else None,
                 before_cursor=before_cursor,
             ),
             ascending=False,
             limit=1,
         )
         return next(iter(records), None)
 
-    def materialization_exists(
+    @cached_method
+    def get_latest_storage_id_for_event_type(
+        self, *, event_type: DagsterEventType
+    ) -> Optional[int]:
+        """Returns the latest storage id across all events of the given event_type.
+
+        Args:
+            event_type (DagsterEventType): The event type to query for.
+        """
+        from dagster._core.event_api import EventRecordsFilter
+
+        latest_record = next(
+            iter(
+                self.instance.get_event_records(
+                    event_records_filter=EventRecordsFilter(event_type=event_type),
+                    limit=1,
+                )
+            ),
+            None,
+        )
+        if latest_record is not None:
+            return latest_record.storage_id
+        return None
+
+    @cached_method
+    def _get_latest_materialization_or_observation_storage_ids_by_asset_partition(
+        self, *, asset_key: AssetKey
+    ) -> Mapping[AssetKeyPartitionKey, Optional[int]]:
+        """Returns a mapping from asset partition to the latest storage id for that asset partition
+        for all asset partitions associated with the given asset key.
+
+        Note that for partitioned assets, an asset partition with a None partition key will be
+        present in the mapping, representing the latest storage id for the asset as a whole.
+        """
+        asset_partition = AssetKeyPartitionKey(asset_key)
+        latest_record = self._get_latest_materialization_or_observation_record(
+            asset_partition=asset_partition
+        )
+        latest_storage_ids = {
+            asset_partition: latest_record.storage_id if latest_record is not None else None
+        }
+        if self.asset_graph.is_partitioned(asset_key):
+            latest_storage_ids.update(
+                {
+                    AssetKeyPartitionKey(asset_key, partition_key): storage_id
+                    for partition_key, storage_id in self.instance.get_latest_storage_id_by_partition(
+                        asset_key, event_type=self._event_type_for_key(asset_key)
+                    ).items()
+                }
+            )
+        return latest_storage_ids
+
+    def get_latest_materialization_or_observation_storage_id(
+        self, asset_partition: AssetKeyPartitionKey
+    ) -> Optional[int]:
+        """Returns the latest storage id for the given asset partition. If the asset has never been
+        materialized, returns None.
+
+        Args:
+            asset_partition (AssetKeyPartitionKey): The asset partition to query.
+        """
+        return self._get_latest_materialization_or_observation_storage_ids_by_asset_partition(
+            asset_key=asset_partition.asset_key
+        ).get(asset_partition)
+
+    def asset_partition_has_materialization_or_observation(
         self,
         asset_partition: AssetKeyPartitionKey,
         after_cursor: Optional[int] = None,
     ) -> bool:
         """Returns True if there is a materialization record for the given asset partition after
-        the specified cursor. Because this function does not need to return the actual record, it
-        is more efficient than get_latest_materialization_record when partitioned assets involved.
+        the specified cursor.
 
         Args:
             asset_partition (AssetKeyPartitionKey): The asset partition to query.
             after_cursor (Optional[int]): Filter parameter such that only records with a storage_id
                 greater than this value will be considered.
         """
-        if asset_partition.partition_key is not None:
-            partition_counts = self.get_materialized_partition_counts(
-                asset_partition.asset_key, after_cursor=after_cursor
-            )
-            return partition_counts.get(asset_partition.partition_key, 0) > 0
-        else:
-            return (
-                self.get_latest_materialization_record(asset_partition, after_cursor=after_cursor)
-                is not None
-            )
-
-    def _materialization_of_a_exists_after_b(
-        self,
-        a: AssetKeyPartitionKey,
-        b: AssetKeyPartitionKey,
-    ) -> bool:
-        """Returns True if there is a materialization record for asset partition a after the latest
-        materialization record for asset partition b.
-
-        Attempts to optimize cases where exactly one of the inputs is partitioned, and we expect
-        this to be called multiple times for the same inputs, only varying the partitioned asset's key.
-
-        Args:
-            a (AssetKeyPartitionKey): The asset partition that we're looking for new
-                materializations of.
-            b (AssetKeyPartitionKey): The anchor asset partition that we're comparing against.
-        """
-        # For performance, we try to only call get_latest_materialization on unpartitioned
-        # assets. To do so, we reverse the order of operations based on the partitioning of
-        # the inputs.
-        if a.partition_key is None:
-            latest_a = self.get_latest_materialization_record(a)
-            if latest_a is None:
-                return False
-            # if a materialization of b exists after the latest materialization of a, then
-            # a materialization of a cannot exist after the latest materialization of b
-            return not self.materialization_exists(b, after_cursor=latest_a.storage_id)
-        else:
-            latest_b = self.get_latest_materialization_record(b)
-            if latest_b is None:
-                return False
-            return self.materialization_exists(a, after_cursor=latest_b.storage_id)
+        return (self.get_latest_materialization_or_observation_storage_id(asset_partition) or 0) > (
+            after_cursor or 0
+        )
 
-    def get_latest_materialization_record(
+    def get_latest_materialization_or_observation_record(
         self,
-        asset: Union[AssetKey, AssetKeyPartitionKey],
+        asset_partition: AssetKeyPartitionKey,
         after_cursor: Optional[int] = None,
         before_cursor: Optional[int] = None,
     ) -> Optional["EventLogRecord"]:
-        """Returns the latest materialization record for the given asset partition between the
-        specified cursors.
+        """Returns the latest record for the given asset partition given the specified cursors.
 
         Args:
-            asset (Union[AssetKey, AssetKeyPartitionKey]): The asset (partition) to query.
+            asset_partition (AssetKeyPartitionKey): The asset partition to query.
             after_cursor (Optional[int]): Filter parameter such that only records with a storage_id
                 greater than this value will be considered.
             before_cursor (Optional[int]): Filter parameter such that only records with a storage_id
                 less than this value will be considered.
         """
-        if isinstance(asset, AssetKey):
-            asset_partition = AssetKeyPartitionKey(asset_key=asset)
-        else:
-            asset_partition = asset
+        check.param_invariant(
+            not (after_cursor and before_cursor),
+            "before_cursor",
+            "Cannot set both before_cursor and after_cursor",
+        )
 
-        # the count of this (asset key, partition key) pair is 0
-        if (
-            asset_partition.partition_key is not None
-            and after_cursor in self._asset_partition_count_cache
-            and asset_partition.asset_key in self._asset_partition_count_cache[after_cursor]
-            and self._asset_partition_count_cache[after_cursor][asset_partition.asset_key].get(
-                asset_partition.partition_key, 0
-            )
-            == 0
+        # first, do a quick check to eliminate the case where we know there is no record
+        if not self.asset_partition_has_materialization_or_observation(
+            asset_partition, after_cursor
         ):
             return None
-
-        # ensure we know the latest overall materialization record for this asset partition
-        if asset_partition not in self._latest_materialization_record_cache:
-            self._latest_materialization_record_cache[
-                asset_partition
-            ] = self._get_latest_materialization_record(
-                asset_partition=asset_partition,
-            )
-
-        # the latest overall record
-        latest_record = self._latest_materialization_record_cache[asset_partition]
-
-        # there are no records for this asset partition after after_cursor
-        if latest_record is None or latest_record.storage_id <= (after_cursor or 0):
-            return None
-
-        if before_cursor is None:
-            return latest_record
-        else:
-            if latest_record is None:
-                # no records exist
-                return None
-            elif latest_record.storage_id < before_cursor:
-                # the latest record is before the cursor, so we can return it
-                return latest_record
-            else:
-                # fall back to an explicit query
-                return self._get_latest_materialization_record(
-                    asset_partition=asset_partition, before_cursor=before_cursor
-                )
-
-    @cached_method
-    def get_materialization_records(
-        self,
-        *,
-        asset_key: AssetKey,
-        after_cursor: Optional[int] = None,
-        tags: Optional[Mapping[str, str]] = None,
-    ) -> Iterable["EventLogRecord"]:
-        from dagster._core.event_api import EventRecordsFilter
-
-        return self.instance.get_event_records(
-            EventRecordsFilter(
-                event_type=DagsterEventType.ASSET_MATERIALIZATION,
-                asset_key=asset_key,
-                after_cursor=after_cursor,
-                tags=tags,
+        # then, if the before_cursor is after our latest record's storage id, we can just return
+        # the latest record
+        elif (before_cursor or 0) > (
+            self.get_latest_materialization_or_observation_storage_id(asset_partition) or 0
+        ):
+            return self._get_latest_materialization_or_observation_record(
+                asset_partition=asset_partition
             )
+        # otherwise, do the explicit query
+        return self._get_latest_materialization_or_observation_record(
+            asset_partition=asset_partition, before_cursor=before_cursor
         )
 
     ####################
     # OBSERVATIONS
     ####################
 
     @cached_method
-    def get_observation_record(
-        self,
-        *,
-        asset_key: AssetKey,
-        before_cursor: Optional[int],
-    ) -> Optional["EventLogRecord"]:
-        from dagster._core.event_api import EventRecordsFilter
-
-        return next(
-            iter(
-                self.instance.get_event_records(
-                    EventRecordsFilter(
-                        event_type=DagsterEventType.ASSET_OBSERVATION,
-                        asset_key=asset_key,
-                        before_cursor=before_cursor,
-                    ),
-                    ascending=False,
-                )
-            ),
-            None,
-        )
-
-    @cached_method
     def next_version_record(
         self,
         *,
         asset_key: AssetKey,
         after_cursor: Optional[int],
         data_version: Optional[DataVersion],
     ) -> Optional["EventLogRecord"]:
@@ -336,80 +338,14 @@
             record_version = extract_data_version_from_entry(record.event_log_entry)
             if record_version is not None and record_version != data_version:
                 return record
 
         # no records found with a new data version
         return None
 
-    def new_version_storage_id(
-        self,
-        observable_source_asset_key: AssetKey,
-        after_cursor: Optional[int] = None,
-    ) -> Optional[int]:
-        """Returns the storage id of the latest asset observation of the given observable source
-        asset key after the specified cursor, or None if no such observation exists.
-
-        Args:
-            observable_source_asset_key (AssetKeyPartitionKey): The observable source asset to query.
-            after_cursor (Optional[int]): Filter parameter such that only records with a storage_id
-                greater than this value will be considered.
-        """
-        previous_version_record = (
-            self.get_observation_record(
-                asset_key=observable_source_asset_key,
-                # we're looking for if a new version exists after `after_cursor`, so we need to know
-                # what the version was before `after_cursor`
-                before_cursor=after_cursor,
-            )
-            # if the after_cursor is None, then no previous version can exist
-            if after_cursor is not None
-            else None
-        )
-        previous_version = (
-            extract_data_version_from_entry(previous_version_record.event_log_entry)
-            if previous_version_record is not None
-            else None
-        )
-
-        next_version_record = self.next_version_record(
-            asset_key=observable_source_asset_key,
-            after_cursor=after_cursor,
-            data_version=previous_version,
-        )
-        return next_version_record.storage_id if next_version_record else None
-
-    def _new_version_of_source_exists_after_asset_partition(
-        self,
-        observable_source_asset_key: AssetKey,
-        asset_partition: AssetKeyPartitionKey,
-    ) -> bool:
-        """Returns True if there is a new version of a given observable source asset after the latest
-        materialization record for a given asset partition.
-
-        Attempts to optimize cases where the downstream asset is partitioned, and we expect
-        this to be called multiple times for the same inputs, only varying the partitioned asset's key.
-
-        Args:
-            observable_source_asset_key (AssetKeyPartitionKey): The observable source asset.
-            asset_partition (AssetKeyPartitionKey): The downstream asset partition.
-        """
-        # TODO: this assumes that each observation record will have a distinct data version, which
-        # is not always the case. In the "next_version_record" implementation, we have the benefit
-        # of having an explicit start cursor, but here the corresponding implementation would require
-        # loading all historical observation records. We'll have to do our own pagination here.
-        latest_observation_record = self.get_observation_record(
-            asset_key=observable_source_asset_key,
-            before_cursor=None,
-        )
-        if latest_observation_record is None:
-            return False
-        return not self.materialization_exists(
-            asset_partition, after_cursor=latest_observation_record.storage_id
-        )
-
     ####################
     # RUNS
     ####################
 
     @cached_method
     def _get_run_record_by_id(self, *, run_id: str) -> Optional[RunRecord]:
         return self.instance.get_run_record_by_id(run_id)
@@ -444,16 +380,15 @@
 
         Args:
             run_id (str): The run id
         """
         run = self._get_run_by_id(run_id=run_id)
         if run is None:
             return set()
-
-        if run.asset_selection:
+        elif run.asset_selection:
             return run.asset_selection
         else:
             # must resort to querying the event log
             return self._get_planned_materializations_for_run_from_events(run_id=run_id)
 
     def is_asset_planned_for_run(
         self, run_id: str, asset: Union[AssetKey, AssetKeyPartitionKey]
@@ -482,15 +417,48 @@
         materializations = self.instance.get_records_for_run(
             run_id=run_id,
             of_type=DagsterEventType.ASSET_MATERIALIZATION,
         ).records
         return set(cast(AssetKey, record.asset_key) for record in materializations)
 
     ####################
-    # PARTITIONS
+    # BACKFILLS
+    ####################
+
+    @cached_method
+    def get_active_backfill_target_asset_graph_subset(self) -> AssetGraphSubset:
+        """Returns an AssetGraphSubset representing the set of assets that are currently targeted by
+        an active asset backfill.
+        """
+        from dagster._core.execution.asset_backfill import AssetBackfillData
+        from dagster._core.execution.backfill import BulkActionStatus
+
+        asset_backfills = [
+            backfill
+            for backfill in self.instance.get_backfills(status=BulkActionStatus.REQUESTED)
+            if backfill.is_asset_backfill
+        ]
+
+        result = AssetGraphSubset(self.asset_graph)
+        for asset_backfill in asset_backfills:
+            if asset_backfill.serialized_asset_backfill_data is None:
+                check.failed("Asset backfill missing serialized_asset_backfill_data")
+
+            asset_backfill_data = AssetBackfillData.from_serialized(
+                asset_backfill.serialized_asset_backfill_data,
+                self.asset_graph,
+                asset_backfill.backfill_timestamp,
+            )
+
+            result |= asset_backfill_data.target_subset
+
+        return result
+
+    ####################
+    # PARTITION COUNTS
     ####################
 
     def get_materialized_partition_counts(
         self, asset_key: AssetKey, after_cursor: Optional[int] = None
     ) -> Mapping[str, int]:
         """Returns a mapping from partition key to the number of times that partition has been
         materialized for a given asset.
@@ -527,14 +495,18 @@
             partition_key
             for partition_key, count in self.get_materialized_partition_counts(
                 asset_key, after_cursor
             ).items()
             if count > 0
         ]
 
+    ####################
+    # DYNAMIC PARTITIONS
+    ####################
+
     def get_dynamic_partitions(self, partitions_def_name: str) -> Sequence[str]:
         """Returns a list of partitions for a partitions definition."""
         if partitions_def_name not in self._dynamic_partitions_cache:
             self._dynamic_partitions_cache[
                 partitions_def_name
             ] = self.instance.get_dynamic_partitions(partitions_def_name)
         return self._dynamic_partitions_cache[partitions_def_name]
@@ -542,82 +514,171 @@
     def has_dynamic_partition(self, partitions_def_name: str, partition_key: str) -> bool:
         return partition_key in self.get_dynamic_partitions(partitions_def_name)
 
     ####################
     # RECONCILIATION
     ####################
 
-    def get_latest_storage_id(self, event_type: DagsterEventType) -> Optional[int]:
-        """Returns the latest storage id for an event of the given event type. If no such event
-        exists, returns None.
+    def _asset_partitions_data_versions(
+        self,
+        asset_key: AssetKey,
+        asset_partitions: Optional[AbstractSet[AssetKeyPartitionKey]],
+        after_cursor: Optional[int] = None,
+        before_cursor: Optional[int] = None,
+    ) -> Mapping[AssetKeyPartitionKey, Optional[DataVersion]]:
+        if not self.asset_graph.is_partitioned(asset_key):
+            asset_partition = AssetKeyPartitionKey(asset_key)
+            latest_record = self.get_latest_materialization_or_observation_record(
+                asset_partition, after_cursor=after_cursor, before_cursor=before_cursor
+            )
+            return (
+                {asset_partition: extract_data_version_from_entry(latest_record.event_log_entry)}
+                if latest_record is not None
+                else {}
+            )
+        else:
+            query_result = self.instance._event_storage.get_latest_tags_by_partition(  # noqa
+                asset_key,
+                event_type=self._event_type_for_key(asset_key),
+                tag_keys=[DATA_VERSION_TAG],
+                after_cursor=after_cursor,
+                before_cursor=before_cursor,
+                asset_partitions=[
+                    asset_partition.partition_key
+                    for asset_partition in asset_partitions
+                    if asset_partition.partition_key is not None
+                ]
+                if asset_partitions is not None
+                else None,
+            )
+            return {
+                AssetKeyPartitionKey(asset_key, partition_key): DataVersion(tags[DATA_VERSION_TAG])
+                if tags.get(DATA_VERSION_TAG)
+                else None
+                for partition_key, tags in query_result.items()
+            }
+
+    def get_asset_partitions_updated_after_cursor(
+        self,
+        asset_key: AssetKey,
+        asset_partitions: Optional[Sequence[AssetKeyPartitionKey]],
+        after_cursor: Optional[int],
+    ) -> AbstractSet[AssetKeyPartitionKey]:
+        """Returns the set of asset partitions that have been updated after the given cursor.
 
         Args:
-            event_type (DagsterEventType): The event type to search for.
+            asset_key (AssetKey): The asset key to check.
+            asset_partitions (Optional[Sequence[AssetKeyPartitionKey]]): If supplied, will filter
+                the set of checked partitions to the given partitions.
+            after_cursor (Optional[int]): The cursor after which to look for updates.
         """
-        from dagster._core.event_api import EventRecordsFilter
+        if not self.asset_partition_has_materialization_or_observation(
+            AssetKeyPartitionKey(asset_key), after_cursor=after_cursor
+        ):
+            return set()
+        # quick check that just compares latest storage ids
+        updated_after_cursor = {
+            asset_partition
+            for asset_partition, latest_storage_id in self._get_latest_materialization_or_observation_storage_ids_by_asset_partition(
+                asset_key=asset_key
+            ).items()
+            if (latest_storage_id or 0) > (after_cursor or 0)
+        }
 
-        records = list(
-            self.instance.get_event_records(
-                event_records_filter=EventRecordsFilter(event_type=event_type), limit=1
-            )
-        )
-        if records:
-            return records[0].storage_id
-        else:
-            return None
+        if asset_partitions:
+            updated_after_cursor = updated_after_cursor.intersection(set(asset_partitions))
+
+        if not updated_after_cursor:
+            return set()
+        # for regular assets, don't bother checking versions
+        # in the future, we may remove this guard
+        if after_cursor is None or not self.asset_graph.is_observable(asset_key):
+            return updated_after_cursor
+
+        # more expensive check to explicitly handle data versions
+        latest_versions = self._asset_partitions_data_versions(
+            asset_key, updated_after_cursor, after_cursor=after_cursor
+        )
+        previous_versions = self._asset_partitions_data_versions(
+            asset_key, updated_after_cursor, before_cursor=after_cursor + 1
+        )
+        return {
+            asset_partition
+            for asset_partition, version in latest_versions.items()
+            if previous_versions.get(asset_partition) != version
+        }
 
     @cached_method
-    def is_reconciled(
-        self, *, asset_partition: AssetKeyPartitionKey, asset_graph: AssetGraph
-    ) -> bool:
-        """Returns a boolean representing if the given `asset_partition` is currently reconciled.
-        An asset (partition) is considered unreconciled if any of:
-        - It has never been materialized
-        - One of its parents has been updated more recently than it has
-        - One of its parents is unreconciled.
+    def get_root_unreconciled_ancestors(
+        self, *, asset_partition: AssetKeyPartitionKey
+    ) -> AbstractSet[AssetKey]:
+        """Return the set of root unreconciled ancestors of the given asset partition, i.e. the set
+        of ancestors of this asset partition which are unreconciled for a reason other than that
+        one of their ancestors is unreconciled.
         """
         # always treat source assets as reconciled
-        if asset_graph.is_source(asset_partition.asset_key):
-            return True
-
-        if not self.materialization_exists(asset_partition):
-            return False
+        if self.asset_graph.is_source(asset_partition.asset_key):
+            return set()
+        elif not self.asset_partition_has_materialization_or_observation(asset_partition):
+            return {asset_partition.asset_key}
 
         time_or_dynamic_partitioned = isinstance(
-            asset_graph.get_partitions_def(asset_partition.asset_key),
+            self.asset_graph.get_partitions_def(asset_partition.asset_key),
             (TimeWindowPartitionsDefinition, DynamicPartitionsDefinition),
         )
-        for parent in asset_graph.get_parents_partitions(
-            self,
-            self._evaluation_time,
-            asset_partition.asset_key,
-            asset_partition.partition_key,
-        ):
+
+        parent_asset_partitions_by_key: Dict[AssetKey, List[AssetKeyPartitionKey]] = defaultdict(
+            list
+        )
+
+        for parent in self.asset_graph.get_parents_partitions(
+            dynamic_partitions_store=self,
+            current_time=self._evaluation_time,
+            asset_key=asset_partition.asset_key,
+            partition_key=asset_partition.partition_key,
+        ).parent_partitions:
+            parent_asset_partitions_by_key[parent.asset_key].append(parent)
+
+        root_unreconciled_ancestors = set()
+        for parent_key, parent_asset_partitions in parent_asset_partitions_by_key.items():
+            # ignore non-observable source parents
+            if self.asset_graph.is_source(parent_key) and not self.asset_graph.is_observable(
+                parent_key
+            ):
+                continue
+
             # when mapping from time or dynamic downstream to unpartitioned upstream, only check
             # for existence of upstream materialization, do not worry about timestamps
-            if time_or_dynamic_partitioned and parent.partition_key is None:
-                return (
-                    # no materializations exist for source assets
-                    asset_graph.is_source(parent.asset_key)
-                    or self.materialization_exists(parent)
-                )
-
-            if asset_graph.is_source(parent.asset_key):
-                if asset_graph.is_observable(
-                    parent.asset_key
-                ) and self._new_version_of_source_exists_after_asset_partition(
-                    observable_source_asset_key=parent.asset_key,
-                    asset_partition=asset_partition,
+            if time_or_dynamic_partitioned and not self.asset_graph.is_partitioned(parent_key):
+                if not all(
+                    self.asset_partition_has_materialization_or_observation(parent)
+                    for parent in parent_asset_partitions
                 ):
-                    return False
-                else:
-                    continue
-            elif self._materialization_of_a_exists_after_b(a=parent, b=asset_partition):
-                return False
-            elif not self.is_reconciled(asset_partition=parent, asset_graph=asset_graph):
-                return False
+                    root_unreconciled_ancestors.add(parent_key)
+                continue
 
-        return True
+            updated_parent_asset_partitions = self.get_asset_partitions_updated_after_cursor(
+                asset_key=parent_key,
+                asset_partitions=parent_asset_partitions,
+                after_cursor=self.get_latest_materialization_or_observation_storage_id(
+                    asset_partition
+                ),
+            )
+            if updated_parent_asset_partitions:
+                # this asset has updated parents, so it must be materialized before it is reconciled
+                root_unreconciled_ancestors.add(asset_partition.asset_key)
+            # recurse over parents
+            for parent in set(parent_asset_partitions) - updated_parent_asset_partitions:
+                root_unreconciled_ancestors.update(
+                    self.get_root_unreconciled_ancestors(asset_partition=parent)
+                )
 
-    @property
-    def evaluation_time(self) -> datetime:
-        return self._evaluation_time
+        return root_unreconciled_ancestors
+
+    def is_reconciled(self, asset_partition: AssetKeyPartitionKey) -> bool:
+        """Returns a boolean representing if the given `asset_partition` is currently reconciled.
+        An asset (partition) is considered unreconciled if any of:
+        - It has never been materialized
+        - One of its parents has been updated more recently than it has
+        - One of its parents is unreconciled.
+        """
+        return len(self.get_root_unreconciled_ancestors(asset_partition=asset_partition)) == 0
```

### Comparing `dagster-1.3.9rc0/dagster/_utils/concurrency.py` & `dagster-1.4.0/dagster/_utils/concurrency.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/dagster_type.py` & `dagster-1.4.0/dagster/_utils/dagster_type.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/env.py` & `dagster-1.4.0/dagster/_utils/env.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/error.py` & `dagster-1.4.0/dagster/_utils/error.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/external.py` & `dagster-1.4.0/dagster/_utils/external.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/forked_pdb.py` & `dagster-1.4.0/dagster/_utils/forked_pdb.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,15 +16,15 @@
         def complex_solid(_):
             # some complicated stuff
 
             ForkedPdb().set_trace()
 
             # some other complicated stuff
 
-    You can initiate pipeline execution via dagit and use the pdb debugger to examine/step through
+    You can initiate pipeline execution via the webserver and use the pdb debugger to examine/step through
     execution at the breakpoint.
     """
 
     def interaction(self, frame, traceback):
         _stdin = sys.stdin
         try:
             sys.stdin = open("/dev/stdin", encoding="utf8")
```

### Comparing `dagster-1.3.9rc0/dagster/_utils/hosted_user_process.py` & `dagster-1.4.0/dagster/_utils/hosted_user_process.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/indenting_printer.py` & `dagster-1.4.0/dagster/_utils/indenting_printer.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/interrupts.py` & `dagster-1.4.0/dagster/_utils/interrupts.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/log.py` & `dagster-1.4.0/dagster/_utils/log.py`

 * *Files 1% similar despite different names*

```diff
@@ -241,18 +241,24 @@
             },
         },
         "loggers": {
             "dagster": {
                 "handlers": [handler],
                 "level": "INFO",
             },
+            # Only one of dagster or dagster-webserver will be used at a time. We configure them
+            # both here to avoid a dependency on the dagster-webserver package.
             "dagit": {
                 "handlers": [handler],
                 "level": "INFO",
             },
+            "dagster-webserver": {
+                "handlers": [handler],
+                "level": "INFO",
+            },
         },
     }
 
     logging.config.dictConfig(LOGGING_CONFIG)
 
 
 def create_console_logger(name, level):
```

### Comparing `dagster-1.3.9rc0/dagster/_utils/merger.py` & `dagster-1.4.0/dagster/_utils/merger.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/net.py` & `dagster-1.4.0/dagster/_utils/net.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/schedules.py` & `dagster-1.4.0/dagster/_utils/schedules.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,15 +28,17 @@
     )
     return dt == cron.get_prev()
 
 
 def is_valid_cron_string(cron_string: str) -> bool:
     if not CroniterShim.is_valid(cron_string):
         return False
-    expanded, _ = CroniterShim.expand(cron_string)
+    # Croniter < 1.4 returns 2 items
+    # Croniter >= 1.4 returns 3 items
+    expanded, *_ = CroniterShim.expand(cron_string)
     # dagster only recognizes cron strings that resolve to 5 parts (e.g. not seconds resolution)
     return len(expanded) == 5
 
 
 def is_valid_cron_schedule(cron_schedule: Union[str, Sequence[str]]) -> bool:
     return (
         is_valid_cron_string(cron_schedule)
@@ -54,15 +56,17 @@
 ) -> Iterator[datetime.datetime]:
     """Generator of datetimes >= start_timestamp for the given cron string."""
     timezone_str = execution_timezone if execution_timezone else "UTC"
 
     utc_datetime = pytz.utc.localize(datetime.datetime.utcfromtimestamp(start_timestamp))
     start_datetime = utc_datetime.astimezone(pytz.timezone(timezone_str))
 
-    cron_parts, nth_weekday_of_month = CroniterShim.expand(cron_string)
+    # Croniter < 1.4 returns 2 items
+    # Croniter >= 1.4 returns 3 items
+    cron_parts, nth_weekday_of_month, *_ = CroniterShim.expand(cron_string)
 
     is_numeric = [len(part) == 1 and part[0] != "*" for part in cron_parts]
     is_wildcard = [len(part) == 1 and part[0] == "*" for part in cron_parts]
 
     delta_fn = None
     should_hour_change = False
     expected_hour = None
@@ -172,15 +176,17 @@
 
     date_iter = CroniterShim(cron_string, end_datetime)
 
     # Go forward one iteration so that the next iteration is the first time that is < end_datetime
     # and matches the cron schedule
     next_date = date_iter.get_next(datetime.datetime)
 
-    cron_parts, _ = CroniterShim.expand(cron_string)
+    # Croniter < 1.4 returns 2 items
+    # Croniter >= 1.4 returns 3 items
+    cron_parts, *_ = CroniterShim.expand(cron_string)
 
     is_numeric = [len(part) == 1 and part[0] != "*" for part in cron_parts]
     is_wildcard = [len(part) == 1 and part[0] == "*" for part in cron_parts]
 
     # Special-case common intervals (hourly/daily/weekly/monthly) since croniter iteration can be
     # much slower than adding a fixed interval
     if all(is_numeric[0:3]) and all(is_wildcard[3:]):  # monthly
```

### Comparing `dagster-1.3.9rc0/dagster/_utils/tags.py` & `dagster-1.4.0/dagster/_utils/tags.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/temp_file.py` & `dagster-1.4.0/dagster/_utils/temp_file.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/test/__init__.py` & `dagster-1.4.0/dagster/_utils/test/__init__.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/test/mysql_instance.py` & `dagster-1.4.0/dagster/_utils/test/mysql_instance.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/test/postgres_instance.py` & `dagster-1.4.0/dagster/_utils/test/postgres_instance.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/test/schedule_storage.py` & `dagster-1.4.0/dagster/_utils/test/schedule_storage.py`

 * *Files 1% similar despite different names*

```diff
@@ -794,7 +794,38 @@
         )
         assert (
             partitions_def.deserialize_subset(
                 res[0].evaluation.partition_subsets_by_condition[0][1].serialized_subset
             )
             == subset
         )
+
+    def test_purge_asset_evaluations(self, storage):
+        if not self.can_purge():
+            pytest.skip("Storage cannot purge")
+
+        storage.add_auto_materialize_asset_evaluations(
+            evaluation_id=11,
+            asset_evaluations=[
+                AutoMaterializeAssetEvaluation(
+                    asset_key=AssetKey("asset_one"),
+                    partition_subsets_by_condition=[],
+                    num_requested=0,
+                    num_skipped=0,
+                    num_discarded=0,
+                ),
+            ],
+        )
+
+        storage.purge_asset_evaluations(before=pendulum.now().subtract(hours=10).timestamp())
+
+        res = storage.get_auto_materialize_asset_evaluations(
+            asset_key=AssetKey("asset_one"), limit=100
+        )
+        assert len(res) == 1
+
+        storage.purge_asset_evaluations(before=pendulum.now().add(minutes=10).timestamp())
+
+        res = storage.get_auto_materialize_asset_evaluations(
+            asset_key=AssetKey("asset_one"), limit=100
+        )
+        assert len(res) == 0
```

### Comparing `dagster-1.3.9rc0/dagster/_utils/timing.py` & `dagster-1.4.0/dagster/_utils/timing.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/typing_api.py` & `dagster-1.4.0/dagster/_utils/typing_api.py`

 * *Files identical despite different names*

### Comparing `dagster-1.3.9rc0/dagster/_utils/yaml_utils.py` & `dagster-1.4.0/dagster/_utils/yaml_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -142,11 +142,15 @@
         return yaml.load(ff, Loader=loader)
 
 
 def load_run_config_yaml(yaml_str: str) -> Mapping[str, object]:
     return yaml.load(yaml_str, Loader=DagsterRunConfigYamlLoader)
 
 
-def dump_run_config_yaml(run_config: Mapping[str, Any]) -> str:
+def dump_run_config_yaml(run_config: Mapping[str, Any], sort_keys: bool = True) -> str:
     return yaml.dump(
-        run_config, Dumper=DagsterRunConfigYamlDumper, default_flow_style=False, allow_unicode=True
+        run_config,
+        Dumper=DagsterRunConfigYamlDumper,
+        default_flow_style=False,
+        allow_unicode=True,
+        sort_keys=sort_keys,
     )
```

### Comparing `dagster-1.3.9rc0/dagster.egg-info/PKG-INFO` & `dagster-1.4.0/dagster.egg-info/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: dagster
-Version: 1.3.9rc0
+Version: 1.4.0
 Summary: The data orchestration platform built for productivity.
 Author: Elementl
 Author-email: hello@elementl.com
 License: Apache-2.0
 Project-URL: Homepage, https://dagster.io
 Project-URL: GitHub, https://github.com/dagster-io/dagster
 Project-URL: Changelog, https://github.com/dagster-io/dagster/releases
@@ -15,15 +15,14 @@
 Project-URL: Blog, https://dagster.io/blog
 Project-URL: Newsletter, https://dagster.io/newsletter-signup
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Environment :: Console
 Classifier: Environment :: Web Environment
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: System Administrators
-Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Topic :: System :: Monitoring
 Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
@@ -62,15 +61,15 @@
     <img src="https://img.shields.io/badge/slack-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=slack" />
   </a>
   <a target="_blank" href="https://linkedin.com/showcase/dagster" style="background:none">
     <img src="https://img.shields.io/badge/linkedin-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=linkedin" />
   </a>
 </div>
 
-__Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.__
+**Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.**
 
 It is designed for **developing and maintaining data assets**, such as tables, data sets, machine learning models, and reports.
 
 With Dagster, you declareas Python functionsthe data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.
 
 Here is an example of a graph of three assets defined in Python:
 
@@ -93,27 +92,28 @@
 
 @asset
 def continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -> DataFrame:
     result = country_populations.groupby("continent").sum()
     result["pop_change_factor"] = continent_change_model.coef_
     return result
 ```
+
 The graph loaded into Dagster's web UI:
 
 <p align="center">
   <img width="400px" alt="An example asset graph as rendered in the Dagster UI" src="https://user-images.githubusercontent.com/654855/183537484-48dde394-91f2-4de0-9b17-a70b3e9a3823.png">
 </p>
 
 Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.
 
 ## Quick Start:
 
 If you're new to Dagster, we recommend reading about its [core concepts](https://docs.dagster.io/concepts) or learning with the hands-on [tutorial](https://docs.dagster.io/tutorial).
 
-Dagster is available on PyPI and officially supports Python 3.7+.
+Dagster is available on PyPI and officially supports Python 3.8+.
 
 ```bash
 pip install dagster dagit
 ```
 
 This installs two modules:
 
@@ -131,20 +131,23 @@
 ## Key Features:
 
   <p align="center">
     <img width="100%" alt="image" src=".github/key-features-cards.svg">
   </p>
 
 ### Dagster as a productivity platform
+
 Identify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.
 
 ### Dagster as a robust orchestration engine
+
 Put your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.
 
 ### Dagster as a unified control plane
+
 Maintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.
 
 <hr />
 
 ## Master the Modern Data Stack with integrations
 
 Dagster provides a growing library of integrations for todays most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.
```

### Comparing `dagster-1.3.9rc0/dagster.egg-info/SOURCES.txt` & `dagster-1.4.0/dagster.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -105,14 +105,15 @@
 dagster/_core/definitions/data_version.py
 dagster/_core/definitions/definition_config_schema.py
 dagster/_core/definitions/definitions_class.py
 dagster/_core/definitions/dependency.py
 dagster/_core/definitions/events.py
 dagster/_core/definitions/executor_definition.py
 dagster/_core/definitions/external_asset_graph.py
+dagster/_core/definitions/freshness_based_auto_materialize.py
 dagster/_core/definitions/freshness_policy.py
 dagster/_core/definitions/freshness_policy_sensor_definition.py
 dagster/_core/definitions/graph_definition.py
 dagster/_core/definitions/hook_definition.py
 dagster/_core/definitions/hook_invocation.py
 dagster/_core/definitions/inference.py
 dagster/_core/definitions/input.py
@@ -304,15 +305,14 @@
 dagster/_core/storage/local_compute_log_manager.py
 dagster/_core/storage/mem_io_manager.py
 dagster/_core/storage/memoizable_io_manager.py
 dagster/_core/storage/noop_compute_log_manager.py
 dagster/_core/storage/output_manager.py
 dagster/_core/storage/partition_status_cache.py
 dagster/_core/storage/root.py
-dagster/_core/storage/root_input_manager.py
 dagster/_core/storage/sql.py
 dagster/_core/storage/sqlalchemy_compat.py
 dagster/_core/storage/sqlite.py
 dagster/_core/storage/sqlite_storage.py
 dagster/_core/storage/tags.py
 dagster/_core/storage/temp_file_manager.py
 dagster/_core/storage/upath_io_manager.py
```

### Comparing `dagster-1.3.9rc0/dagster.egg-info/requires.txt` & `dagster-1.4.0/dagster.egg-info/requires.txt`

 * *Files 23% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 click>=5.0
 coloredlogs<=14.0,>=6.1
 Jinja2
 PyYAML>=5.1
-alembic!=1.6.3,!=1.7.0,<1.11.0,>=1.2.1
+alembic!=1.11.0,!=1.6.3,!=1.7.0,>=1.2.1
 croniter>=0.3.34
+grpcio>=1.44.0
+grpcio-health-checking>=1.44.0
 packaging>=20.9
 pendulum
 protobuf>=3.20.0
 python-dateutil
 python-dotenv
 pytz
 requests
@@ -17,42 +19,34 @@
 tqdm
 typing_extensions>=4.4.0
 sqlalchemy>=1.0
 toposort>=1.0
 watchdog>=0.8.3
 docstring-parser
 universal_pathlib
-pydantic!=1.10.7
+pydantic!=1.10.7,<2.0.0
 
 [:platform_system == "Windows"]
 psutil>=1.0
 pywin32!=226
 
-[:python_version < "3.11"]
-grpcio<1.48.0,>=1.44.0
-grpcio-health-checking<1.48.0,>=1.44.0
-
 [:python_version < "3.7"]
 contextvars
 
-[:python_version >= "3.11"]
-grpcio>=1.44.0
-grpcio-health-checking>=1.44.0
-
 [black]
 black[jupyter]==22.12.0
 
 [docker]
 docker
 
 [mypy]
 mypy==0.991
 
 [pyright]
-pyright==1.1.298
+pyright==1.1.316
 pandas-stubs
 types-backports
 types-certifi
 types-chardet
 types-croniter
 types-cryptography
 types-mock
@@ -67,33 +61,29 @@
 types-six
 types-sqlalchemy==1.4.53.34
 types-tabulate
 types-tzlocal
 types-toml
 
 [ruff]
-ruff==0.0.255
+ruff==0.0.277
 
 [test]
 docker
+grpcio-tools>=1.44.0
 mock==3.0.5
 objgraph
 pytest-cov==2.10.1
 pytest-dependency==0.5.1
 pytest-mock==3.3.1
 pytest-rerunfailures==10.0
 pytest-runner==5.2
 pytest-xdist==2.1.0
 pytest==7.0.1
 responses
-snapshottest==0.6.0
+syrupy<4
 tox==3.25.0
 yamllint
 
-[test:python_version < "3.11"]
-grpcio-tools<1.48.0,>=1.44.0
-
-[test:python_version >= "3.11"]
-grpcio-tools>=1.44.0
-
 [test:python_version >= "3.8"]
 buildkite-test-collector
+morefs[asynclocal]
```

### Comparing `dagster-1.3.9rc0/setup.py` & `dagster-1.4.0/setup.py`

 * *Files 8% similar despite different names*

```diff
@@ -23,17 +23,14 @@
         exec(fp.read(), version)
 
     return version["__version__"]
 
 
 # grpcio 1.44.0 is the min version compatible with both protobuf 3 and 4
 GRPC_VERSION_FLOOR = "1.44.0"
-# Also pinned <1.48.0 until the resolution of https://github.com/grpc/grpc/issues/31885
-# (except on python 3.11, where newer versions are required just to install the grpcio package)
-GRPC_VERSION_CAP = "1.48.0"
 
 
 setup(
     name="dagster",
     version=get_version(),
     author="Elementl",
     author_email="hello@elementl.com",
@@ -54,15 +51,14 @@
     },
     classifiers=[
         "Development Status :: 5 - Production/Stable",
         "Environment :: Console",
         "Environment :: Web Environment",
         "Intended Audience :: Developers",
         "Intended Audience :: System Administrators",
-        "Programming Language :: Python :: 3.7",
         "Programming Language :: Python :: 3.8",
         "Programming Language :: Python :: 3.9",
         "Programming Language :: Python :: 3.10",
         "Programming Language :: Python :: 3.11",
         "License :: OSI Approved :: Apache Software License",
         "Topic :: System :: Monitoring",
         "Topic :: Software Development :: Libraries :: Application Frameworks",
@@ -75,20 +71,18 @@
         "click>=5.0",
         "coloredlogs>=6.1, <=14.0",
         "contextvars; python_version < '3.7'",
         "Jinja2",
         "PyYAML>=5.1",
         # core (not explicitly expressed atm)
         # pin around issues in specific versions of alembic that broke our migrations
-        "alembic>=1.2.1,!=1.6.3,!=1.7.0,<1.11.0",
+        "alembic>=1.2.1,!=1.6.3,!=1.7.0,!=1.11.0",
         "croniter>=0.3.34",
-        f"grpcio>={GRPC_VERSION_FLOOR},<{GRPC_VERSION_CAP}; python_version<'3.11'",
-        f"grpcio>={GRPC_VERSION_FLOOR}; python_version>='3.11'",
-        f"grpcio-health-checking>={GRPC_VERSION_FLOOR},<{GRPC_VERSION_CAP}; python_version<'3.11'",
-        f"grpcio-health-checking>={GRPC_VERSION_FLOOR}; python_version>='3.11'",
+        f"grpcio>={GRPC_VERSION_FLOOR}",
+        f"grpcio-health-checking>={GRPC_VERSION_FLOOR}",
         "packaging>=20.9",
         "pendulum",
         "protobuf>=3.20.0",  # min protobuf version to be compatible with both protobuf 3 and 4
         "python-dateutil",
         "python-dotenv",
         "pytz",
         "requests",
@@ -102,45 +96,45 @@
         "watchdog>=0.8.3",
         'psutil >= 1.0; platform_system=="Windows"',
         # https://github.com/mhammond/pywin32/issues/1439
         'pywin32 != 226; platform_system=="Windows"',
         "docstring-parser",
         "universal_pathlib",
         # https://github.com/pydantic/pydantic/issues/5821
-        "pydantic != 1.10.7",
+        "pydantic != 1.10.7,<2.0.0",
     ],
     extras_require={
         "docker": ["docker"],
         "test": [
             "buildkite-test-collector ; python_version>='3.8'",
             "docker",
-            f"grpcio-tools>={GRPC_VERSION_FLOOR},<{GRPC_VERSION_CAP}; python_version<'3.11'",
-            f"grpcio-tools>={GRPC_VERSION_FLOOR}; python_version>='3.11'",
+            f"grpcio-tools>={GRPC_VERSION_FLOOR}",
             "mock==3.0.5",
             "objgraph",
             "pytest-cov==2.10.1",
             "pytest-dependency==0.5.1",
             "pytest-mock==3.3.1",
             "pytest-rerunfailures==10.0",
             "pytest-runner==5.2",
             "pytest-xdist==2.1.0",
             "pytest==7.0.1",  # last version supporting python 3.6
             "responses",
-            "snapshottest==0.6.0",
+            "syrupy<4",  # 3.7 compatible,
             "tox==3.25.0",
             "yamllint",
+            "morefs[asynclocal]; python_version>='3.8'",
         ],
         "black": [
             "black[jupyter]==22.12.0",
         ],
         "mypy": [
             "mypy==0.991",
         ],
         "pyright": [
-            "pyright==1.1.298",
+            "pyright==1.1.316",
             ### Stub packages
             "pandas-stubs",  # version will be resolved against pandas
             "types-backports",  # version will be resolved against backports
             "types-certifi",  # version will be resolved against certifi
             "types-chardet",  # chardet is a 2+-order dependency of some Dagster libs
             "types-croniter",  # version will be resolved against croniter
             "types-cryptography",  # version will be resolved against cryptography
@@ -156,15 +150,15 @@
             "types-six",  # needed but not specified by grpcio
             "types-sqlalchemy==1.4.53.34",  # later versions introduce odd errors
             "types-tabulate",  # version will be resolved against tabulate
             "types-tzlocal",  # version will be resolved against tzlocal
             "types-toml",  # version will be resolved against toml
         ],
         "ruff": [
-            "ruff==0.0.255",
+            "ruff==0.0.277",
         ],
     },
     entry_points={
         "console_scripts": [
             "dagster = dagster.cli:main",
             "dagster-daemon = dagster.daemon.cli:main",
         ]
```

