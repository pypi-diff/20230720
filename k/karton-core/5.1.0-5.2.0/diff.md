# Comparing `tmp/karton_core-5.1.0-py3-none-any.whl.zip` & `tmp/karton_core-5.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,28 +1,28 @@
-Zip file size: 44888 bytes, number of entries: 26
--rw-r--r--  2.0 unx      539 b- defN 23-May-16 12:19 karton_core-5.1.0-nspkg.pth
--rw-r--r--  2.0 unx      337 b- defN 23-May-16 12:19 karton/core/__init__.py
--rw-r--r--  2.0 unx       22 b- defN 23-May-16 12:19 karton/core/__version__.py
--rw-r--r--  2.0 unx    29037 b- defN 23-May-16 12:19 karton/core/backend.py
--rw-r--r--  2.0 unx     7676 b- defN 23-May-16 12:19 karton/core/base.py
--rw-r--r--  2.0 unx     8118 b- defN 23-May-16 12:19 karton/core/config.py
--rw-r--r--  2.0 unx      149 b- defN 23-May-16 12:19 karton/core/exceptions.py
--rw-r--r--  2.0 unx     4868 b- defN 23-May-16 12:19 karton/core/inspect.py
--rw-r--r--  2.0 unx    14769 b- defN 23-May-16 12:19 karton/core/karton.py
--rw-r--r--  2.0 unx     2040 b- defN 23-May-16 12:19 karton/core/logger.py
--rw-r--r--  2.0 unx     8310 b- defN 23-May-16 12:19 karton/core/main.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 12:19 karton/core/py.typed
--rw-r--r--  2.0 unx    20321 b- defN 23-May-16 12:19 karton/core/resource.py
--rw-r--r--  2.0 unx    15193 b- defN 23-May-16 12:19 karton/core/task.py
--rw-r--r--  2.0 unx     9102 b- defN 23-May-16 12:19 karton/core/test.py
--rw-r--r--  2.0 unx     3763 b- defN 23-May-16 12:19 karton/core/utils.py
--rw-r--r--  2.0 unx       63 b- defN 23-May-16 12:19 karton/system/__init__.py
--rw-r--r--  2.0 unx       56 b- defN 23-May-16 12:19 karton/system/__main__.py
--rw-r--r--  2.0 unx    13933 b- defN 23-May-16 12:19 karton/system/system.py
--rw-r--r--  2.0 unx     1519 b- defN 23-May-16 12:19 karton_core-5.1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     6830 b- defN 23-May-16 12:19 karton_core-5.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-16 12:19 karton_core-5.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       99 b- defN 23-May-16 12:19 karton_core-5.1.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 23-May-16 12:19 karton_core-5.1.0.dist-info/namespace_packages.txt
--rw-r--r--  2.0 unx        7 b- defN 23-May-16 12:19 karton_core-5.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2104 b- defN 23-May-16 12:19 karton_core-5.1.0.dist-info/RECORD
-26 files, 148954 bytes uncompressed, 41504 bytes compressed:  72.1%
+Zip file size: 46471 bytes, number of entries: 26
+-rw-r--r--  2.0 unx      539 b- defN 23-Jul-20 15:58 karton_core-5.2.0-nspkg.pth
+-rw-r--r--  2.0 unx      337 b- defN 23-Jul-20 15:58 karton/core/__init__.py
+-rw-r--r--  2.0 unx       22 b- defN 23-Jul-20 15:58 karton/core/__version__.py
+-rw-r--r--  2.0 unx    32954 b- defN 23-Jul-20 15:58 karton/core/backend.py
+-rw-r--r--  2.0 unx     8083 b- defN 23-Jul-20 15:58 karton/core/base.py
+-rw-r--r--  2.0 unx     8118 b- defN 23-Jul-20 15:58 karton/core/config.py
+-rw-r--r--  2.0 unx      149 b- defN 23-Jul-20 15:58 karton/core/exceptions.py
+-rw-r--r--  2.0 unx     4868 b- defN 23-Jul-20 15:58 karton/core/inspect.py
+-rw-r--r--  2.0 unx    14863 b- defN 23-Jul-20 15:58 karton/core/karton.py
+-rw-r--r--  2.0 unx     2040 b- defN 23-Jul-20 15:58 karton/core/logger.py
+-rw-r--r--  2.0 unx     8310 b- defN 23-Jul-20 15:58 karton/core/main.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-20 15:58 karton/core/py.typed
+-rw-r--r--  2.0 unx    20321 b- defN 23-Jul-20 15:58 karton/core/resource.py
+-rw-r--r--  2.0 unx    19103 b- defN 23-Jul-20 15:58 karton/core/task.py
+-rw-r--r--  2.0 unx     9102 b- defN 23-Jul-20 15:58 karton/core/test.py
+-rw-r--r--  2.0 unx     4090 b- defN 23-Jul-20 15:58 karton/core/utils.py
+-rw-r--r--  2.0 unx       63 b- defN 23-Jul-20 15:58 karton/system/__init__.py
+-rw-r--r--  2.0 unx       56 b- defN 23-Jul-20 15:58 karton/system/__main__.py
+-rw-r--r--  2.0 unx    13791 b- defN 23-Jul-20 15:58 karton/system/system.py
+-rw-r--r--  2.0 unx     1519 b- defN 23-Jul-20 15:58 karton_core-5.2.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6852 b- defN 23-Jul-20 15:58 karton_core-5.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-20 15:58 karton_core-5.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       99 b- defN 23-Jul-20 15:58 karton_core-5.2.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-Jul-20 15:58 karton_core-5.2.0.dist-info/namespace_packages.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-Jul-20 15:58 karton_core-5.2.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2104 b- defN 23-Jul-20 15:58 karton_core-5.2.0.dist-info/RECORD
+26 files, 157489 bytes uncompressed, 43087 bytes compressed:  72.6%
```

## zipnote {}

```diff
@@ -1,8 +1,8 @@
-Filename: karton_core-5.1.0-nspkg.pth
+Filename: karton_core-5.2.0-nspkg.pth
 Comment: 
 
 Filename: karton/core/__init__.py
 Comment: 
 
 Filename: karton/core/__version__.py
 Comment: 
@@ -51,29 +51,29 @@
 
 Filename: karton/system/__main__.py
 Comment: 
 
 Filename: karton/system/system.py
 Comment: 
 
-Filename: karton_core-5.1.0.dist-info/LICENSE
+Filename: karton_core-5.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: karton_core-5.1.0.dist-info/METADATA
+Filename: karton_core-5.2.0.dist-info/METADATA
 Comment: 
 
-Filename: karton_core-5.1.0.dist-info/WHEEL
+Filename: karton_core-5.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: karton_core-5.1.0.dist-info/entry_points.txt
+Filename: karton_core-5.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: karton_core-5.1.0.dist-info/namespace_packages.txt
+Filename: karton_core-5.2.0.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: karton_core-5.1.0.dist-info/top_level.txt
+Filename: karton_core-5.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: karton_core-5.1.0.dist-info/RECORD
+Filename: karton_core-5.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## karton/core/__version__.py

```diff
@@ -1 +1 @@
-__version__ = "5.1.0"
+__version__ = "5.2.0"
```

## karton/core/backend.py

```diff
@@ -11,15 +11,15 @@
 import boto3
 from redis import AuthenticationError, StrictRedis
 from redis.client import Pipeline
 from urllib3.response import HTTPResponse
 
 from .exceptions import InvalidIdentityError
 from .task import Task, TaskPriority, TaskState
-from .utils import chunks
+from .utils import chunks, chunks_iter
 
 KARTON_TASKS_QUEUE = "karton.tasks"
 KARTON_OPERATIONS_QUEUE = "karton.operations"
 KARTON_LOG_CHANNEL = "karton.log"
 KARTON_BINDS_HSET = "karton.binds"
 KARTON_TASK_NAMESPACE = "karton.task"
 KARTON_OUTPUTS_NAMESPACE = "karton.outputs"
@@ -373,47 +373,119 @@
         :return: Task object
         """
         task_data = self.redis.get(f"{KARTON_TASK_NAMESPACE}:{task_uid}")
         if not task_data:
             return None
         return Task.unserialize(task_data, backend=self)
 
-    def get_tasks(self, task_uid_list: List[str], chunk_size: int = 1000) -> List[Task]:
+    def get_tasks(
+        self,
+        task_uid_list: List[str],
+        chunk_size: int = 1000,
+        parse_resources: bool = True,
+    ) -> List[Task]:
         """
         Get multiple tasks for given identifier list
 
         :param task_uid_list: List of task identifiers
         :param chunk_size: Size of chunks passed to the Redis MGET command
+        :param parse_resources: If set to False, resources are not parsed.
+            It speeds up deserialization. Read :py:meth:`Task.unserialize`
+            documentation to learn more.
         :return: List of task objects
         """
         keys = chunks(
             [f"{KARTON_TASK_NAMESPACE}:{task_uid}" for task_uid in task_uid_list],
             chunk_size,
         )
         return [
             Task.unserialize(task_data, backend=self)
+            if parse_resources
+            else Task.unserialize(task_data, parse_resources=False)
             for chunk in keys
             for task_data in self.redis.mget(chunk)
             if task_data is not None
         ]
 
-    def get_all_tasks(self, chunk_size: int = 1000) -> List[Task]:
+    def _iter_tasks(
+        self,
+        task_keys: Iterator[str],
+        chunk_size: int = 1000,
+        parse_resources: bool = True,
+    ) -> Iterator[Task]:
+        for chunk in chunks_iter(task_keys, chunk_size):
+            yield from (
+                Task.unserialize(task_data, backend=self)
+                if parse_resources
+                else Task.unserialize(task_data, parse_resources=False)
+                for task_data in self.redis.mget(chunk)
+                if task_data is not None
+            )
+
+    def iter_tasks(
+        self,
+        task_uid_list: Iterable[str],
+        chunk_size: int = 1000,
+        parse_resources: bool = True,
+    ) -> Iterator[Task]:
+        """
+        Get multiple tasks for given identifier list as an iterator
+        :param task_uid_list: List of task fully-qualified identifiers
+        :param chunk_size: Size of chunks passed to the Redis MGET command
+        :param parse_resources: If set to False, resources are not parsed.
+            It speeds up deserialization. Read :py:meth:`Task.unserialize` documentation
+            to learn more.
+        :return: Iterator with task objects
+        """
+        return self._iter_tasks(
+            map(
+                lambda task_uid: f"{KARTON_TASK_NAMESPACE}:{task_uid}",
+                task_uid_list,
+            ),
+            chunk_size=chunk_size,
+            parse_resources=parse_resources,
+        )
+
+    def iter_all_tasks(
+        self, chunk_size: int = 1000, parse_resources: bool = True
+    ) -> Iterator[Task]:
+        """
+        Iterates all tasks registered in Redis
+        :param chunk_size: Size of chunks passed to the Redis SCAN and MGET command
+        :param parse_resources: If set to False, resources are not parsed.
+            It speeds up deserialization. Read :py:meth:`Task.unserialize` documentation
+            to learn more.
+        :return: Iterator with Task objects
+        """
+        task_keys = self.redis.scan_iter(
+            match=f"{KARTON_TASK_NAMESPACE}:*", count=chunk_size
+        )
+        return self._iter_tasks(
+            task_keys, chunk_size=chunk_size, parse_resources=parse_resources
+        )
+
+    def get_all_tasks(
+        self, chunk_size: int = 1000, parse_resources: bool = True
+    ) -> List[Task]:
         """
         Get all tasks registered in Redis
 
+        .. warning::
+            This method loads all tasks into memory.
+            It's recommended to use :py:meth:`iter_all_tasks` instead.
+
         :param chunk_size: Size of chunks passed to the Redis MGET command
+        :param parse_resources: If set to False, resources are not parsed.
+            It speeds up deserialization. Read :py:meth:`Task.unserialize` documentation
+            to learn more.
         :return: List with Task objects
         """
-        tasks = self.redis.keys(f"{KARTON_TASK_NAMESPACE}:*")
-        return [
-            Task.unserialize(task_data)
-            for chunk in chunks(tasks, chunk_size)
-            for task_data in self.redis.mget(chunk)
-            if task_data is not None
-        ]
+        return list(
+            self.iter_all_tasks(chunk_size=chunk_size, parse_resources=parse_resources)
+        )
 
     def register_task(self, task: Task, pipe: Optional[Pipeline] = None) -> None:
         """
         Register or update task in Redis.
 
         :param task: Task object
         :param pipe: Optional pipeline object if operation is a part of pipeline
@@ -447,22 +519,32 @@
         task.last_update = time.time()
         self.register_task(task, pipe=pipe)
 
     def delete_task(self, task: Task) -> None:
         """
         Remove task from Redis
 
+        .. warning::
+            Used internally by karton.system.
+            If you want to cancel task: mark it as finished and let it be deleted
+            by karton.system.
+
         :param task: Task object
         """
         self.redis.delete(f"{KARTON_TASK_NAMESPACE}:{task.uid}")
 
     def delete_tasks(self, tasks: Iterable[Task], chunk_size: int = 1000) -> None:
         """
         Remove multiple tasks from Redis
 
+        .. warning::
+            Used internally by karton.system.
+            If you want to cancel task: mark it as finished and let it be deleted
+            by karton.system.
+
         :param tasks: List of Task objects
         :param chunk_size: Size of chunks passed to the Redis DELETE command
         """
         keys = [f"{KARTON_TASK_NAMESPACE}:{task.uid}" for task in tasks]
         for chunk in chunks(keys, chunk_size):
             self.redis.delete(*chunk)
 
@@ -481,14 +563,22 @@
         Return all task UIDs in a queue
 
         :param queue: Queue name
         :return: List with task identifiers contained in queue
         """
         return self.redis.lrange(queue, 0, -1)
 
+    def delete_consumer_queues(self, identity: str) -> None:
+        """
+        Deletes consumer queues for given identity
+
+        :param identity: Consumer identity
+        """
+        self.redis.delete(*self.get_queue_names(identity))
+
     def remove_task_queue(self, queue: str) -> List[Task]:
         """
         Remove task queue with all contained tasks
 
         :param queue: Queue name
         :return: List with Task objects contained in queue
         """
@@ -531,14 +621,28 @@
 
         :param queues: Redis queue name or list of names
         :param timeout: Waiting for item timeout (default: 0 = wait forever)
         :return: Tuple of [queue_name, item] objects or None if timeout has been reached
         """
         return self.redis.blpop(queues, timeout=timeout)
 
+    def increment_multiple_metrics(
+        self, metric: KartonMetrics, increments: Dict[str, int]
+    ) -> None:
+        """
+        Increments metrics for multiple identities by given value via single pipeline
+        :param metric: Operation metric type
+        :param increments: Dictionary of Karton service identities and value
+            to add to the metric
+        """
+        p = self.redis.pipeline()
+        for identity, increment in increments.items():
+            p.hincrby(metric.value, identity, increment)
+        p.execute()
+
     def consume_queues_batch(self, queue: str, max_count: int) -> List[str]:
         """
         Get a batch of items from the queue
 
         :param queue: Redis queue name
         :param max_count: Maximum batch count
         """
```

## karton/core/base.py

```diff
@@ -1,10 +1,11 @@
 import abc
 import argparse
 import logging
+import os
 import textwrap
 from contextlib import contextmanager
 from typing import Optional, Union, cast
 
 from .__version__ import __version__
 from .backend import KartonBackend, KartonServiceInfo
 from .config import Config
@@ -40,14 +41,19 @@
         if identity is not None:
             self.identity = identity
 
         # If passed via configuration: override
         if self.config.has_option("karton", "identity"):
             self.identity = self.config.get("karton", "identity")
 
+        self.debug = self.config.getboolean("karton", "debug", False)
+
+        if self.debug:
+            self.identity += "-" + os.urandom(4).hex() + "-dev"
+
         self.service_info = None
         if self.identity is not None and self.with_service_info:
             self.service_info = KartonServiceInfo(
                 identity=self.identity,
                 karton_version=__version__,
                 service_version=self.version,
             )
@@ -97,15 +103,17 @@
 
         logger.setLevel(log_level)
         stream_handler = logging.StreamHandler()
         stream_handler.setFormatter(
             logging.Formatter("[%(asctime)s][%(levelname)s] %(message)s")
         )
         logger.addHandler(stream_handler)
-        logger.addHandler(self._log_handler)
+
+        if not self.debug:
+            logger.addHandler(self._log_handler)
 
     @property
     def log_handler(self) -> KartonLogHandler:
         """
         Return KartonLogHandler bound to this Karton service.
 
         Can be used to setup logging on your own by adding this handler
@@ -150,27 +158,33 @@
             "--version", action="version", version=cast(str, cls.version)
         )
         parser.add_argument("--config-file", help="Alternative configuration path")
         parser.add_argument(
             "--identity", help="Alternative identity for Karton service"
         )
         parser.add_argument("--log-level", help="Logging level of Karton logger")
+        parser.add_argument(
+            "--debug", help="Enable debugging mode", action="store_true", default=None
+        )
         return parser
 
     @classmethod
     def config_from_args(cls, config: Config, args: argparse.Namespace) -> None:
         """
         Updates configuration with settings from arguments
 
         This method should be overridden and call super methods
         if you want to add more arguments.
         """
         config.load_from_dict(
             {
-                "karton": {"identity": args.identity},
+                "karton": {
+                    "identity": args.identity,
+                    "debug": args.debug,
+                },
                 "logging": {"level": args.log_level},
             }
         )
 
     @classmethod
     def karton_from_args(cls, args: Optional[argparse.Namespace] = None):
         """
```

## karton/core/karton.py

```diff
@@ -67,14 +67,15 @@
         """
         self.log.debug("Dispatched task %s", task.uid)
 
         # Complete information about task
         if self.current_task is not None:
             task.set_task_parent(self.current_task)
             task.merge_persistent_payload(self.current_task)
+            task.merge_persistent_headers(self.current_task)
             task.priority = self.current_task.priority
 
         task.last_update = time.time()
         task.headers.update({"origin": self.identity})
 
         # Ensure all local resources have good buckets
         for resource in task.iterate_resources():
@@ -116,16 +117,17 @@
         backend: Optional[KartonBackend] = None,
     ) -> None:
         super().__init__(config=config, identity=identity, backend=backend)
 
         if self.filters is None:
             raise ValueError("Cannot bind consumer on Empty binds")
 
-        self.persistent = self.config.getboolean(
-            "karton", "persistent", self.persistent
+        self.persistent = (
+            self.config.getboolean("karton", "persistent", self.persistent)
+            and not self.debug
         )
         self.task_timeout = self.config.getint("karton", "task_timeout")
         self.current_task: Optional[Task] = None
         self._pre_hooks: List[Tuple[Optional[str], Callable[[Task], None]]] = []
         self._post_hooks: List[
             Tuple[Optional[str], Callable[[Task, Optional[Exception]], None]]
         ] = []
```

## karton/core/task.py

```diff
@@ -18,14 +18,16 @@
 
 from .resource import RemoteResource, ResourceBase
 from .utils import recursive_iter, recursive_iter_with_keys, recursive_map
 
 if TYPE_CHECKING:
     from .backend import KartonBackend  # noqa
 
+import orjson
+
 
 class TaskState(enum.Enum):
     DECLARED = "Declared"  # Task declared in TASKS_QUEUE
     SPAWNED = "Spawned"  # Task spawned into subsystem queue
     STARTED = "Started"  # Task is running in subsystem
     FINISHED = "Finished"  # Task finished (ready to forget)
     CRASHED = "Crashed"  # Task crashed
@@ -42,46 +44,70 @@
     Task representation with headers and resources.
 
     :param headers: Routing information for other systems, this is what allows for \
                     evaluation of given system usefulness for given task. \
                     Systems filter by these.
     :param payload: Any instance of :py:class:`dict` - contains resources \
                     and additional informations
+    :param headers_persistent: Persistent headers for whole task subtree, \
+                               propagated from initial task.
     :param payload_persistent: Persistent payload set for whole task subtree, \
                                propagated from initial task
     :param priority: Priority of whole task subtree, \
                      propagated from initial task like `payload_persistent`
     :param parent_uid: Id of a routed task that has created this task by a karton with \
                        :py:meth:`.send_task`
     :param root_uid: Id of an unrouted task that is the root of this \
         task's analysis tree
     :param orig_uid: Id of an unrouted (or crashed routed) task that was forked to \
                      create this task
     :param uid: This tasks unique identifier
     :param error: Traceback of a exception that happened while performing this task
     """
 
+    __slots__ = (
+        "uid",
+        "root_uid",
+        "orig_uid",
+        "parent_uid",
+        "error",
+        "headers",
+        "status",
+        "last_update",
+        "priority",
+        "payload",
+        "payload_persistent",
+        "_headers_persistent_keys",
+    )
+
     def __init__(
         self,
         headers: Dict[str, Any],
         payload: Optional[Dict[str, Any]] = None,
+        headers_persistent: Optional[Dict[str, Any]] = None,
         payload_persistent: Optional[Dict[str, Any]] = None,
         priority: Optional[TaskPriority] = None,
         parent_uid: Optional[str] = None,
         root_uid: Optional[str] = None,
         orig_uid: Optional[str] = None,
         uid: Optional[str] = None,
         error: Optional[List[str]] = None,
+        _status: Optional[TaskState] = None,
+        _last_update: Optional[float] = None,
     ) -> None:
         payload = payload or {}
         payload_persistent = payload_persistent or {}
+        headers_persistent = headers_persistent or {}
+
         if not isinstance(payload, dict):
             raise ValueError("Payload should be an instance of a dict")
         if not isinstance(payload_persistent, dict):
             raise ValueError("Persistent payload should be an instance of a dict")
+        if not isinstance(headers_persistent, dict):
+            raise ValueError("Persistent headers should be an instance of a dict")
 
         if uid is None:
             self.uid = str(uuid.uuid4())
         else:
             self.uid = uid
 
         if root_uid is None:
@@ -89,35 +115,45 @@
         else:
             self.root_uid = root_uid
 
         self.orig_uid = orig_uid
         self.parent_uid = parent_uid
 
         self.error = error
-        self.headers = headers
-        self.status = TaskState.DECLARED
+        self.headers = {**headers, **headers_persistent}
+        self._headers_persistent_keys = set(headers_persistent.keys())
+        self.status = _status or TaskState.DECLARED
 
-        self.last_update: float = time.time()
+        self.last_update: float = _last_update or time.time()
         self.priority = priority or TaskPriority.NORMAL
 
         self.payload = dict(payload)
         self.payload_persistent = dict(payload_persistent)
 
+    @property
+    def headers_persistent(self) -> Dict[str, Any]:
+        return {k: v for k, v in self.headers.items() if self.is_header_persistent(k)}
+
+    @property
+    def receiver(self) -> Optional[str]:
+        return self.headers.get("receiver")
+
     def fork_task(self) -> "Task":
         """
         Fork task to transfer single task to many queues (but use different UID).
 
         Used internally by karton-system
 
         :return: Forked copy of the original task
 
         :meta private:
         """
         new_task = Task(
             headers=self.headers,
+            headers_persistent=self.headers_persistent,
             payload=self.payload,
             payload_persistent=self.payload_persistent,
             priority=self.priority,
             parent_uid=self.parent_uid,
             root_uid=self.root_uid,
             orig_uid=self.uid,
         )
@@ -156,14 +192,15 @@
             ported to :code:`task.derive_task(headers)`
 
         :param headers: New headers for the task
         :return: Copy of task with new headers
         """
         new_task = Task(
             headers=headers,
+            headers_persistent=self.headers_persistent,
             payload=self.payload,
             payload_persistent=self.payload_persistent,
         )
         return new_task
 
     def matches_filters(self, filters: List[Dict[str, Any]]) -> bool:
         """
@@ -223,44 +260,80 @@
         """
         for name, content in other_task.payload_persistent.items():
             self.payload_persistent[name] = content
             if name in self.payload:
                 # Delete conflicting non-persistent payload
                 del self.payload[name]
 
+    def merge_persistent_headers(self, other_task: "Task") -> None:
+        """
+        Merge persistent headers from another task
+
+        :param other_task: Task from which to merge persistent headers
+
+        :meta private:
+        """
+        self.headers.update(other_task.headers_persistent)
+        self._headers_persistent_keys = self._headers_persistent_keys.union(
+            other_task._headers_persistent_keys
+        )
+
+    def to_dict(self) -> Dict[str, Any]:
+        """
+        Transform task data into dictionary
+        :return: Task data dictionary
+
+        :meta private:
+        """
+
+        def serialize_resources(obj):
+            if type(obj) is dict:
+                return {k: serialize_resources(v) for k, v in obj.items()}
+            elif type(obj) is list or type(obj) is tuple:
+                return [serialize_resources(v) for v in obj]
+            elif isinstance(obj, ResourceBase):
+                return {"__karton_resource__": obj.to_dict()}
+            else:
+                return obj
+
+        headers_persistent = self.headers_persistent
+        payload_persistent = {
+            **self.payload_persistent,
+            # Compatibility with Karton <5.2.0
+            # Consumers <5.2.0 are not merging headers_persistent
+            # from previous task, so we need to hide it there to
+            # let karton-system fix it for us during deserialization
+            "__headers_persistent": headers_persistent,
+        }
+
+        return {
+            "uid": self.uid,
+            "root_uid": self.root_uid,
+            "parent_uid": self.parent_uid,
+            "orig_uid": self.orig_uid,
+            "status": self.status.value,
+            "priority": self.priority.value,
+            "last_update": self.last_update,
+            "payload": serialize_resources(self.payload),
+            "payload_persistent": serialize_resources(payload_persistent),
+            "headers": self.headers,
+            "headers_persistent": headers_persistent,
+            "error": self.error,
+        }
+
     def serialize(self, indent: Optional[int] = None) -> str:
         """
         Serialize task data into JSON string
         :param indent: Indent to use while serializing
         :return: Serialized task data
 
         :meta private:
         """
-
-        class KartonResourceEncoder(json.JSONEncoder):
-            def default(kself, obj: Any):
-                if isinstance(obj, ResourceBase):
-                    return {"__karton_resource__": obj.to_dict()}
-                return json.JSONEncoder.default(kself, obj)
-
         return json.dumps(
-            {
-                "uid": self.uid,
-                "root_uid": self.root_uid,
-                "parent_uid": self.parent_uid,
-                "orig_uid": self.orig_uid,
-                "status": self.status.value,
-                "priority": self.priority.value,
-                "last_update": self.last_update,
-                "payload": self.payload,
-                "payload_persistent": self.payload_persistent,
-                "headers": self.headers,
-                "error": self.error,
-            },
-            cls=KartonResourceEncoder,
+            self.to_dict(),
             indent=indent,
             sort_keys=True,
         )
 
     def walk_payload_bags(self) -> Iterator[Tuple[Dict[str, Any], str, Any]]:
         """
         Iterate over all payload bags and direct payloads contained in them
@@ -308,21 +381,32 @@
         """
         for element in recursive_iter([self.payload, self.payload_persistent]):
             if isinstance(element, ResourceBase):
                 yield element
 
     @staticmethod
     def unserialize(
-        data: Union[str, bytes], backend: Optional["KartonBackend"] = None
+        data: Union[str, bytes],
+        backend: Optional["KartonBackend"] = None,
+        parse_resources: bool = True,
     ) -> "Task":
         """
         Unserialize Task instance from JSON string
 
         :param data: JSON-serialized task
         :param backend: Backend instance to be bound to RemoteResource objects
+        :param parse_resources: |
+            If set to False (default is True), method doesn't
+            deserialize '__karton_resource__' entries, which speeds up deserialization
+            process. This flag is used mainly for multiple task processing e.g.
+            filtering based on status.
+            When resource deserialization is turned off, Task.unserialize will try
+            to use faster 3rd-party JSON parser (orjson) if it's installed. It's not
+            added as a required dependency but can speed up things if you need to check
+            status of multiple tasks at once.
         :return: Unserialized Task object
 
         :meta private:
         """
 
         def unserialize_resources(value: Any) -> Any:
             """
@@ -332,34 +416,48 @@
             if isinstance(value, dict) and "__karton_resource__" in value:
                 return RemoteResource.from_dict(value["__karton_resource__"], backend)
             return value
 
         if not isinstance(data, str):
             data = data.decode("utf8")
 
-        task_data = json.loads(data, object_hook=unserialize_resources)
+        if parse_resources:
+            task_data = json.loads(data, object_hook=unserialize_resources)
+        else:
+            task_data = orjson.loads(data)
+
+        # Compatibility with Karton <5.2.0
+        headers_persistent_fallback = task_data["payload_persistent"].get(
+            "__headers_persistent", None
+        )
+        headers_persistent = task_data.get(
+            "headers_persistent", headers_persistent_fallback
+        )
 
-        task = Task(task_data["headers"])
-        task.uid = task_data["uid"]
-        task.root_uid = task_data["root_uid"]
-        task.parent_uid = task_data["parent_uid"]
-        # Compatibility with <= 3.x.x (get)
-        task.orig_uid = task_data.get("orig_uid", None)
-        task.status = TaskState(task_data["status"])
-        # Compatibility with <= 3.x.x (get)
-        task.error = task_data.get("error")
-        # Compatibility with <= 2.x.x (get)
-        task.priority = (
-            TaskPriority(task_data.get("priority"))
-            if "priority" in task_data
-            else TaskPriority.NORMAL
-        )
-        task.last_update = task_data.get("last_update", None)
-        task.payload = task_data["payload"]
-        task.payload_persistent = task_data["payload_persistent"]
+        task = Task(
+            task_data["headers"],
+            headers_persistent=headers_persistent,
+            uid=task_data["uid"],
+            root_uid=task_data["root_uid"],
+            parent_uid=task_data["parent_uid"],
+            # Compatibility with <= 3.x.x (get)
+            orig_uid=task_data.get("orig_uid", None),
+            payload=task_data["payload"],
+            payload_persistent=task_data["payload_persistent"],
+            # Compatibility with <= 3.x.x (get)
+            error=task_data.get("error"),
+            # Compatibility with <= 2.x.x (get)
+            priority=(
+                TaskPriority(task_data.get("priority"))
+                if "priority" in task_data
+                else TaskPriority.NORMAL
+            ),
+            _status=TaskState(task_data["status"]),
+            _last_update=task_data.get("last_update", None),
+        )
         return task
 
     def __repr__(self) -> str:
         return self.serialize()
 
     def add_payload(self, name: str, content: Any, persistent: bool = False) -> None:
         """
@@ -444,14 +542,23 @@
         Checks whether payload exists
 
         :param name: Name of the payload to be checked
         :return: If tasks payload contains a value with given name
         """
         return name in self.payload or name in self.payload_persistent
 
+    def is_header_persistent(self, name: str) -> bool:
+        """
+        Checks whether header exists and is persistent
+
+        :param name: Name of the header to be checked
+        :return: If tasks header with given name is persistent
+        """
+        return name in self._headers_persistent_keys
+
     def is_payload_persistent(self, name: str) -> bool:
         """
         Checks whether payload exists and is persistent
 
         :param name: Name of the payload to be checked
         :return: If tasks payload with given name is persistent
         """
```

## karton/core/utils.py

```diff
@@ -1,21 +1,32 @@
 import functools
+import itertools
 import signal
 from contextlib import contextmanager
 from typing import Any, Callable, Iterator, Sequence, Tuple, TypeVar
 
 from .exceptions import HardShutdownInterrupt, TaskTimeoutError
 
 T = TypeVar("T")
 
 
 def chunks(seq: Sequence[T], size: int) -> Iterator[Sequence[T]]:
     return (seq[pos : pos + size] for pos in range(0, len(seq), size))
 
 
+def chunks_iter(seq: Iterator[T], size: int) -> Iterator[Sequence[T]]:
+    # We need to ensure that seq is iterator, so this method works correctly
+    it = iter(seq)
+    while True:
+        elements = list(itertools.islice(it, size))
+        if len(elements) == 0:
+            return
+        yield elements
+
+
 def recursive_iter(obj: Any) -> Iterator[Any]:
     """
     Yields all values recursively from nested list/dict structures
 
     :param obj: Object to iterate over
     """
     if isinstance(obj, (list, tuple)):
```

## karton/system/system.py

```diff
@@ -49,60 +49,60 @@
 
     def gc_collect_resources(self) -> None:
         # Collects unreferenced resources left in object storage
         karton_bucket = self.backend.default_bucket_name
         resources_to_remove = set(self.backend.list_objects(karton_bucket))
         # Note: it is important to get list of resources before getting list of tasks!
         # Task is created before resource upload to lock the reference to the resource.
-        tasks = self.backend.get_all_tasks()
+        tasks = self.backend.iter_all_tasks()
         for task in tasks:
             for resource in task.iterate_resources():
                 # If resource is referenced by task: remove it from set
                 if (
                     resource.bucket == karton_bucket
                     and resource.uid in resources_to_remove
                 ):
                     resources_to_remove.remove(resource.uid)
         # Remove unreferenced resources
         if resources_to_remove:
             self.backend.remove_objects(karton_bucket, resources_to_remove)
 
-    def gc_collect_abandoned_queues(self):
-        online_consumers = self.backend.get_online_consumers()
-        for bind in self.backend.get_binds():
-            identity = bind.identity
-            if identity not in online_consumers and not bind.persistent:
-                # If offline and not persistent: remove queue
-                for queue in self.backend.get_queue_names(identity):
-                    self.log.info(
-                        "Non-persistent: unwinding tasks from queue %s", queue
-                    )
-                    removed_tasks = self.backend.remove_task_queue(queue)
-                    for removed_task in removed_tasks:
-                        self.log.info("Unwinding task %s", str(removed_task.uid))
-                        # Mark task as finished
-                        self.backend.set_task_status(removed_task, TaskState.FINISHED)
-                    self.log.info("Non-persistent: removing bind %s", identity)
-                    self.backend.unregister_bind(identity)
-
     def gc_collect_tasks(self) -> None:
         # Collects finished tasks
         root_tasks = set()
         running_root_tasks = set()
-        tasks = self.backend.get_all_tasks()
-        enqueued_task_uids = self.backend.get_task_ids_from_queue(KARTON_TASKS_QUEUE)
+        unrouted_task_uids = self.backend.get_task_ids_from_queue(KARTON_TASKS_QUEUE)
 
         current_time = time.time()
         to_delete = []
 
-        for task in tasks:
+        queues_to_clear = set()
+        online_consumers = self.backend.get_online_consumers()
+        for bind in self.backend.get_binds():
+            identity = bind.identity
+            if identity not in online_consumers and not bind.persistent:
+                # If offline and not persistent: mark queue to be removed
+                queues_to_clear.add(identity)
+                self.log.info("Non-persistent: removing bind %s", identity)
+                self.backend.unregister_bind(identity)
+                self.backend.delete_consumer_queues(identity)
+
+        for task in self.backend.iter_all_tasks(parse_resources=False):
             root_tasks.add(task.root_uid)
-            if (
+            if task.receiver in queues_to_clear:
+                to_delete.append(task)
+                self.log.info(
+                    "Task %s is abandoned by inactive non-persistent consumer."
+                    "Killed. (receiver: %s)",
+                    task.uid,
+                    task.headers.get("receiver", "<unknown>"),
+                )
+            elif (
                 task.status == TaskState.DECLARED
-                and task.uid not in enqueued_task_uids
+                and task.uid not in unrouted_task_uids
                 and task.last_update is not None
                 and current_time > task.last_update + self.task_dispatched_timeout
             ):
                 to_delete.append(task)
                 self.log.warning(
                     "Task %s is in Dispatched state more than %d seconds. "
                     "Killed. (origin: %s)",
@@ -154,15 +154,14 @@
         for finished_root_task in root_tasks.difference(running_root_tasks):
             # TODO: Notification needed
             self.log.debug("GC: Finished root task %s", finished_root_task)
 
     def gc_collect(self) -> None:
         if time.time() > (self.last_gc_trigger + self.gc_interval):
             try:
-                self.gc_collect_abandoned_queues()
                 self.gc_collect_tasks()
                 self.gc_collect_resources()
             except Exception:
                 self.log.exception("GC: Exception during garbage collection")
             self.last_gc_trigger = time.time()
 
     def route_task(self, task: Task, binds: List[KartonBind]) -> None:
```

## Comparing `karton_core-5.1.0-nspkg.pth` & `karton_core-5.2.0-nspkg.pth`

 * *Files identical despite different names*

## Comparing `karton_core-5.1.0.dist-info/LICENSE` & `karton_core-5.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `karton_core-5.1.0.dist-info/METADATA` & `karton_core-5.2.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 Metadata-Version: 2.1
 Name: karton-core
-Version: 5.1.0
+Version: 5.2.0
 Summary: Distributed malware analysis orchestration framework
 Home-page: https://github.com/CERT-Polska/karton
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python
 Classifier: Operating System :: OS Independent
 Classifier: License :: OSI Approved :: BSD License
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: boto3
+Requires-Dist: orjson
 Requires-Dist: redis
 
 # Karton <img src="img/logo.svg" width="64">
 
 Distributed malware processing framework based on Python, Redis and S3.
 
 ## The idea
```

## Comparing `karton_core-5.1.0.dist-info/RECORD` & `karton_core-5.2.0.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,26 +1,26 @@
-karton_core-5.1.0-nspkg.pth,sha256=vHa-jm6pBTeInFrmnsHMg9AOeD88czzQy-6QCFbpRcM,539
+karton_core-5.2.0-nspkg.pth,sha256=vHa-jm6pBTeInFrmnsHMg9AOeD88czzQy-6QCFbpRcM,539
 karton/core/__init__.py,sha256=QuT0BWZyp799eY90tK3H1OD2hwuusqMJq8vQwpB3kG4,337
-karton/core/__version__.py,sha256=lJXb1iNZTQ4hv-awaWFttBVNuyeuI-R9bpTLzszc-Ps,22
-karton/core/backend.py,sha256=HQGGGnIVdY3Tye_5sW0xvitFVi98FgSEv5j3FgtbnNE,29037
-karton/core/base.py,sha256=efOBDe83ocW-VIDaY6keIgRlIrvGA1ENPK6vNBflmLc,7676
+karton/core/__version__.py,sha256=VqMNAkt1E9Y42lSKxWJZRHYFFLxlGbjtTti_CRNjAv8,22
+karton/core/backend.py,sha256=nZjo6M-gB8QD3s48dCIaz0ihJCpAda7hO38NYQGlswA,32954
+karton/core/base.py,sha256=C6Lco3E0XCsxvEjeVOLR9fxh_IWJ1vjC9BqUYsQyewE,8083
 karton/core/config.py,sha256=7oKchitq6pWzPuXRfjBXqVT_BgGIz2p-CDo1RGaNJQg,8118
 karton/core/exceptions.py,sha256=ILtwpn9K0FxwGevYu2_oBvez1ajaC9lLRmF8F8I0fS4,149
 karton/core/inspect.py,sha256=rIa0u4u12vG_RudPfc9UAS4RZD56W8qbUa8n1dDIkX0,4868
-karton/core/karton.py,sha256=ijl743X8RNO3_W-wCavfEAdlNfu5JDJ0x0GieNcUrbE,14769
+karton/core/karton.py,sha256=-BWsNvbL_yFFlSPjEOXlBqkENEWLTfIB0ETjowh7Mjo,14863
 karton/core/logger.py,sha256=J3XAyG88U0cwYC9zR6E3QD1uJenrQh7zS9-HgxhqeAs,2040
 karton/core/main.py,sha256=ir1-dhn3vbwfh2YHiM6ZYfRBbjwLvJSz0d8tuK1mb_4,8310
 karton/core/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 karton/core/resource.py,sha256=tA3y_38H9HVKIrCeAU70zHUkQUv0BuCQWMC470JLxxc,20321
-karton/core/task.py,sha256=EHmekvcDurvsmyQqPhelSiqeMvMT23I4WCYyd_OaGac,15193
+karton/core/task.py,sha256=NITwXi0t5YDcbm8eOqLr-gx7lxcd6UANFbXVmykzgAM,19103
 karton/core/test.py,sha256=tms-YM7sUKQDHN0vm2_W7DIvHnO_ld_VPsWHnsbKSfk,9102
-karton/core/utils.py,sha256=K4-NskWgpZbxMzG4SCZeCcORnVkIFYl9H5tNlBPyKMA,3763
+karton/core/utils.py,sha256=sEVqGdVPyYswWuVn8wYXBQmln8Az826N_2HgC__pmW8,4090
 karton/system/__init__.py,sha256=JF51OqRU_Y4c0unOulvmv1KzSHSq4ZpXU8ZsH4nefRM,63
 karton/system/__main__.py,sha256=QJkwIlSwaPRdzwKlNmCAL41HtDAa73db9MZKWmOfxGM,56
-karton/system/system.py,sha256=virVjQiPkH-jKVuOvLJkAprgkVSZkB7mjsPH-4qj1ls,13933
-karton_core-5.1.0.dist-info/LICENSE,sha256=o8h7hYhn7BJC_-DmrfqWwLjaR_Gbe0TZOOQJuN2ca3I,1519
-karton_core-5.1.0.dist-info/METADATA,sha256=uoMe_f5zIYy8Ue09dqGcl-Qk-NldJG-37ynxNBoo5GQ,6830
-karton_core-5.1.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-karton_core-5.1.0.dist-info/entry_points.txt,sha256=FJj5EZuvFP0LkagjX_dLbRGBUnuLjgBiSyiFfq4c86U,99
-karton_core-5.1.0.dist-info/namespace_packages.txt,sha256=X8SslCPsqXDCnGZqrYYolzT3xPzJMq1r-ZQSc0jfAEA,7
-karton_core-5.1.0.dist-info/top_level.txt,sha256=X8SslCPsqXDCnGZqrYYolzT3xPzJMq1r-ZQSc0jfAEA,7
-karton_core-5.1.0.dist-info/RECORD,,
+karton/system/system.py,sha256=AsQLGF_8YI-o4dCOe2hyQefYlhGEguFSoPg1EcQtJyU,13791
+karton_core-5.2.0.dist-info/LICENSE,sha256=o8h7hYhn7BJC_-DmrfqWwLjaR_Gbe0TZOOQJuN2ca3I,1519
+karton_core-5.2.0.dist-info/METADATA,sha256=zAYIjJ_2_xW1AfT0143FrPl3CW81d0rE2ACsDD2wmQY,6852
+karton_core-5.2.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+karton_core-5.2.0.dist-info/entry_points.txt,sha256=FJj5EZuvFP0LkagjX_dLbRGBUnuLjgBiSyiFfq4c86U,99
+karton_core-5.2.0.dist-info/namespace_packages.txt,sha256=X8SslCPsqXDCnGZqrYYolzT3xPzJMq1r-ZQSc0jfAEA,7
+karton_core-5.2.0.dist-info/top_level.txt,sha256=X8SslCPsqXDCnGZqrYYolzT3xPzJMq1r-ZQSc0jfAEA,7
+karton_core-5.2.0.dist-info/RECORD,,
```

