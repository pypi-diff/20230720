# Comparing `tmp/maximal-1.0.tar.gz` & `tmp/maximal-1.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "maximal-1.0.tar", last modified: Sat Feb  4 23:46:08 2023, max compression
+gzip compressed data, was "maximal-1.1.tar", last modified: Thu Jul 20 08:17:01 2023, max compression
```

## Comparing `maximal-1.0.tar` & `maximal-1.1.tar`

### file list

```diff
@@ -1,18 +1,22 @@
-drwxrwxr-x   0 ivan      (1000) ivan      (1000)        0 2023-02-04 23:46:08.416701 maximal-1.0/
--rw-rw-r--   0 ivan      (1000) ivan      (1000)      133 2023-02-04 23:41:55.000000 maximal-1.0/CHANGELOG.txt
--rw-rw-r--   0 ivan      (1000) ivan      (1000)     1071 2023-02-04 23:41:55.000000 maximal-1.0/LICENSE
--rw-rw-r--   0 ivan      (1000) ivan      (1000)       26 2023-02-04 23:41:55.000000 maximal-1.0/MANIFEST.in
--rw-rw-r--   0 ivan      (1000) ivan      (1000)     2577 2023-02-04 23:46:08.416701 maximal-1.0/PKG-INFO
--rw-rw-r--   0 ivan      (1000) ivan      (1000)     2019 2023-02-04 23:41:55.000000 maximal-1.0/README.md
-drwxrwxr-x   0 ivan      (1000) ivan      (1000)        0 2023-02-04 23:46:08.416701 maximal-1.0/maximal/
--rw-rw-r--   0 ivan      (1000) ivan      (1000)      468 2023-02-04 23:41:55.000000 maximal-1.0/maximal/__init__.py
--rw-rw-r--   0 ivan      (1000) ivan      (1000)    10731 2023-02-04 23:41:55.000000 maximal-1.0/maximal/layers.py
--rw-rw-r--   0 ivan      (1000) ivan      (1000)      941 2023-02-04 23:41:55.000000 maximal-1.0/maximal/schedules.py
-drwxrwxr-x   0 ivan      (1000) ivan      (1000)        0 2023-02-04 23:46:08.416701 maximal-1.0/maximal.egg-info/
--rw-rw-r--   0 ivan      (1000) ivan      (1000)     2577 2023-02-04 23:46:08.000000 maximal-1.0/maximal.egg-info/PKG-INFO
--rw-rw-r--   0 ivan      (1000) ivan      (1000)      265 2023-02-04 23:46:08.000000 maximal-1.0/maximal.egg-info/SOURCES.txt
--rw-rw-r--   0 ivan      (1000) ivan      (1000)        1 2023-02-04 23:46:08.000000 maximal-1.0/maximal.egg-info/dependency_links.txt
--rw-rw-r--   0 ivan      (1000) ivan      (1000)       22 2023-02-04 23:46:08.000000 maximal-1.0/maximal.egg-info/requires.txt
--rw-rw-r--   0 ivan      (1000) ivan      (1000)        8 2023-02-04 23:46:08.000000 maximal-1.0/maximal.egg-info/top_level.txt
--rw-rw-r--   0 ivan      (1000) ivan      (1000)       38 2023-02-04 23:46:08.416701 maximal-1.0/setup.cfg
--rw-rw-r--   0 ivan      (1000) ivan      (1000)     2027 2023-02-04 23:41:55.000000 maximal-1.0/setup.py
+drwxrwxr-x   0 ivan      (1000) ivan      (1000)        0 2023-07-20 08:17:01.974593 maximal-1.1/
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)      133 2023-02-04 23:41:55.000000 maximal-1.1/CHANGELOG.txt
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     1071 2023-02-04 23:41:55.000000 maximal-1.1/LICENSE
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)       26 2023-02-04 23:41:55.000000 maximal-1.1/MANIFEST.in
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     2762 2023-07-20 08:17:01.974593 maximal-1.1/PKG-INFO
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     2204 2023-07-15 11:41:34.000000 maximal-1.1/README.md
+drwxrwxr-x   0 ivan      (1000) ivan      (1000)        0 2023-07-20 08:17:01.974593 maximal-1.1/maximal/
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)      654 2023-07-20 08:11:00.000000 maximal-1.1/maximal/__init__.py
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)    10731 2023-07-13 18:42:14.000000 maximal-1.1/maximal/layers.py
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     2014 2023-07-20 08:11:00.000000 maximal-1.1/maximal/models.py
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)      941 2023-02-04 23:41:55.000000 maximal-1.1/maximal/schedules.py
+drwxrwxr-x   0 ivan      (1000) ivan      (1000)        0 2023-07-20 08:17:01.974593 maximal-1.1/maximal.egg-info/
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     2762 2023-07-20 08:17:01.000000 maximal-1.1/maximal.egg-info/PKG-INFO
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)      339 2023-07-20 08:17:01.000000 maximal-1.1/maximal.egg-info/SOURCES.txt
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)        1 2023-07-20 08:17:01.000000 maximal-1.1/maximal.egg-info/dependency_links.txt
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)       27 2023-07-20 08:17:01.000000 maximal-1.1/maximal.egg-info/requires.txt
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)        8 2023-07-20 08:17:01.000000 maximal-1.1/maximal.egg-info/top_level.txt
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)       38 2023-07-20 08:17:01.974593 maximal-1.1/setup.cfg
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     2043 2023-07-20 08:11:00.000000 maximal-1.1/setup.py
+drwxrwxr-x   0 ivan      (1000) ivan      (1000)        0 2023-07-20 08:17:01.974593 maximal-1.1/tests/
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     3275 2023-07-20 08:11:00.000000 maximal-1.1/tests/test_layer_outputs.py
+-rw-rw-r--   0 ivan      (1000) ivan      (1000)     1716 2023-07-20 08:11:00.000000 maximal-1.1/tests/test_save_and_load.py
```

### Comparing `maximal-1.0/LICENSE` & `maximal-1.1/LICENSE`

 * *Files identical despite different names*

### Comparing `maximal-1.0/PKG-INFO` & `maximal-1.1/PKG-INFO`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 00000000: 4d65 7461 6461 7461 2d56 6572 7369 6f6e  Metadata-Version
 00000010: 3a20 322e 310a 4e61 6d65 3a20 6d61 7869  : 2.1.Name: maxi
-00000020: 6d61 6c0a 5665 7273 696f 6e3a 2031 2e30  mal.Version: 1.0
+00000020: 6d61 6c0a 5665 7273 696f 6e3a 2031 2e31  mal.Version: 1.1
 00000030: 0a53 756d 6d61 7279 3a20 5465 6e73 6f72  .Summary: Tensor
 00000040: 466c 6f77 2d63 6f6d 7061 7469 626c 6520  Flow-compatible 
 00000050: 5472 616e 7366 6f72 6d65 7220 6c61 7965  Transformer laye
 00000060: 7273 2061 6e64 206d 6f64 656c 732e 0a48  rs and models..H
 00000070: 6f6d 652d 7061 6765 3a20 6874 7470 733a  ome-page: https:
 00000080: 2f2f 6769 7468 7562 2e63 6f6d 2f49 7661  //github.com/Iva
 00000090: 6e42 6f6e 6769 6f72 6e69 2f6d 6178 696d  nBongiorni/maxim
@@ -45,118 +45,129 @@
 000002c0: 6f6e 206c 6962 7261 7279 2074 6861 7420  on library that 
 000002d0: 7072 6f76 6964 6573 206d 6f64 656c 7320  provides models 
 000002e0: 616e 6420 6c61 7965 7273 2074 6f20 696d  and layers to im
 000002f0: 706c 656d 656e 7420 6375 7374 6f6d 2054  plement custom T
 00000300: 7261 6e73 666f 726d 6572 206e 6575 7261  ransformer neura
 00000310: 6c20 6e65 7477 6f72 6b73 2e0a 0a42 7569  l networks...Bui
 00000320: 6c74 206f 6e20 5465 6e73 6f72 466c 6f77  lt on TensorFlow
-00000330: 2032 2e0a 0a23 2049 6e73 7461 6c6c 6174   2...# Installat
-00000340: 696f 6e0a 4974 7320 696e 7374 616c 6c61  ion.Its installa
-00000350: 7469 6f6e 2069 7320 7374 7261 6967 6874  tion is straight
-00000360: 666f 7277 6172 643a 0a0a 6060 600a 7069  forward:..```.pi
-00000370: 7020 696e 7374 616c 6c20 6d61 7869 6d61  p install maxima
-00000380: 6c0a 6060 600a 0a23 2048 6f77 2074 6f20  l.```..# How to 
-00000390: 7573 6520 6974 3f0a 606d 6178 696d 616c  use it?.`maximal
-000003a0: 6020 6973 2063 6f6d 6d6f 6e6c 7920 6361  ` is commonly ca
-000003b0: 6c6c 6564 2061 733a 0a0a 6060 600a 696d  lled as:..```.im
-000003c0: 706f 7274 206d 6178 696d 616c 0a66 726f  port maximal.fro
-000003d0: 6d20 6d61 7869 6d61 6c2e 6c61 7965 7273  m maximal.layers
-000003e0: 2069 6d70 6f72 7420 5472 616e 7366 6f72   import Transfor
-000003f0: 6d65 724c 6179 6572 2c20 4750 544c 6179  merLayer, GPTLay
-00000400: 6572 0a60 6060 0a0a 616e 6420 6361 6e20  er.```..and can 
-00000410: 6265 2075 7365 6420 696e 2061 2060 7466  be used in a `tf
-00000420: 2e6b 6572 6173 6020 6d6f 6465 6c20 6173  .keras` model as
-00000430: 2061 6e79 2063 6f6d 6d6f 6e20 6c61 7965   any common laye
-00000440: 722e 0a0a 0a23 2044 6f63 756d 656e 7461  r....# Documenta
-00000450: 7469 6f6e 0a41 6e20 5b4f 6666 6963 6961  tion.An [Officia
-00000460: 6c20 5765 6273 6974 655d 2868 7474 7073  l Website](https
-00000470: 3a2f 2f69 7661 6e62 6f6e 6769 6f72 6e69  ://ivanbongiorni
-00000480: 2e67 6974 6875 622e 696f 2f6d 6178 696d  .github.io/maxim
-00000490: 616c 2f29 2069 7320 6e6f 7720 6176 6169  al/) is now avai
-000004a0: 6c61 626c 6520 7769 7468 2064 6f63 756d  lable with docum
-000004b0: 656e 7461 7469 6f6e 2061 6e64 2074 7574  entation and tut
-000004c0: 6f72 6961 6c73 2e0a 0a0a 2320 456c 656d  orials....# Elem
-000004d0: 656e 7473 0a0a 496e 2060 6c61 7965 7273  ents..In `layers
-000004e0: 2e70 7960 3a0a 2d20 6053 656c 6641 7474  .py`:.- `SelfAtt
-000004f0: 656e 7469 6f6e 603a 2060 6b65 7261 732e  ention`: `keras.
-00000500: 4c61 7965 7260 2c20 636f 6d70 7574 6573  Layer`, computes
-00000510: 202a 5363 616c 6564 2044 6f74 2d50 726f   *Scaled Dot-Pro
-00000520: 6475 6374 2041 7474 656e 7469 6f6e 2a2e  duct Attention*.
-00000530: 0a0a 2d20 604d 756c 7469 4865 6164 5365  ..- `MultiHeadSe
-00000540: 6c66 4174 7465 6e74 696f 6e60 3a20 606b  lfAttention`: `k
-00000550: 6572 6173 2e4c 6179 6572 602c 2069 7420  eras.Layer`, it 
-00000560: 6973 2061 2063 6f6e 6361 7465 6e61 7469  is a concatenati
-00000570: 6f6e 206f 6620 6053 656c 6641 7474 656e  on of `SelfAtten
-00000580: 7469 6f6e 6020 6c61 7965 7273 2c20 7265  tion` layers, re
-00000590: 7369 7a65 6420 6261 636b 2074 6f20 6f72  sized back to or
-000005a0: 6967 696e 616c 2069 6e70 7574 2073 6861  iginal input sha
-000005b0: 7065 2074 6872 6f75 6768 206c 696e 6561  pe through linea
-000005c0: 7220 7472 616e 7366 6f72 6d61 7469 6f6e  r transformation
-000005d0: 2e0a 0a2d 2060 506f 7369 7469 6f6e 616c  ...- `Positional
-000005e0: 456d 6265 6464 696e 6760 3a20 606b 6572  Embedding`: `ker
-000005f0: 6173 2e4c 6179 6572 602c 2069 6d70 6c65  as.Layer`, imple
-00000600: 6d65 6e74 7320 646f 7562 6c65 2045 6d62  ments double Emb
-00000610: 6564 6469 6e67 206c 6179 6572 7320 7573  edding layers us
-00000620: 6564 2069 6e20 5472 616e 7366 6f72 6d65  ed in Transforme
-00000630: 7273 206c 6974 6572 6174 7572 652c 2066  rs literature, f
-00000640: 6f72 2074 6f6b 656e 7320 616e 6420 706f  or tokens and po
-00000650: 7369 7469 6f6e 732e 2050 6f73 6974 696f  sitions. Positio
-00000660: 6e61 6c20 656e 636f 6469 6e67 2069 7320  nal encoding is 
-00000670: 6c65 6172 6e65 6420 7468 726f 7567 6820  learned through 
-00000680: 6120 6074 662e 6b65 7261 732e 6c61 7965  a `tf.keras.laye
-00000690: 7273 2e45 6d62 6564 6469 6e67 2829 6020  rs.Embedding()` 
-000006a0: 6c61 7965 722c 2069 6e73 7465 6164 206f  layer, instead o
-000006b0: 6620 6465 7465 726d 696e 6973 7469 6320  f deterministic 
-000006c0: 706f 7369 7469 6f6e 616c 2065 6e63 6f64  positional encod
-000006d0: 696e 6720 696e 2074 6865 206f 7269 6769  ing in the origi
-000006e0: 6e61 6c20 7061 7065 722e 0a0a 2d20 6054  nal paper...- `T
-000006f0: 7261 6e73 666f 726d 6572 4c61 7965 7260  ransformerLayer`
-00000700: 3a20 606b 6572 6173 2e4c 6179 6572 6020  : `keras.Layer` 
-00000710: 7369 6e67 6c65 2054 7261 6e73 666f 726d  single Transform
-00000720: 6572 2045 6e63 6f64 6572 2070 6965 6365  er Encoder piece
-00000730: 2e20 4974 2063 616e 2062 6520 7573 6564  . It can be used
-00000740: 2069 6e73 6964 6520 616e 7920 6053 6571   inside any `Seq
-00000750: 7565 6e74 6961 6c28 2960 206d 6f64 656c  uential()` model
-00000760: 2069 6e20 4b65 7261 732e 0a0a 2d20 6047   in Keras...- `G
-00000770: 5054 4c61 7965 7260 3a20 606b 6572 6173  PTLayer`: `keras
-00000780: 2e4c 6179 6572 6020 4750 5420 626c 6f63  .Layer` GPT bloc
-00000790: 6b2e 2053 696d 696c 6172 2074 6f20 6054  k. Similar to `T
-000007a0: 7261 6e73 666f 726d 6572 4c61 7965 7260  ransformerLayer`
-000007b0: 2062 7574 2077 6974 6820 6361 7573 616c   but with causal
-000007c0: 2041 7474 656e 7469 6f6e 206d 6563 6861   Attention mecha
-000007d0: 6e69 736d 2e20 4974 2063 616e 2062 6520  nism. It can be 
-000007e0: 7573 6564 2069 6e73 6964 6520 616e 7920  used inside any 
-000007f0: 6053 6571 7565 6e74 6961 6c28 2960 206d  `Sequential()` m
-00000800: 6f64 656c 2069 6e20 4b65 7261 732e 0a0a  odel in Keras...
-00000810: 0a49 6e20 6073 6368 6564 756c 6573 2e70  .In `schedules.p
-00000820: 7960 3a0a 2d20 604f 7269 6769 6e61 6c54  y`:.- `OriginalT
-00000830: 7261 6e73 666f 726d 6572 5363 6865 6475  ransformerSchedu
-00000840: 6c65 603a 2060 6b65 7261 732e 4c61 7965  le`: `keras.Laye
-00000850: 7260 2069 6d70 6c65 6d65 6e74 7320 7468  r` implements th
-00000860: 6520 6c65 6172 6e69 6e67 2072 6174 6520  e learning rate 
-00000870: 7363 6865 6475 6c65 206f 6620 7468 6520  schedule of the 
-00000880: 6f72 6967 696e 616c 2054 7261 6e73 666f  original Transfo
-00000890: 726d 6572 2070 6170 6572 2e20 4974 2069  rmer paper. It i
-000008a0: 7320 7461 6b65 6e20 6672 6f6d 2074 6869  s taken from thi
-000008b0: 7320 5b6f 6666 6963 6961 6c20 5465 6e73  s [official Tens
-000008c0: 6f72 466c 6f77 2074 7574 6f72 6961 6c5d  orFlow tutorial]
-000008d0: 2868 7474 7073 3a2f 2f77 7777 2e74 656e  (https://www.ten
-000008e0: 736f 7266 6c6f 772e 6f72 672f 7465 7874  sorflow.org/text
-000008f0: 2f74 7574 6f72 6961 6c73 2f74 7261 6e73  /tutorials/trans
-00000900: 666f 726d 6572 292e 0a0a 2320 5265 7175  former)...# Requ
-00000910: 6972 656d 656e 7473 0a60 6060 0a6e 756d  irements.```.num
-00000920: 7079 0a74 656e 736f 7266 6c6f 7720 3e3d  py.tensorflow >=
-00000930: 2032 2e30 0a60 6060 0a0a 2320 4175 7468   2.0.```..# Auth
-00000940: 6f72 0a49 7661 6e20 426f 6e67 696f 726e  or.Ivan Bongiorn
-00000950: 692e 205b 4c69 6e6b 6564 496e 5d28 6874  i. [LinkedIn](ht
-00000960: 7470 733a 2f2f 7777 772e 6c69 6e6b 6564  tps://www.linked
-00000970: 696e 2e63 6f6d 2f69 6e2f 6976 616e 2d62  in.com/in/ivan-b
-00000980: 6f6e 6769 6f72 6e69 2d62 3861 3538 3331  ongiorni-b8a5831
-00000990: 3634 2f29 0a0a 2320 4c69 6365 6e73 650a  64/)..# License.
-000009a0: 3230 3230 2049 7661 6e20 426f 6e67 696f  2020 Ivan Bongio
-000009b0: 726e 690a 0a54 6869 7320 7265 706f 7369  rni..This reposi
-000009c0: 746f 7279 2069 7320 6c69 6365 6e73 6564  tory is licensed
-000009d0: 2075 6e64 6572 2074 6865 204d 4954 206c   under the MIT l
-000009e0: 6963 656e 7365 2e20 5365 6520 5b4c 4943  icense. See [LIC
-000009f0: 454e 4345 2e74 7874 5d28 2920 666f 7220  ENCE.txt]() for 
-00000a00: 6675 7274 6865 7220 6465 7461 696c 732e  further details.
-00000a10: 0a                                       .
+00000330: 2032 2e0a 0a3c 6120 6872 6566 3d22 7572   2...<a href="ur
+00000340: 6c22 3e3c 696d 6720 7372 633d 2268 7474  l"><img src="htt
+00000350: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
+00000360: 4976 616e 426f 6e67 696f 726e 692f 6d61  IvanBongiorni/ma
+00000370: 7869 6d61 6c2f 626c 6f62 2f6d 6169 6e2f  ximal/blob/main/
+00000380: 7574 696c 732f 6d61 7869 6d61 6c5f 7374  utils/maximal_st
+00000390: 6162 6c65 6469 6666 7573 696f 6e5f 3030  ablediffusion_00
+000003a0: 2e70 6e67 2220 616c 6967 6e3d 2263 656e  .png" align="cen
+000003b0: 7465 7222 3e3c 2f61 3e0a 3c62 723e 0a2a  ter"></a>.<br>.*
+000003c0: 4c6f 676f 2067 656e 6572 6174 6564 2062  Logo generated b
+000003d0: 7920 5374 6162 6c65 2044 6966 6675 7369  y Stable Diffusi
+000003e0: 6f6e 2032 2e31 2a0a 3c62 723e 0a0a 2320  on 2.1*.<br>..# 
+000003f0: 496e 7374 616c 6c61 7469 6f6e 0a49 7473  Installation.Its
+00000400: 2069 6e73 7461 6c6c 6174 696f 6e20 6973   installation is
+00000410: 2073 7472 6169 6768 7466 6f72 7761 7264   straightforward
+00000420: 3a0a 0a60 6060 0a70 6970 2069 6e73 7461  :..```.pip insta
+00000430: 6c6c 206d 6178 696d 616c 0a60 6060 0a0a  ll maximal.```..
+00000440: 2320 486f 7720 746f 2075 7365 2069 743f  # How to use it?
+00000450: 0a60 6d61 7869 6d61 6c60 2069 7320 636f  .`maximal` is co
+00000460: 6d6d 6f6e 6c79 2063 616c 6c65 6420 6173  mmonly called as
+00000470: 3a0a 0a60 6060 0a69 6d70 6f72 7420 6d61  :..```.import ma
+00000480: 7869 6d61 6c0a 6672 6f6d 206d 6178 696d  ximal.from maxim
+00000490: 616c 2e6c 6179 6572 7320 696d 706f 7274  al.layers import
+000004a0: 2054 7261 6e73 666f 726d 6572 4c61 7965   TransformerLaye
+000004b0: 722c 2047 5054 4c61 7965 720a 6060 600a  r, GPTLayer.```.
+000004c0: 0a61 6e64 2063 616e 2062 6520 7573 6564  .and can be used
+000004d0: 2069 6e20 6120 6074 662e 6b65 7261 7360   in a `tf.keras`
+000004e0: 206d 6f64 656c 2061 7320 616e 7920 636f   model as any co
+000004f0: 6d6d 6f6e 206c 6179 6572 2e0a 0a0a 2320  mmon layer....# 
+00000500: 446f 6375 6d65 6e74 6174 696f 6e0a 416e  Documentation.An
+00000510: 205b 4f66 6669 6369 616c 2057 6562 7369   [Official Websi
+00000520: 7465 5d28 6874 7470 733a 2f2f 6976 616e  te](https://ivan
+00000530: 626f 6e67 696f 726e 692e 6769 7468 7562  bongiorni.github
+00000540: 2e69 6f2f 6d61 7869 6d61 6c2f 2920 6973  .io/maximal/) is
+00000550: 206e 6f77 2061 7661 696c 6162 6c65 2077   now available w
+00000560: 6974 6820 646f 6375 6d65 6e74 6174 696f  ith documentatio
+00000570: 6e20 616e 6420 7475 746f 7269 616c 732e  n and tutorials.
+00000580: 0a0a 0a23 2045 6c65 6d65 6e74 730a 0a49  ...# Elements..I
+00000590: 6e20 606c 6179 6572 732e 7079 603a 0a2d  n `layers.py`:.-
+000005a0: 2060 5365 6c66 4174 7465 6e74 696f 6e60   `SelfAttention`
+000005b0: 3a20 606b 6572 6173 2e4c 6179 6572 602c  : `keras.Layer`,
+000005c0: 2063 6f6d 7075 7465 7320 2a53 6361 6c65   computes *Scale
+000005d0: 6420 446f 742d 5072 6f64 7563 7420 4174  d Dot-Product At
+000005e0: 7465 6e74 696f 6e2a 2e0a 0a2d 2060 4d75  tention*...- `Mu
+000005f0: 6c74 6948 6561 6453 656c 6641 7474 656e  ltiHeadSelfAtten
+00000600: 7469 6f6e 603a 2060 6b65 7261 732e 4c61  tion`: `keras.La
+00000610: 7965 7260 2c20 6974 2069 7320 6120 636f  yer`, it is a co
+00000620: 6e63 6174 656e 6174 696f 6e20 6f66 2060  ncatenation of `
+00000630: 5365 6c66 4174 7465 6e74 696f 6e60 206c  SelfAttention` l
+00000640: 6179 6572 732c 2072 6573 697a 6564 2062  ayers, resized b
+00000650: 6163 6b20 746f 206f 7269 6769 6e61 6c20  ack to original 
+00000660: 696e 7075 7420 7368 6170 6520 7468 726f  input shape thro
+00000670: 7567 6820 6c69 6e65 6172 2074 7261 6e73  ugh linear trans
+00000680: 666f 726d 6174 696f 6e2e 0a0a 2d20 6050  formation...- `P
+00000690: 6f73 6974 696f 6e61 6c45 6d62 6564 6469  ositionalEmbeddi
+000006a0: 6e67 603a 2060 6b65 7261 732e 4c61 7965  ng`: `keras.Laye
+000006b0: 7260 2c20 696d 706c 656d 656e 7473 2064  r`, implements d
+000006c0: 6f75 626c 6520 456d 6265 6464 696e 6720  ouble Embedding 
+000006d0: 6c61 7965 7273 2075 7365 6420 696e 2054  layers used in T
+000006e0: 7261 6e73 666f 726d 6572 7320 6c69 7465  ransformers lite
+000006f0: 7261 7475 7265 2c20 666f 7220 746f 6b65  rature, for toke
+00000700: 6e73 2061 6e64 2070 6f73 6974 696f 6e73  ns and positions
+00000710: 2e20 506f 7369 7469 6f6e 616c 2065 6e63  . Positional enc
+00000720: 6f64 696e 6720 6973 206c 6561 726e 6564  oding is learned
+00000730: 2074 6872 6f75 6768 2061 2060 7466 2e6b   through a `tf.k
+00000740: 6572 6173 2e6c 6179 6572 732e 456d 6265  eras.layers.Embe
+00000750: 6464 696e 6728 2960 206c 6179 6572 2c20  dding()` layer, 
+00000760: 696e 7374 6561 6420 6f66 2064 6574 6572  instead of deter
+00000770: 6d69 6e69 7374 6963 2070 6f73 6974 696f  ministic positio
+00000780: 6e61 6c20 656e 636f 6469 6e67 2069 6e20  nal encoding in 
+00000790: 7468 6520 6f72 6967 696e 616c 2070 6170  the original pap
+000007a0: 6572 2e0a 0a2d 2060 5472 616e 7366 6f72  er...- `Transfor
+000007b0: 6d65 724c 6179 6572 603a 2060 6b65 7261  merLayer`: `kera
+000007c0: 732e 4c61 7965 7260 2073 696e 676c 6520  s.Layer` single 
+000007d0: 5472 616e 7366 6f72 6d65 7220 456e 636f  Transformer Enco
+000007e0: 6465 7220 7069 6563 652e 2049 7420 6361  der piece. It ca
+000007f0: 6e20 6265 2075 7365 6420 696e 7369 6465  n be used inside
+00000800: 2061 6e79 2060 5365 7175 656e 7469 616c   any `Sequential
+00000810: 2829 6020 6d6f 6465 6c20 696e 204b 6572  ()` model in Ker
+00000820: 6173 2e0a 0a2d 2060 4750 544c 6179 6572  as...- `GPTLayer
+00000830: 603a 2060 6b65 7261 732e 4c61 7965 7260  `: `keras.Layer`
+00000840: 2047 5054 2062 6c6f 636b 2e20 5369 6d69   GPT block. Simi
+00000850: 6c61 7220 746f 2060 5472 616e 7366 6f72  lar to `Transfor
+00000860: 6d65 724c 6179 6572 6020 6275 7420 7769  merLayer` but wi
+00000870: 7468 2063 6175 7361 6c20 4174 7465 6e74  th causal Attent
+00000880: 696f 6e20 6d65 6368 616e 6973 6d2e 2049  ion mechanism. I
+00000890: 7420 6361 6e20 6265 2075 7365 6420 696e  t can be used in
+000008a0: 7369 6465 2061 6e79 2060 5365 7175 656e  side any `Sequen
+000008b0: 7469 616c 2829 6020 6d6f 6465 6c20 696e  tial()` model in
+000008c0: 204b 6572 6173 2e0a 0a0a 496e 2060 7363   Keras....In `sc
+000008d0: 6865 6475 6c65 732e 7079 603a 0a2d 2060  hedules.py`:.- `
+000008e0: 4f72 6967 696e 616c 5472 616e 7366 6f72  OriginalTransfor
+000008f0: 6d65 7253 6368 6564 756c 6560 3a20 606b  merSchedule`: `k
+00000900: 6572 6173 2e4c 6179 6572 6020 696d 706c  eras.Layer` impl
+00000910: 656d 656e 7473 2074 6865 206c 6561 726e  ements the learn
+00000920: 696e 6720 7261 7465 2073 6368 6564 756c  ing rate schedul
+00000930: 6520 6f66 2074 6865 206f 7269 6769 6e61  e of the origina
+00000940: 6c20 5472 616e 7366 6f72 6d65 7220 7061  l Transformer pa
+00000950: 7065 722e 2049 7420 6973 2074 616b 656e  per. It is taken
+00000960: 2066 726f 6d20 7468 6973 205b 6f66 6669   from this [offi
+00000970: 6369 616c 2054 656e 736f 7246 6c6f 7720  cial TensorFlow 
+00000980: 7475 746f 7269 616c 5d28 6874 7470 733a  tutorial](https:
+00000990: 2f2f 7777 772e 7465 6e73 6f72 666c 6f77  //www.tensorflow
+000009a0: 2e6f 7267 2f74 6578 742f 7475 746f 7269  .org/text/tutori
+000009b0: 616c 732f 7472 616e 7366 6f72 6d65 7229  als/transformer)
+000009c0: 2e0a 0a23 2052 6571 7569 7265 6d65 6e74  ...# Requirement
+000009d0: 730a 6060 600a 6e75 6d70 790a 7465 6e73  s.```.numpy.tens
+000009e0: 6f72 666c 6f77 203e 3d20 322e 300a 6060  orflow >= 2.0.``
+000009f0: 600a 0a23 2041 7574 686f 720a 4976 616e  `..# Author.Ivan
+00000a00: 2042 6f6e 6769 6f72 6e69 2e20 5b4c 696e   Bongiorni. [Lin
+00000a10: 6b65 6449 6e5d 2868 7474 7073 3a2f 2f77  kedIn](https://w
+00000a20: 7777 2e6c 696e 6b65 6469 6e2e 636f 6d2f  ww.linkedin.com/
+00000a30: 696e 2f69 7661 6e2d 626f 6e67 696f 726e  in/ivan-bongiorn
+00000a40: 692d 6238 6135 3833 3136 342f 290a 0a23  i-b8a583164/)..#
+00000a50: 204c 6963 656e 7365 0a32 3032 3020 4976   License.2020 Iv
+00000a60: 616e 2042 6f6e 6769 6f72 6e69 0a0a 5468  an Bongiorni..Th
+00000a70: 6973 2072 6570 6f73 6974 6f72 7920 6973  is repository is
+00000a80: 206c 6963 656e 7365 6420 756e 6465 7220   licensed under 
+00000a90: 7468 6520 4d49 5420 6c69 6365 6e73 652e  the MIT license.
+00000aa0: 2053 6565 205b 4c49 4345 4e43 452e 7478   See [LICENCE.tx
+00000ab0: 745d 2829 2066 6f72 2066 7572 7468 6572  t]() for further
+00000ac0: 2064 6574 6169 6c73 2e0a                  details..
```

### Comparing `maximal-1.0/README.md` & `maximal-1.1/README.md`

 * *Files 8% similar despite different names*

```diff
@@ -10,118 +10,129 @@
 00000090: 7468 6f6e 206c 6962 7261 7279 2074 6861  thon library tha
 000000a0: 7420 7072 6f76 6964 6573 206d 6f64 656c  t provides model
 000000b0: 7320 616e 6420 6c61 7965 7273 2074 6f20  s and layers to 
 000000c0: 696d 706c 656d 656e 7420 6375 7374 6f6d  implement custom
 000000d0: 2054 7261 6e73 666f 726d 6572 206e 6575   Transformer neu
 000000e0: 7261 6c20 6e65 7477 6f72 6b73 2e0a 0a42  ral networks...B
 000000f0: 7569 6c74 206f 6e20 5465 6e73 6f72 466c  uilt on TensorFl
-00000100: 6f77 2032 2e0a 0a23 2049 6e73 7461 6c6c  ow 2...# Install
-00000110: 6174 696f 6e0a 4974 7320 696e 7374 616c  ation.Its instal
-00000120: 6c61 7469 6f6e 2069 7320 7374 7261 6967  lation is straig
-00000130: 6874 666f 7277 6172 643a 0a0a 6060 600a  htforward:..```.
-00000140: 7069 7020 696e 7374 616c 6c20 6d61 7869  pip install maxi
-00000150: 6d61 6c0a 6060 600a 0a23 2048 6f77 2074  mal.```..# How t
-00000160: 6f20 7573 6520 6974 3f0a 606d 6178 696d  o use it?.`maxim
-00000170: 616c 6020 6973 2063 6f6d 6d6f 6e6c 7920  al` is commonly 
-00000180: 6361 6c6c 6564 2061 733a 0a0a 6060 600a  called as:..```.
-00000190: 696d 706f 7274 206d 6178 696d 616c 0a66  import maximal.f
-000001a0: 726f 6d20 6d61 7869 6d61 6c2e 6c61 7965  rom maximal.laye
-000001b0: 7273 2069 6d70 6f72 7420 5472 616e 7366  rs import Transf
-000001c0: 6f72 6d65 724c 6179 6572 2c20 4750 544c  ormerLayer, GPTL
-000001d0: 6179 6572 0a60 6060 0a0a 616e 6420 6361  ayer.```..and ca
-000001e0: 6e20 6265 2075 7365 6420 696e 2061 2060  n be used in a `
-000001f0: 7466 2e6b 6572 6173 6020 6d6f 6465 6c20  tf.keras` model 
-00000200: 6173 2061 6e79 2063 6f6d 6d6f 6e20 6c61  as any common la
-00000210: 7965 722e 0a0a 0a23 2044 6f63 756d 656e  yer....# Documen
-00000220: 7461 7469 6f6e 0a41 6e20 5b4f 6666 6963  tation.An [Offic
-00000230: 6961 6c20 5765 6273 6974 655d 2868 7474  ial Website](htt
-00000240: 7073 3a2f 2f69 7661 6e62 6f6e 6769 6f72  ps://ivanbongior
-00000250: 6e69 2e67 6974 6875 622e 696f 2f6d 6178  ni.github.io/max
-00000260: 696d 616c 2f29 2069 7320 6e6f 7720 6176  imal/) is now av
-00000270: 6169 6c61 626c 6520 7769 7468 2064 6f63  ailable with doc
-00000280: 756d 656e 7461 7469 6f6e 2061 6e64 2074  umentation and t
-00000290: 7574 6f72 6961 6c73 2e0a 0a0a 2320 456c  utorials....# El
-000002a0: 656d 656e 7473 0a0a 496e 2060 6c61 7965  ements..In `laye
-000002b0: 7273 2e70 7960 3a0a 2d20 6053 656c 6641  rs.py`:.- `SelfA
-000002c0: 7474 656e 7469 6f6e 603a 2060 6b65 7261  ttention`: `kera
-000002d0: 732e 4c61 7965 7260 2c20 636f 6d70 7574  s.Layer`, comput
-000002e0: 6573 202a 5363 616c 6564 2044 6f74 2d50  es *Scaled Dot-P
-000002f0: 726f 6475 6374 2041 7474 656e 7469 6f6e  roduct Attention
-00000300: 2a2e 0a0a 2d20 604d 756c 7469 4865 6164  *...- `MultiHead
-00000310: 5365 6c66 4174 7465 6e74 696f 6e60 3a20  SelfAttention`: 
-00000320: 606b 6572 6173 2e4c 6179 6572 602c 2069  `keras.Layer`, i
-00000330: 7420 6973 2061 2063 6f6e 6361 7465 6e61  t is a concatena
-00000340: 7469 6f6e 206f 6620 6053 656c 6641 7474  tion of `SelfAtt
-00000350: 656e 7469 6f6e 6020 6c61 7965 7273 2c20  ention` layers, 
-00000360: 7265 7369 7a65 6420 6261 636b 2074 6f20  resized back to 
-00000370: 6f72 6967 696e 616c 2069 6e70 7574 2073  original input s
-00000380: 6861 7065 2074 6872 6f75 6768 206c 696e  hape through lin
-00000390: 6561 7220 7472 616e 7366 6f72 6d61 7469  ear transformati
-000003a0: 6f6e 2e0a 0a2d 2060 506f 7369 7469 6f6e  on...- `Position
-000003b0: 616c 456d 6265 6464 696e 6760 3a20 606b  alEmbedding`: `k
-000003c0: 6572 6173 2e4c 6179 6572 602c 2069 6d70  eras.Layer`, imp
-000003d0: 6c65 6d65 6e74 7320 646f 7562 6c65 2045  lements double E
-000003e0: 6d62 6564 6469 6e67 206c 6179 6572 7320  mbedding layers 
-000003f0: 7573 6564 2069 6e20 5472 616e 7366 6f72  used in Transfor
-00000400: 6d65 7273 206c 6974 6572 6174 7572 652c  mers literature,
-00000410: 2066 6f72 2074 6f6b 656e 7320 616e 6420   for tokens and 
-00000420: 706f 7369 7469 6f6e 732e 2050 6f73 6974  positions. Posit
-00000430: 696f 6e61 6c20 656e 636f 6469 6e67 2069  ional encoding i
-00000440: 7320 6c65 6172 6e65 6420 7468 726f 7567  s learned throug
-00000450: 6820 6120 6074 662e 6b65 7261 732e 6c61  h a `tf.keras.la
-00000460: 7965 7273 2e45 6d62 6564 6469 6e67 2829  yers.Embedding()
-00000470: 6020 6c61 7965 722c 2069 6e73 7465 6164  ` layer, instead
-00000480: 206f 6620 6465 7465 726d 696e 6973 7469   of deterministi
-00000490: 6320 706f 7369 7469 6f6e 616c 2065 6e63  c positional enc
-000004a0: 6f64 696e 6720 696e 2074 6865 206f 7269  oding in the ori
-000004b0: 6769 6e61 6c20 7061 7065 722e 0a0a 2d20  ginal paper...- 
-000004c0: 6054 7261 6e73 666f 726d 6572 4c61 7965  `TransformerLaye
-000004d0: 7260 3a20 606b 6572 6173 2e4c 6179 6572  r`: `keras.Layer
-000004e0: 6020 7369 6e67 6c65 2054 7261 6e73 666f  ` single Transfo
-000004f0: 726d 6572 2045 6e63 6f64 6572 2070 6965  rmer Encoder pie
-00000500: 6365 2e20 4974 2063 616e 2062 6520 7573  ce. It can be us
-00000510: 6564 2069 6e73 6964 6520 616e 7920 6053  ed inside any `S
-00000520: 6571 7565 6e74 6961 6c28 2960 206d 6f64  equential()` mod
-00000530: 656c 2069 6e20 4b65 7261 732e 0a0a 2d20  el in Keras...- 
-00000540: 6047 5054 4c61 7965 7260 3a20 606b 6572  `GPTLayer`: `ker
-00000550: 6173 2e4c 6179 6572 6020 4750 5420 626c  as.Layer` GPT bl
-00000560: 6f63 6b2e 2053 696d 696c 6172 2074 6f20  ock. Similar to 
-00000570: 6054 7261 6e73 666f 726d 6572 4c61 7965  `TransformerLaye
-00000580: 7260 2062 7574 2077 6974 6820 6361 7573  r` but with caus
-00000590: 616c 2041 7474 656e 7469 6f6e 206d 6563  al Attention mec
-000005a0: 6861 6e69 736d 2e20 4974 2063 616e 2062  hanism. It can b
-000005b0: 6520 7573 6564 2069 6e73 6964 6520 616e  e used inside an
-000005c0: 7920 6053 6571 7565 6e74 6961 6c28 2960  y `Sequential()`
-000005d0: 206d 6f64 656c 2069 6e20 4b65 7261 732e   model in Keras.
-000005e0: 0a0a 0a49 6e20 6073 6368 6564 756c 6573  ...In `schedules
-000005f0: 2e70 7960 3a0a 2d20 604f 7269 6769 6e61  .py`:.- `Origina
-00000600: 6c54 7261 6e73 666f 726d 6572 5363 6865  lTransformerSche
-00000610: 6475 6c65 603a 2060 6b65 7261 732e 4c61  dule`: `keras.La
-00000620: 7965 7260 2069 6d70 6c65 6d65 6e74 7320  yer` implements 
-00000630: 7468 6520 6c65 6172 6e69 6e67 2072 6174  the learning rat
-00000640: 6520 7363 6865 6475 6c65 206f 6620 7468  e schedule of th
-00000650: 6520 6f72 6967 696e 616c 2054 7261 6e73  e original Trans
-00000660: 666f 726d 6572 2070 6170 6572 2e20 4974  former paper. It
-00000670: 2069 7320 7461 6b65 6e20 6672 6f6d 2074   is taken from t
-00000680: 6869 7320 5b6f 6666 6963 6961 6c20 5465  his [official Te
-00000690: 6e73 6f72 466c 6f77 2074 7574 6f72 6961  nsorFlow tutoria
-000006a0: 6c5d 2868 7474 7073 3a2f 2f77 7777 2e74  l](https://www.t
-000006b0: 656e 736f 7266 6c6f 772e 6f72 672f 7465  ensorflow.org/te
-000006c0: 7874 2f74 7574 6f72 6961 6c73 2f74 7261  xt/tutorials/tra
-000006d0: 6e73 666f 726d 6572 292e 0a0a 2320 5265  nsformer)...# Re
-000006e0: 7175 6972 656d 656e 7473 0a60 6060 0a6e  quirements.```.n
-000006f0: 756d 7079 0a74 656e 736f 7266 6c6f 7720  umpy.tensorflow 
-00000700: 3e3d 2032 2e30 0a60 6060 0a0a 2320 4175  >= 2.0.```..# Au
-00000710: 7468 6f72 0a49 7661 6e20 426f 6e67 696f  thor.Ivan Bongio
-00000720: 726e 692e 205b 4c69 6e6b 6564 496e 5d28  rni. [LinkedIn](
-00000730: 6874 7470 733a 2f2f 7777 772e 6c69 6e6b  https://www.link
-00000740: 6564 696e 2e63 6f6d 2f69 6e2f 6976 616e  edin.com/in/ivan
-00000750: 2d62 6f6e 6769 6f72 6e69 2d62 3861 3538  -bongiorni-b8a58
-00000760: 3331 3634 2f29 0a0a 2320 4c69 6365 6e73  3164/)..# Licens
-00000770: 650a 3230 3230 2049 7661 6e20 426f 6e67  e.2020 Ivan Bong
-00000780: 696f 726e 690a 0a54 6869 7320 7265 706f  iorni..This repo
-00000790: 7369 746f 7279 2069 7320 6c69 6365 6e73  sitory is licens
-000007a0: 6564 2075 6e64 6572 2074 6865 204d 4954  ed under the MIT
-000007b0: 206c 6963 656e 7365 2e20 5365 6520 5b4c   license. See [L
-000007c0: 4943 454e 4345 2e74 7874 5d28 2920 666f  ICENCE.txt]() fo
-000007d0: 7220 6675 7274 6865 7220 6465 7461 696c  r further detail
-000007e0: 732e 0a                                  s..
+00000100: 6f77 2032 2e0a 0a3c 6120 6872 6566 3d22  ow 2...<a href="
+00000110: 7572 6c22 3e3c 696d 6720 7372 633d 2268  url"><img src="h
+00000120: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
+00000130: 6d2f 4976 616e 426f 6e67 696f 726e 692f  m/IvanBongiorni/
+00000140: 6d61 7869 6d61 6c2f 626c 6f62 2f6d 6169  maximal/blob/mai
+00000150: 6e2f 7574 696c 732f 6d61 7869 6d61 6c5f  n/utils/maximal_
+00000160: 7374 6162 6c65 6469 6666 7573 696f 6e5f  stablediffusion_
+00000170: 3030 2e70 6e67 2220 616c 6967 6e3d 2263  00.png" align="c
+00000180: 656e 7465 7222 3e3c 2f61 3e0a 3c62 723e  enter"></a>.<br>
+00000190: 0a2a 4c6f 676f 2067 656e 6572 6174 6564  .*Logo generated
+000001a0: 2062 7920 5374 6162 6c65 2044 6966 6675   by Stable Diffu
+000001b0: 7369 6f6e 2032 2e31 2a0a 3c62 723e 0a0a  sion 2.1*.<br>..
+000001c0: 2320 496e 7374 616c 6c61 7469 6f6e 0a49  # Installation.I
+000001d0: 7473 2069 6e73 7461 6c6c 6174 696f 6e20  ts installation 
+000001e0: 6973 2073 7472 6169 6768 7466 6f72 7761  is straightforwa
+000001f0: 7264 3a0a 0a60 6060 0a70 6970 2069 6e73  rd:..```.pip ins
+00000200: 7461 6c6c 206d 6178 696d 616c 0a60 6060  tall maximal.```
+00000210: 0a0a 2320 486f 7720 746f 2075 7365 2069  ..# How to use i
+00000220: 743f 0a60 6d61 7869 6d61 6c60 2069 7320  t?.`maximal` is 
+00000230: 636f 6d6d 6f6e 6c79 2063 616c 6c65 6420  commonly called 
+00000240: 6173 3a0a 0a60 6060 0a69 6d70 6f72 7420  as:..```.import 
+00000250: 6d61 7869 6d61 6c0a 6672 6f6d 206d 6178  maximal.from max
+00000260: 696d 616c 2e6c 6179 6572 7320 696d 706f  imal.layers impo
+00000270: 7274 2054 7261 6e73 666f 726d 6572 4c61  rt TransformerLa
+00000280: 7965 722c 2047 5054 4c61 7965 720a 6060  yer, GPTLayer.``
+00000290: 600a 0a61 6e64 2063 616e 2062 6520 7573  `..and can be us
+000002a0: 6564 2069 6e20 6120 6074 662e 6b65 7261  ed in a `tf.kera
+000002b0: 7360 206d 6f64 656c 2061 7320 616e 7920  s` model as any 
+000002c0: 636f 6d6d 6f6e 206c 6179 6572 2e0a 0a0a  common layer....
+000002d0: 2320 446f 6375 6d65 6e74 6174 696f 6e0a  # Documentation.
+000002e0: 416e 205b 4f66 6669 6369 616c 2057 6562  An [Official Web
+000002f0: 7369 7465 5d28 6874 7470 733a 2f2f 6976  site](https://iv
+00000300: 616e 626f 6e67 696f 726e 692e 6769 7468  anbongiorni.gith
+00000310: 7562 2e69 6f2f 6d61 7869 6d61 6c2f 2920  ub.io/maximal/) 
+00000320: 6973 206e 6f77 2061 7661 696c 6162 6c65  is now available
+00000330: 2077 6974 6820 646f 6375 6d65 6e74 6174   with documentat
+00000340: 696f 6e20 616e 6420 7475 746f 7269 616c  ion and tutorial
+00000350: 732e 0a0a 0a23 2045 6c65 6d65 6e74 730a  s....# Elements.
+00000360: 0a49 6e20 606c 6179 6572 732e 7079 603a  .In `layers.py`:
+00000370: 0a2d 2060 5365 6c66 4174 7465 6e74 696f  .- `SelfAttentio
+00000380: 6e60 3a20 606b 6572 6173 2e4c 6179 6572  n`: `keras.Layer
+00000390: 602c 2063 6f6d 7075 7465 7320 2a53 6361  `, computes *Sca
+000003a0: 6c65 6420 446f 742d 5072 6f64 7563 7420  led Dot-Product 
+000003b0: 4174 7465 6e74 696f 6e2a 2e0a 0a2d 2060  Attention*...- `
+000003c0: 4d75 6c74 6948 6561 6453 656c 6641 7474  MultiHeadSelfAtt
+000003d0: 656e 7469 6f6e 603a 2060 6b65 7261 732e  ention`: `keras.
+000003e0: 4c61 7965 7260 2c20 6974 2069 7320 6120  Layer`, it is a 
+000003f0: 636f 6e63 6174 656e 6174 696f 6e20 6f66  concatenation of
+00000400: 2060 5365 6c66 4174 7465 6e74 696f 6e60   `SelfAttention`
+00000410: 206c 6179 6572 732c 2072 6573 697a 6564   layers, resized
+00000420: 2062 6163 6b20 746f 206f 7269 6769 6e61   back to origina
+00000430: 6c20 696e 7075 7420 7368 6170 6520 7468  l input shape th
+00000440: 726f 7567 6820 6c69 6e65 6172 2074 7261  rough linear tra
+00000450: 6e73 666f 726d 6174 696f 6e2e 0a0a 2d20  nsformation...- 
+00000460: 6050 6f73 6974 696f 6e61 6c45 6d62 6564  `PositionalEmbed
+00000470: 6469 6e67 603a 2060 6b65 7261 732e 4c61  ding`: `keras.La
+00000480: 7965 7260 2c20 696d 706c 656d 656e 7473  yer`, implements
+00000490: 2064 6f75 626c 6520 456d 6265 6464 696e   double Embeddin
+000004a0: 6720 6c61 7965 7273 2075 7365 6420 696e  g layers used in
+000004b0: 2054 7261 6e73 666f 726d 6572 7320 6c69   Transformers li
+000004c0: 7465 7261 7475 7265 2c20 666f 7220 746f  terature, for to
+000004d0: 6b65 6e73 2061 6e64 2070 6f73 6974 696f  kens and positio
+000004e0: 6e73 2e20 506f 7369 7469 6f6e 616c 2065  ns. Positional e
+000004f0: 6e63 6f64 696e 6720 6973 206c 6561 726e  ncoding is learn
+00000500: 6564 2074 6872 6f75 6768 2061 2060 7466  ed through a `tf
+00000510: 2e6b 6572 6173 2e6c 6179 6572 732e 456d  .keras.layers.Em
+00000520: 6265 6464 696e 6728 2960 206c 6179 6572  bedding()` layer
+00000530: 2c20 696e 7374 6561 6420 6f66 2064 6574  , instead of det
+00000540: 6572 6d69 6e69 7374 6963 2070 6f73 6974  erministic posit
+00000550: 696f 6e61 6c20 656e 636f 6469 6e67 2069  ional encoding i
+00000560: 6e20 7468 6520 6f72 6967 696e 616c 2070  n the original p
+00000570: 6170 6572 2e0a 0a2d 2060 5472 616e 7366  aper...- `Transf
+00000580: 6f72 6d65 724c 6179 6572 603a 2060 6b65  ormerLayer`: `ke
+00000590: 7261 732e 4c61 7965 7260 2073 696e 676c  ras.Layer` singl
+000005a0: 6520 5472 616e 7366 6f72 6d65 7220 456e  e Transformer En
+000005b0: 636f 6465 7220 7069 6563 652e 2049 7420  coder piece. It 
+000005c0: 6361 6e20 6265 2075 7365 6420 696e 7369  can be used insi
+000005d0: 6465 2061 6e79 2060 5365 7175 656e 7469  de any `Sequenti
+000005e0: 616c 2829 6020 6d6f 6465 6c20 696e 204b  al()` model in K
+000005f0: 6572 6173 2e0a 0a2d 2060 4750 544c 6179  eras...- `GPTLay
+00000600: 6572 603a 2060 6b65 7261 732e 4c61 7965  er`: `keras.Laye
+00000610: 7260 2047 5054 2062 6c6f 636b 2e20 5369  r` GPT block. Si
+00000620: 6d69 6c61 7220 746f 2060 5472 616e 7366  milar to `Transf
+00000630: 6f72 6d65 724c 6179 6572 6020 6275 7420  ormerLayer` but 
+00000640: 7769 7468 2063 6175 7361 6c20 4174 7465  with causal Atte
+00000650: 6e74 696f 6e20 6d65 6368 616e 6973 6d2e  ntion mechanism.
+00000660: 2049 7420 6361 6e20 6265 2075 7365 6420   It can be used 
+00000670: 696e 7369 6465 2061 6e79 2060 5365 7175  inside any `Sequ
+00000680: 656e 7469 616c 2829 6020 6d6f 6465 6c20  ential()` model 
+00000690: 696e 204b 6572 6173 2e0a 0a0a 496e 2060  in Keras....In `
+000006a0: 7363 6865 6475 6c65 732e 7079 603a 0a2d  schedules.py`:.-
+000006b0: 2060 4f72 6967 696e 616c 5472 616e 7366   `OriginalTransf
+000006c0: 6f72 6d65 7253 6368 6564 756c 6560 3a20  ormerSchedule`: 
+000006d0: 606b 6572 6173 2e4c 6179 6572 6020 696d  `keras.Layer` im
+000006e0: 706c 656d 656e 7473 2074 6865 206c 6561  plements the lea
+000006f0: 726e 696e 6720 7261 7465 2073 6368 6564  rning rate sched
+00000700: 756c 6520 6f66 2074 6865 206f 7269 6769  ule of the origi
+00000710: 6e61 6c20 5472 616e 7366 6f72 6d65 7220  nal Transformer 
+00000720: 7061 7065 722e 2049 7420 6973 2074 616b  paper. It is tak
+00000730: 656e 2066 726f 6d20 7468 6973 205b 6f66  en from this [of
+00000740: 6669 6369 616c 2054 656e 736f 7246 6c6f  ficial TensorFlo
+00000750: 7720 7475 746f 7269 616c 5d28 6874 7470  w tutorial](http
+00000760: 733a 2f2f 7777 772e 7465 6e73 6f72 666c  s://www.tensorfl
+00000770: 6f77 2e6f 7267 2f74 6578 742f 7475 746f  ow.org/text/tuto
+00000780: 7269 616c 732f 7472 616e 7366 6f72 6d65  rials/transforme
+00000790: 7229 2e0a 0a23 2052 6571 7569 7265 6d65  r)...# Requireme
+000007a0: 6e74 730a 6060 600a 6e75 6d70 790a 7465  nts.```.numpy.te
+000007b0: 6e73 6f72 666c 6f77 203e 3d20 322e 300a  nsorflow >= 2.0.
+000007c0: 6060 600a 0a23 2041 7574 686f 720a 4976  ```..# Author.Iv
+000007d0: 616e 2042 6f6e 6769 6f72 6e69 2e20 5b4c  an Bongiorni. [L
+000007e0: 696e 6b65 6449 6e5d 2868 7474 7073 3a2f  inkedIn](https:/
+000007f0: 2f77 7777 2e6c 696e 6b65 6469 6e2e 636f  /www.linkedin.co
+00000800: 6d2f 696e 2f69 7661 6e2d 626f 6e67 696f  m/in/ivan-bongio
+00000810: 726e 692d 6238 6135 3833 3136 342f 290a  rni-b8a583164/).
+00000820: 0a23 204c 6963 656e 7365 0a32 3032 3020  .# License.2020 
+00000830: 4976 616e 2042 6f6e 6769 6f72 6e69 0a0a  Ivan Bongiorni..
+00000840: 5468 6973 2072 6570 6f73 6974 6f72 7920  This repository 
+00000850: 6973 206c 6963 656e 7365 6420 756e 6465  is licensed unde
+00000860: 7220 7468 6520 4d49 5420 6c69 6365 6e73  r the MIT licens
+00000870: 652e 2053 6565 205b 4c49 4345 4e43 452e  e. See [LICENCE.
+00000880: 7478 745d 2829 2066 6f72 2066 7572 7468  txt]() for furth
+00000890: 6572 2064 6574 6169 6c73 2e0a            er details..
```

### Comparing `maximal-1.0/maximal/layers.py` & `maximal-1.1/maximal/layers.py`

 * *Files identical despite different names*

### Comparing `maximal-1.0/maximal/schedules.py` & `maximal-1.1/maximal/schedules.py`

 * *Files identical despite different names*

### Comparing `maximal-1.0/maximal.egg-info/PKG-INFO` & `maximal-1.1/maximal.egg-info/PKG-INFO`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 00000000: 4d65 7461 6461 7461 2d56 6572 7369 6f6e  Metadata-Version
 00000010: 3a20 322e 310a 4e61 6d65 3a20 6d61 7869  : 2.1.Name: maxi
-00000020: 6d61 6c0a 5665 7273 696f 6e3a 2031 2e30  mal.Version: 1.0
+00000020: 6d61 6c0a 5665 7273 696f 6e3a 2031 2e31  mal.Version: 1.1
 00000030: 0a53 756d 6d61 7279 3a20 5465 6e73 6f72  .Summary: Tensor
 00000040: 466c 6f77 2d63 6f6d 7061 7469 626c 6520  Flow-compatible 
 00000050: 5472 616e 7366 6f72 6d65 7220 6c61 7965  Transformer laye
 00000060: 7273 2061 6e64 206d 6f64 656c 732e 0a48  rs and models..H
 00000070: 6f6d 652d 7061 6765 3a20 6874 7470 733a  ome-page: https:
 00000080: 2f2f 6769 7468 7562 2e63 6f6d 2f49 7661  //github.com/Iva
 00000090: 6e42 6f6e 6769 6f72 6e69 2f6d 6178 696d  nBongiorni/maxim
@@ -45,118 +45,129 @@
 000002c0: 6f6e 206c 6962 7261 7279 2074 6861 7420  on library that 
 000002d0: 7072 6f76 6964 6573 206d 6f64 656c 7320  provides models 
 000002e0: 616e 6420 6c61 7965 7273 2074 6f20 696d  and layers to im
 000002f0: 706c 656d 656e 7420 6375 7374 6f6d 2054  plement custom T
 00000300: 7261 6e73 666f 726d 6572 206e 6575 7261  ransformer neura
 00000310: 6c20 6e65 7477 6f72 6b73 2e0a 0a42 7569  l networks...Bui
 00000320: 6c74 206f 6e20 5465 6e73 6f72 466c 6f77  lt on TensorFlow
-00000330: 2032 2e0a 0a23 2049 6e73 7461 6c6c 6174   2...# Installat
-00000340: 696f 6e0a 4974 7320 696e 7374 616c 6c61  ion.Its installa
-00000350: 7469 6f6e 2069 7320 7374 7261 6967 6874  tion is straight
-00000360: 666f 7277 6172 643a 0a0a 6060 600a 7069  forward:..```.pi
-00000370: 7020 696e 7374 616c 6c20 6d61 7869 6d61  p install maxima
-00000380: 6c0a 6060 600a 0a23 2048 6f77 2074 6f20  l.```..# How to 
-00000390: 7573 6520 6974 3f0a 606d 6178 696d 616c  use it?.`maximal
-000003a0: 6020 6973 2063 6f6d 6d6f 6e6c 7920 6361  ` is commonly ca
-000003b0: 6c6c 6564 2061 733a 0a0a 6060 600a 696d  lled as:..```.im
-000003c0: 706f 7274 206d 6178 696d 616c 0a66 726f  port maximal.fro
-000003d0: 6d20 6d61 7869 6d61 6c2e 6c61 7965 7273  m maximal.layers
-000003e0: 2069 6d70 6f72 7420 5472 616e 7366 6f72   import Transfor
-000003f0: 6d65 724c 6179 6572 2c20 4750 544c 6179  merLayer, GPTLay
-00000400: 6572 0a60 6060 0a0a 616e 6420 6361 6e20  er.```..and can 
-00000410: 6265 2075 7365 6420 696e 2061 2060 7466  be used in a `tf
-00000420: 2e6b 6572 6173 6020 6d6f 6465 6c20 6173  .keras` model as
-00000430: 2061 6e79 2063 6f6d 6d6f 6e20 6c61 7965   any common laye
-00000440: 722e 0a0a 0a23 2044 6f63 756d 656e 7461  r....# Documenta
-00000450: 7469 6f6e 0a41 6e20 5b4f 6666 6963 6961  tion.An [Officia
-00000460: 6c20 5765 6273 6974 655d 2868 7474 7073  l Website](https
-00000470: 3a2f 2f69 7661 6e62 6f6e 6769 6f72 6e69  ://ivanbongiorni
-00000480: 2e67 6974 6875 622e 696f 2f6d 6178 696d  .github.io/maxim
-00000490: 616c 2f29 2069 7320 6e6f 7720 6176 6169  al/) is now avai
-000004a0: 6c61 626c 6520 7769 7468 2064 6f63 756d  lable with docum
-000004b0: 656e 7461 7469 6f6e 2061 6e64 2074 7574  entation and tut
-000004c0: 6f72 6961 6c73 2e0a 0a0a 2320 456c 656d  orials....# Elem
-000004d0: 656e 7473 0a0a 496e 2060 6c61 7965 7273  ents..In `layers
-000004e0: 2e70 7960 3a0a 2d20 6053 656c 6641 7474  .py`:.- `SelfAtt
-000004f0: 656e 7469 6f6e 603a 2060 6b65 7261 732e  ention`: `keras.
-00000500: 4c61 7965 7260 2c20 636f 6d70 7574 6573  Layer`, computes
-00000510: 202a 5363 616c 6564 2044 6f74 2d50 726f   *Scaled Dot-Pro
-00000520: 6475 6374 2041 7474 656e 7469 6f6e 2a2e  duct Attention*.
-00000530: 0a0a 2d20 604d 756c 7469 4865 6164 5365  ..- `MultiHeadSe
-00000540: 6c66 4174 7465 6e74 696f 6e60 3a20 606b  lfAttention`: `k
-00000550: 6572 6173 2e4c 6179 6572 602c 2069 7420  eras.Layer`, it 
-00000560: 6973 2061 2063 6f6e 6361 7465 6e61 7469  is a concatenati
-00000570: 6f6e 206f 6620 6053 656c 6641 7474 656e  on of `SelfAtten
-00000580: 7469 6f6e 6020 6c61 7965 7273 2c20 7265  tion` layers, re
-00000590: 7369 7a65 6420 6261 636b 2074 6f20 6f72  sized back to or
-000005a0: 6967 696e 616c 2069 6e70 7574 2073 6861  iginal input sha
-000005b0: 7065 2074 6872 6f75 6768 206c 696e 6561  pe through linea
-000005c0: 7220 7472 616e 7366 6f72 6d61 7469 6f6e  r transformation
-000005d0: 2e0a 0a2d 2060 506f 7369 7469 6f6e 616c  ...- `Positional
-000005e0: 456d 6265 6464 696e 6760 3a20 606b 6572  Embedding`: `ker
-000005f0: 6173 2e4c 6179 6572 602c 2069 6d70 6c65  as.Layer`, imple
-00000600: 6d65 6e74 7320 646f 7562 6c65 2045 6d62  ments double Emb
-00000610: 6564 6469 6e67 206c 6179 6572 7320 7573  edding layers us
-00000620: 6564 2069 6e20 5472 616e 7366 6f72 6d65  ed in Transforme
-00000630: 7273 206c 6974 6572 6174 7572 652c 2066  rs literature, f
-00000640: 6f72 2074 6f6b 656e 7320 616e 6420 706f  or tokens and po
-00000650: 7369 7469 6f6e 732e 2050 6f73 6974 696f  sitions. Positio
-00000660: 6e61 6c20 656e 636f 6469 6e67 2069 7320  nal encoding is 
-00000670: 6c65 6172 6e65 6420 7468 726f 7567 6820  learned through 
-00000680: 6120 6074 662e 6b65 7261 732e 6c61 7965  a `tf.keras.laye
-00000690: 7273 2e45 6d62 6564 6469 6e67 2829 6020  rs.Embedding()` 
-000006a0: 6c61 7965 722c 2069 6e73 7465 6164 206f  layer, instead o
-000006b0: 6620 6465 7465 726d 696e 6973 7469 6320  f deterministic 
-000006c0: 706f 7369 7469 6f6e 616c 2065 6e63 6f64  positional encod
-000006d0: 696e 6720 696e 2074 6865 206f 7269 6769  ing in the origi
-000006e0: 6e61 6c20 7061 7065 722e 0a0a 2d20 6054  nal paper...- `T
-000006f0: 7261 6e73 666f 726d 6572 4c61 7965 7260  ransformerLayer`
-00000700: 3a20 606b 6572 6173 2e4c 6179 6572 6020  : `keras.Layer` 
-00000710: 7369 6e67 6c65 2054 7261 6e73 666f 726d  single Transform
-00000720: 6572 2045 6e63 6f64 6572 2070 6965 6365  er Encoder piece
-00000730: 2e20 4974 2063 616e 2062 6520 7573 6564  . It can be used
-00000740: 2069 6e73 6964 6520 616e 7920 6053 6571   inside any `Seq
-00000750: 7565 6e74 6961 6c28 2960 206d 6f64 656c  uential()` model
-00000760: 2069 6e20 4b65 7261 732e 0a0a 2d20 6047   in Keras...- `G
-00000770: 5054 4c61 7965 7260 3a20 606b 6572 6173  PTLayer`: `keras
-00000780: 2e4c 6179 6572 6020 4750 5420 626c 6f63  .Layer` GPT bloc
-00000790: 6b2e 2053 696d 696c 6172 2074 6f20 6054  k. Similar to `T
-000007a0: 7261 6e73 666f 726d 6572 4c61 7965 7260  ransformerLayer`
-000007b0: 2062 7574 2077 6974 6820 6361 7573 616c   but with causal
-000007c0: 2041 7474 656e 7469 6f6e 206d 6563 6861   Attention mecha
-000007d0: 6e69 736d 2e20 4974 2063 616e 2062 6520  nism. It can be 
-000007e0: 7573 6564 2069 6e73 6964 6520 616e 7920  used inside any 
-000007f0: 6053 6571 7565 6e74 6961 6c28 2960 206d  `Sequential()` m
-00000800: 6f64 656c 2069 6e20 4b65 7261 732e 0a0a  odel in Keras...
-00000810: 0a49 6e20 6073 6368 6564 756c 6573 2e70  .In `schedules.p
-00000820: 7960 3a0a 2d20 604f 7269 6769 6e61 6c54  y`:.- `OriginalT
-00000830: 7261 6e73 666f 726d 6572 5363 6865 6475  ransformerSchedu
-00000840: 6c65 603a 2060 6b65 7261 732e 4c61 7965  le`: `keras.Laye
-00000850: 7260 2069 6d70 6c65 6d65 6e74 7320 7468  r` implements th
-00000860: 6520 6c65 6172 6e69 6e67 2072 6174 6520  e learning rate 
-00000870: 7363 6865 6475 6c65 206f 6620 7468 6520  schedule of the 
-00000880: 6f72 6967 696e 616c 2054 7261 6e73 666f  original Transfo
-00000890: 726d 6572 2070 6170 6572 2e20 4974 2069  rmer paper. It i
-000008a0: 7320 7461 6b65 6e20 6672 6f6d 2074 6869  s taken from thi
-000008b0: 7320 5b6f 6666 6963 6961 6c20 5465 6e73  s [official Tens
-000008c0: 6f72 466c 6f77 2074 7574 6f72 6961 6c5d  orFlow tutorial]
-000008d0: 2868 7474 7073 3a2f 2f77 7777 2e74 656e  (https://www.ten
-000008e0: 736f 7266 6c6f 772e 6f72 672f 7465 7874  sorflow.org/text
-000008f0: 2f74 7574 6f72 6961 6c73 2f74 7261 6e73  /tutorials/trans
-00000900: 666f 726d 6572 292e 0a0a 2320 5265 7175  former)...# Requ
-00000910: 6972 656d 656e 7473 0a60 6060 0a6e 756d  irements.```.num
-00000920: 7079 0a74 656e 736f 7266 6c6f 7720 3e3d  py.tensorflow >=
-00000930: 2032 2e30 0a60 6060 0a0a 2320 4175 7468   2.0.```..# Auth
-00000940: 6f72 0a49 7661 6e20 426f 6e67 696f 726e  or.Ivan Bongiorn
-00000950: 692e 205b 4c69 6e6b 6564 496e 5d28 6874  i. [LinkedIn](ht
-00000960: 7470 733a 2f2f 7777 772e 6c69 6e6b 6564  tps://www.linked
-00000970: 696e 2e63 6f6d 2f69 6e2f 6976 616e 2d62  in.com/in/ivan-b
-00000980: 6f6e 6769 6f72 6e69 2d62 3861 3538 3331  ongiorni-b8a5831
-00000990: 3634 2f29 0a0a 2320 4c69 6365 6e73 650a  64/)..# License.
-000009a0: 3230 3230 2049 7661 6e20 426f 6e67 696f  2020 Ivan Bongio
-000009b0: 726e 690a 0a54 6869 7320 7265 706f 7369  rni..This reposi
-000009c0: 746f 7279 2069 7320 6c69 6365 6e73 6564  tory is licensed
-000009d0: 2075 6e64 6572 2074 6865 204d 4954 206c   under the MIT l
-000009e0: 6963 656e 7365 2e20 5365 6520 5b4c 4943  icense. See [LIC
-000009f0: 454e 4345 2e74 7874 5d28 2920 666f 7220  ENCE.txt]() for 
-00000a00: 6675 7274 6865 7220 6465 7461 696c 732e  further details.
-00000a10: 0a                                       .
+00000330: 2032 2e0a 0a3c 6120 6872 6566 3d22 7572   2...<a href="ur
+00000340: 6c22 3e3c 696d 6720 7372 633d 2268 7474  l"><img src="htt
+00000350: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
+00000360: 4976 616e 426f 6e67 696f 726e 692f 6d61  IvanBongiorni/ma
+00000370: 7869 6d61 6c2f 626c 6f62 2f6d 6169 6e2f  ximal/blob/main/
+00000380: 7574 696c 732f 6d61 7869 6d61 6c5f 7374  utils/maximal_st
+00000390: 6162 6c65 6469 6666 7573 696f 6e5f 3030  ablediffusion_00
+000003a0: 2e70 6e67 2220 616c 6967 6e3d 2263 656e  .png" align="cen
+000003b0: 7465 7222 3e3c 2f61 3e0a 3c62 723e 0a2a  ter"></a>.<br>.*
+000003c0: 4c6f 676f 2067 656e 6572 6174 6564 2062  Logo generated b
+000003d0: 7920 5374 6162 6c65 2044 6966 6675 7369  y Stable Diffusi
+000003e0: 6f6e 2032 2e31 2a0a 3c62 723e 0a0a 2320  on 2.1*.<br>..# 
+000003f0: 496e 7374 616c 6c61 7469 6f6e 0a49 7473  Installation.Its
+00000400: 2069 6e73 7461 6c6c 6174 696f 6e20 6973   installation is
+00000410: 2073 7472 6169 6768 7466 6f72 7761 7264   straightforward
+00000420: 3a0a 0a60 6060 0a70 6970 2069 6e73 7461  :..```.pip insta
+00000430: 6c6c 206d 6178 696d 616c 0a60 6060 0a0a  ll maximal.```..
+00000440: 2320 486f 7720 746f 2075 7365 2069 743f  # How to use it?
+00000450: 0a60 6d61 7869 6d61 6c60 2069 7320 636f  .`maximal` is co
+00000460: 6d6d 6f6e 6c79 2063 616c 6c65 6420 6173  mmonly called as
+00000470: 3a0a 0a60 6060 0a69 6d70 6f72 7420 6d61  :..```.import ma
+00000480: 7869 6d61 6c0a 6672 6f6d 206d 6178 696d  ximal.from maxim
+00000490: 616c 2e6c 6179 6572 7320 696d 706f 7274  al.layers import
+000004a0: 2054 7261 6e73 666f 726d 6572 4c61 7965   TransformerLaye
+000004b0: 722c 2047 5054 4c61 7965 720a 6060 600a  r, GPTLayer.```.
+000004c0: 0a61 6e64 2063 616e 2062 6520 7573 6564  .and can be used
+000004d0: 2069 6e20 6120 6074 662e 6b65 7261 7360   in a `tf.keras`
+000004e0: 206d 6f64 656c 2061 7320 616e 7920 636f   model as any co
+000004f0: 6d6d 6f6e 206c 6179 6572 2e0a 0a0a 2320  mmon layer....# 
+00000500: 446f 6375 6d65 6e74 6174 696f 6e0a 416e  Documentation.An
+00000510: 205b 4f66 6669 6369 616c 2057 6562 7369   [Official Websi
+00000520: 7465 5d28 6874 7470 733a 2f2f 6976 616e  te](https://ivan
+00000530: 626f 6e67 696f 726e 692e 6769 7468 7562  bongiorni.github
+00000540: 2e69 6f2f 6d61 7869 6d61 6c2f 2920 6973  .io/maximal/) is
+00000550: 206e 6f77 2061 7661 696c 6162 6c65 2077   now available w
+00000560: 6974 6820 646f 6375 6d65 6e74 6174 696f  ith documentatio
+00000570: 6e20 616e 6420 7475 746f 7269 616c 732e  n and tutorials.
+00000580: 0a0a 0a23 2045 6c65 6d65 6e74 730a 0a49  ...# Elements..I
+00000590: 6e20 606c 6179 6572 732e 7079 603a 0a2d  n `layers.py`:.-
+000005a0: 2060 5365 6c66 4174 7465 6e74 696f 6e60   `SelfAttention`
+000005b0: 3a20 606b 6572 6173 2e4c 6179 6572 602c  : `keras.Layer`,
+000005c0: 2063 6f6d 7075 7465 7320 2a53 6361 6c65   computes *Scale
+000005d0: 6420 446f 742d 5072 6f64 7563 7420 4174  d Dot-Product At
+000005e0: 7465 6e74 696f 6e2a 2e0a 0a2d 2060 4d75  tention*...- `Mu
+000005f0: 6c74 6948 6561 6453 656c 6641 7474 656e  ltiHeadSelfAtten
+00000600: 7469 6f6e 603a 2060 6b65 7261 732e 4c61  tion`: `keras.La
+00000610: 7965 7260 2c20 6974 2069 7320 6120 636f  yer`, it is a co
+00000620: 6e63 6174 656e 6174 696f 6e20 6f66 2060  ncatenation of `
+00000630: 5365 6c66 4174 7465 6e74 696f 6e60 206c  SelfAttention` l
+00000640: 6179 6572 732c 2072 6573 697a 6564 2062  ayers, resized b
+00000650: 6163 6b20 746f 206f 7269 6769 6e61 6c20  ack to original 
+00000660: 696e 7075 7420 7368 6170 6520 7468 726f  input shape thro
+00000670: 7567 6820 6c69 6e65 6172 2074 7261 6e73  ugh linear trans
+00000680: 666f 726d 6174 696f 6e2e 0a0a 2d20 6050  formation...- `P
+00000690: 6f73 6974 696f 6e61 6c45 6d62 6564 6469  ositionalEmbeddi
+000006a0: 6e67 603a 2060 6b65 7261 732e 4c61 7965  ng`: `keras.Laye
+000006b0: 7260 2c20 696d 706c 656d 656e 7473 2064  r`, implements d
+000006c0: 6f75 626c 6520 456d 6265 6464 696e 6720  ouble Embedding 
+000006d0: 6c61 7965 7273 2075 7365 6420 696e 2054  layers used in T
+000006e0: 7261 6e73 666f 726d 6572 7320 6c69 7465  ransformers lite
+000006f0: 7261 7475 7265 2c20 666f 7220 746f 6b65  rature, for toke
+00000700: 6e73 2061 6e64 2070 6f73 6974 696f 6e73  ns and positions
+00000710: 2e20 506f 7369 7469 6f6e 616c 2065 6e63  . Positional enc
+00000720: 6f64 696e 6720 6973 206c 6561 726e 6564  oding is learned
+00000730: 2074 6872 6f75 6768 2061 2060 7466 2e6b   through a `tf.k
+00000740: 6572 6173 2e6c 6179 6572 732e 456d 6265  eras.layers.Embe
+00000750: 6464 696e 6728 2960 206c 6179 6572 2c20  dding()` layer, 
+00000760: 696e 7374 6561 6420 6f66 2064 6574 6572  instead of deter
+00000770: 6d69 6e69 7374 6963 2070 6f73 6974 696f  ministic positio
+00000780: 6e61 6c20 656e 636f 6469 6e67 2069 6e20  nal encoding in 
+00000790: 7468 6520 6f72 6967 696e 616c 2070 6170  the original pap
+000007a0: 6572 2e0a 0a2d 2060 5472 616e 7366 6f72  er...- `Transfor
+000007b0: 6d65 724c 6179 6572 603a 2060 6b65 7261  merLayer`: `kera
+000007c0: 732e 4c61 7965 7260 2073 696e 676c 6520  s.Layer` single 
+000007d0: 5472 616e 7366 6f72 6d65 7220 456e 636f  Transformer Enco
+000007e0: 6465 7220 7069 6563 652e 2049 7420 6361  der piece. It ca
+000007f0: 6e20 6265 2075 7365 6420 696e 7369 6465  n be used inside
+00000800: 2061 6e79 2060 5365 7175 656e 7469 616c   any `Sequential
+00000810: 2829 6020 6d6f 6465 6c20 696e 204b 6572  ()` model in Ker
+00000820: 6173 2e0a 0a2d 2060 4750 544c 6179 6572  as...- `GPTLayer
+00000830: 603a 2060 6b65 7261 732e 4c61 7965 7260  `: `keras.Layer`
+00000840: 2047 5054 2062 6c6f 636b 2e20 5369 6d69   GPT block. Simi
+00000850: 6c61 7220 746f 2060 5472 616e 7366 6f72  lar to `Transfor
+00000860: 6d65 724c 6179 6572 6020 6275 7420 7769  merLayer` but wi
+00000870: 7468 2063 6175 7361 6c20 4174 7465 6e74  th causal Attent
+00000880: 696f 6e20 6d65 6368 616e 6973 6d2e 2049  ion mechanism. I
+00000890: 7420 6361 6e20 6265 2075 7365 6420 696e  t can be used in
+000008a0: 7369 6465 2061 6e79 2060 5365 7175 656e  side any `Sequen
+000008b0: 7469 616c 2829 6020 6d6f 6465 6c20 696e  tial()` model in
+000008c0: 204b 6572 6173 2e0a 0a0a 496e 2060 7363   Keras....In `sc
+000008d0: 6865 6475 6c65 732e 7079 603a 0a2d 2060  hedules.py`:.- `
+000008e0: 4f72 6967 696e 616c 5472 616e 7366 6f72  OriginalTransfor
+000008f0: 6d65 7253 6368 6564 756c 6560 3a20 606b  merSchedule`: `k
+00000900: 6572 6173 2e4c 6179 6572 6020 696d 706c  eras.Layer` impl
+00000910: 656d 656e 7473 2074 6865 206c 6561 726e  ements the learn
+00000920: 696e 6720 7261 7465 2073 6368 6564 756c  ing rate schedul
+00000930: 6520 6f66 2074 6865 206f 7269 6769 6e61  e of the origina
+00000940: 6c20 5472 616e 7366 6f72 6d65 7220 7061  l Transformer pa
+00000950: 7065 722e 2049 7420 6973 2074 616b 656e  per. It is taken
+00000960: 2066 726f 6d20 7468 6973 205b 6f66 6669   from this [offi
+00000970: 6369 616c 2054 656e 736f 7246 6c6f 7720  cial TensorFlow 
+00000980: 7475 746f 7269 616c 5d28 6874 7470 733a  tutorial](https:
+00000990: 2f2f 7777 772e 7465 6e73 6f72 666c 6f77  //www.tensorflow
+000009a0: 2e6f 7267 2f74 6578 742f 7475 746f 7269  .org/text/tutori
+000009b0: 616c 732f 7472 616e 7366 6f72 6d65 7229  als/transformer)
+000009c0: 2e0a 0a23 2052 6571 7569 7265 6d65 6e74  ...# Requirement
+000009d0: 730a 6060 600a 6e75 6d70 790a 7465 6e73  s.```.numpy.tens
+000009e0: 6f72 666c 6f77 203e 3d20 322e 300a 6060  orflow >= 2.0.``
+000009f0: 600a 0a23 2041 7574 686f 720a 4976 616e  `..# Author.Ivan
+00000a00: 2042 6f6e 6769 6f72 6e69 2e20 5b4c 696e   Bongiorni. [Lin
+00000a10: 6b65 6449 6e5d 2868 7474 7073 3a2f 2f77  kedIn](https://w
+00000a20: 7777 2e6c 696e 6b65 6469 6e2e 636f 6d2f  ww.linkedin.com/
+00000a30: 696e 2f69 7661 6e2d 626f 6e67 696f 726e  in/ivan-bongiorn
+00000a40: 692d 6238 6135 3833 3136 342f 290a 0a23  i-b8a583164/)..#
+00000a50: 204c 6963 656e 7365 0a32 3032 3020 4976   License.2020 Iv
+00000a60: 616e 2042 6f6e 6769 6f72 6e69 0a0a 5468  an Bongiorni..Th
+00000a70: 6973 2072 6570 6f73 6974 6f72 7920 6973  is repository is
+00000a80: 206c 6963 656e 7365 6420 756e 6465 7220   licensed under 
+00000a90: 7468 6520 4d49 5420 6c69 6365 6e73 652e  the MIT license.
+00000aa0: 2053 6565 205b 4c49 4345 4e43 452e 7478   See [LICENCE.tx
+00000ab0: 745d 2829 2066 6f72 2066 7572 7468 6572  t]() for further
+00000ac0: 2064 6574 6169 6c73 2e0a                  details..
```

### Comparing `maximal-1.0/setup.py` & `maximal-1.1/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,22 +32,23 @@
         'Topic :: Scientific/Engineering',
         'License :: OSI Approved :: MIT License',
         'Programming Language :: Python :: 3'
   ]
 
 setup(
     name='maximal',
-    version=1.0,
+    version=1.1,
     description='TensorFlow-compatible Transformer layers and models.',
     #long_description=open('README.md').read() + '\n\n' + open('CHANGELOG.txt').read(),
     url='https://github.com/IvanBongiorni/maximal',
     author='Ivan Bongiorni',
     author_email='ivanbongiorni@protonmail.com',
     license='MIT',
     classifiers=classifiers,
     long_description=open('README.md').read(),
     packages=find_packages(),
     install_requires=[
+        'h5py',
         'numpy',
         'tensorflow>=2.0'
     ]
 )
```

